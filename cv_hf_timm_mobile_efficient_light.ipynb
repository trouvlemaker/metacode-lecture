{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c50ffe",
   "metadata": {},
   "source": [
    "# ğŸš€ 2ê°• (ê°€ë²¼ìš´ ì„¤ì •): **MobileNet/EfficientNet ì „ì´í•™ìŠµ (timm + Hugging Face Hub)**\n",
    "\n",
    "- **ëª©í‘œ**: ë” ê°€ë²¼ìš´ ë°±ë³¸(MobileNetV3-Small, EfficientNet-B0)ìœ¼ë¡œ CPUì—ì„œë„ ì¾Œì í•œ ì „ì´í•™ìŠµ ë°ëª¨\n",
    "- **ê°€ì •**: `timm`ì´ Hugging Face Hubì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤(`hf_hub:` í”„ë¦¬í”½ìŠ¤ ì‚¬ìš©)\n",
    "\n",
    "**ì‹¤í–‰ ëª¨ë“œ**\n",
    "- ê¸°ë³¸: **Feature Extraction** (ë°±ë³¸ freeze) â†’ CPU OK\n",
    "- ì˜µì…˜: **Fine-tuning** (ì¼ë¶€ ë¸”ë¡ unfreeze) â†’ GPU ê¶Œì¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87e6fd",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q timm\n",
    "import os, numpy as np, random, matplotlib.pyplot as plt, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets as tvdatasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import timm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30ab5a",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ì¤€ë¹„ (ê°€ë²¼ìš´ ì„¤ì • ì ìš©)\n",
    "\n",
    "- **Subset**: 10% (CPUì—ì„œë„ ë¹ ë¥´ê²Œ)\n",
    "- **Batch size**: 16\n",
    "- **Epochs**: 2 (ë°ëª¨ìš©)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 ë¡œë“œ (PIL)\n",
    "root='./data'\n",
    "train_full = tvdatasets.CIFAR10(root=root, train=True, download=True)\n",
    "test_set   = tvdatasets.CIFAR10(root=root, train=False, download=True)\n",
    "class_names = train_full.classes\n",
    "\n",
    "def to_numpy_list(tv_dataset):\n",
    "    imgs, labs = [], []\n",
    "    for img, lab in tv_dataset:\n",
    "        imgs.append(np.array(img)); labs.append(lab)\n",
    "    return imgs, labs\n",
    "\n",
    "images_train, labels_train = to_numpy_list(train_full)\n",
    "images_test,  labels_test  = to_numpy_list(test_set)\n",
    "\n",
    "# Subset 10%\n",
    "SUBSET=0.1\n",
    "idx = np.random.RandomState(42).permutation(len(images_train))\n",
    "sel = idx[:int(len(images_train)*SUBSET)]\n",
    "images_train = [images_train[i] for i in sel]\n",
    "labels_train = [labels_train[i] for i in sel]\n",
    "\n",
    "tr_imgs, va_imgs, tr_lbls, va_lbls = train_test_split(\n",
    "    images_train, labels_train, test_size=0.2, stratify=labels_train, random_state=42\n",
    ")\n",
    "\n",
    "print('Train/Val/Test sizes:', len(tr_imgs), len(va_imgs), len(images_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd5acc",
   "metadata": {},
   "source": [
    "## 2. ë³€í™˜(Transform) â€” ëª¨ë¸ë³„ ì…ë ¥ ê·œê²© ë§ì¶”ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timmì˜ ê¸°ë³¸ ì…ë ¥ í¬ê¸°/ì „ì²˜ë¦¬ë¥¼ ì‚¬ìš©\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from PIL import Image\n",
    "\n",
    "# MobileNetV3-Small & EfficientNet-B0 repo IDs on HF Hub via timm\n",
    "mobilenet_repo = 'timm/mobilenetv3_small_100.lamb_in1k'\n",
    "efficient_repo = 'timm/efficientnet_b0.ra_in1k'\n",
    "\n",
    "# ì„ì‹œ ëª¨ë¸ë¡œ config í™•ì¸ (ë‚˜ì¤‘ì— ì‹¤ì œ ëª¨ë¸ ìƒì„± ì‹œì—ë„ ê°™ì€ ì…ë ¥)\n",
    "tmp_model = timm.create_model(f'hf_hub:{mobilenet_repo}', pretrained=True, num_classes=10)\n",
    "config = resolve_data_config({}, model=tmp_model)  # modelì˜ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œë¶€í„° transform ìƒì„±\n",
    "transform = create_transform(**config)\n",
    "\n",
    "IMG_SIZE = config.get('input_size', (3,224,224))[-1]\n",
    "print('Resolved input size:', IMG_SIZE)\n",
    "\n",
    "class NumpyCIFARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images; self.labels = labels; self.transform = transform\n",
    "    def __len__(self): return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.fromarray(self.images[idx])\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "train_ds = NumpyCIFARDataset(tr_imgs, tr_lbls, transform=transform)\n",
    "val_ds   = NumpyCIFARDataset(va_imgs, va_lbls, transform=transform)\n",
    "test_ds  = NumpyCIFARDataset(images_test, labels_test, transform=transform)\n",
    "\n",
    "BATCH=16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f897d",
   "metadata": {},
   "source": [
    "## 3. ë°±ë³¸ ì„ íƒ & ì „ì´í•™ìŠµ ì„¤ì •\n",
    "- ì•„ë˜ ìŠ¤ìœ„ì¹˜ë¡œ **MobileNetV3-Small** â†” **EfficientNet-B0** ë¥¼ ê°„ë‹¨íˆ êµì²´\n",
    "- **Feature Extraction**: ë°±ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ `requires_grad=False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'mobilenet'  # 'mobilenet' or 'efficientnet'\n",
    "\n",
    "if BACKBONE == 'mobilenet':\n",
    "    repo_id = mobilenet_repo\n",
    "else:\n",
    "    repo_id = efficient_repo\n",
    "\n",
    "model = timm.create_model(f'hf_hub:{repo_id}', pretrained=True, num_classes=10)\n",
    "model.to(device)\n",
    "\n",
    "# Feature Extraction: ë°±ë³¸ ë™ê²° (classifier/head ì œì™¸)\n",
    "for name, p in model.named_parameters():\n",
    "    if 'classifier' in name or 'fc' in name or 'head' in name:  # timm ëª¨ë¸ë³„ head ì´ë¦„\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "def count_trainable(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print('Trainable params:', count_trainable(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270084d",
   "metadata": {},
   "source": [
    "## 4. í•™ìŠµ ë£¨í”„ (Feature Extraction, CPU OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train(); tl=0; tc=0; n=0\n",
    "    for x,y in loader:\n",
    "        x,y=x.to(device),torch.tensor(y).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward(); optimizer.step()\n",
    "        tl += loss.item()*x.size(0)\n",
    "        tc += (out.argmax(1)==y).sum().item(); n += x.size(0)\n",
    "    return tl/n, tc/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval(); tl=0; tc=0; n=0\n",
    "    for x,y in loader:\n",
    "        x,y=x.to(device),torch.tensor(y).to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        tl += loss.item()*x.size(0)\n",
    "        tc += (out.argmax(1)==y).sum().item(); n += x.size(0)\n",
    "    return tl/n, tc/n\n",
    "\n",
    "EPOCHS=2  # ê°€ë²¼ìš´ ì„¤ì •\n",
    "hist={'tr_acc':[],'va_acc':[],'tr_loss':[],'va_loss':[]}\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    tr_l,tr_a = train_epoch(model, train_loader)\n",
    "    va_l,va_a = eval_epoch(model, val_loader)\n",
    "    hist['tr_loss'].append(tr_l); hist['va_loss'].append(va_l)\n",
    "    hist['tr_acc'].append(tr_a);  hist['va_acc'].append(va_a)\n",
    "    print(f'[Ep {ep}/{EPOCHS}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a439dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¡ì„ \n",
    "plt.figure(figsize=(6,4)); plt.plot(hist['tr_acc'],label='train_acc'); plt.plot(hist['va_acc'],label='val_acc')\n",
    "plt.title('Accuracy (Feature Extraction)'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "plt.figure(figsize=(6,4)); plt.plot(hist['tr_loss'],label='train_loss'); plt.plot(hist['va_loss'],label='val_loss')\n",
    "plt.title('Loss (Feature Extraction)'); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a1c49",
   "metadata": {},
   "source": [
    "## 5. í‰ê°€ + í˜¼ë™í–‰ë ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def preds_and_labels(model, loader):\n",
    "    model.eval(); ys=[]; ps=[]\n",
    "    for x,y in loader:\n",
    "        x=x.to(device)\n",
    "        logits=model(x)\n",
    "        ps.append(logits.argmax(1).cpu().numpy())\n",
    "        ys.append(np.array(y))\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "y_true, y_pred = preds_and_labels(model, test_loader)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f'[TEST] Accuracy: {acc:.4f}')\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred,labels=list(range(10)))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest'); plt.title(f'Confusion Matrix ({BACKBONE})'); plt.colorbar()\n",
    "plt.xticks(range(10), class_names, rotation=45); plt.yticks(range(10), class_names)\n",
    "plt.tight_layout(); plt.xlabel('Pred'); plt.ylabel('True'); plt.show()\n",
    "\n",
    "print('\\n[Classification Report]\\n', classification_report(y_true,y_pred,target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfec41",
   "metadata": {},
   "source": [
    "## 6. (ì„ íƒ) Fine-tuning â€” GPU ê¶Œì¥\n",
    "\n",
    "- ë°±ë³¸ ì¼ë¶€ ë¸”ë¡ì„ `requires_grad=True`ë¡œ í’€ì–´ì„œ ì¶”ê°€ í•™ìŠµ\n",
    "- CPUì—ì„œëŠ” ë§¤ìš° ëŠë¦¬ë¯€ë¡œ **GPUì—ì„œë§Œ ì‹¤í–‰**ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ac797",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_FINETUNE = False  # GPUë©´ Trueë¡œ ë°”ê¿” ì‹¤í–‰\n",
    "if DO_FINETUNE:\n",
    "    # ì˜ˆì‹œ: EfficientNet/MobileNetì˜ ë§ˆì§€ë§‰ stage íŒŒë¼ë¯¸í„° í’€ê¸°\n",
    "    for name, p in model.named_parameters():\n",
    "        if any(k in name for k in ['blocks.5','blocks.6','stages.3','stages.4']):\n",
    "            p.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "    FT_EPOCHS=2\n",
    "    for ep in range(1, FT_EPOCHS+1):\n",
    "        tr_l,tr_a = train_epoch(model, train_loader)\n",
    "        va_l,va_a = eval_epoch(model, val_loader)\n",
    "        print(f'[FT Ep {ep}/{FT_EPOCHS}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8010db",
   "metadata": {},
   "source": [
    "## 7. ì €ì¥/ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ba642",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{BACKBONE}_fe_cifar10_light.pt')\n",
    "print('Saved:', f'{BACKBONE}_fe_cifar10_light.pt')\n",
    "\n",
    "# ë¡œë”© ì˜ˆì‹œ\n",
    "m2 = timm.create_model(f'hf_hub:{repo_id}', pretrained=True, num_classes=10).to(device)\n",
    "# Feature extractionì¼ ë•Œ head êµ¬ì¡° ë™ì¼í•´ì•¼ í•¨\n",
    "m2.load_state_dict(torch.load(f'{BACKBONE}_fe_cifar10_light.pt', map_location=device))\n",
    "m2.eval(); print('Reload OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
