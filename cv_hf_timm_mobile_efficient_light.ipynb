{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c50ffe",
   "metadata": {},
   "source": [
    "# 🚀 2강 (가벼운 설정): **MobileNet/EfficientNet 전이학습 (timm + Hugging Face Hub)**\n",
    "\n",
    "- **목표**: 더 가벼운 백본(MobileNetV3-Small, EfficientNet-B0)으로 CPU에서도 쾌적한 전이학습 데모\n",
    "- **가정**: `timm`이 Hugging Face Hub에서 가중치를 받아옵니다(`hf_hub:` 프리픽스 사용)\n",
    "\n",
    "**실행 모드**\n",
    "- 기본: **Feature Extraction** (백본 freeze) → CPU OK\n",
    "- 옵션: **Fine-tuning** (일부 블록 unfreeze) → GPU 권장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87e6fd",
   "metadata": {},
   "source": [
    "## 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q timm\n",
    "import os, numpy as np, random, matplotlib.pyplot as plt, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets as tvdatasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import timm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30ab5a",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비 (가벼운 설정 적용)\n",
    "\n",
    "- **Subset**: 10% (CPU에서도 빠르게)\n",
    "- **Batch size**: 16\n",
    "- **Epochs**: 2 (데모용)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 로드 (PIL)\n",
    "root='./data'\n",
    "train_full = tvdatasets.CIFAR10(root=root, train=True, download=True)\n",
    "test_set   = tvdatasets.CIFAR10(root=root, train=False, download=True)\n",
    "class_names = train_full.classes\n",
    "\n",
    "def to_numpy_list(tv_dataset):\n",
    "    imgs, labs = [], []\n",
    "    for img, lab in tv_dataset:\n",
    "        imgs.append(np.array(img)); labs.append(lab)\n",
    "    return imgs, labs\n",
    "\n",
    "images_train, labels_train = to_numpy_list(train_full)\n",
    "images_test,  labels_test  = to_numpy_list(test_set)\n",
    "\n",
    "# Subset 10%\n",
    "SUBSET=0.1\n",
    "idx = np.random.RandomState(42).permutation(len(images_train))\n",
    "sel = idx[:int(len(images_train)*SUBSET)]\n",
    "images_train = [images_train[i] for i in sel]\n",
    "labels_train = [labels_train[i] for i in sel]\n",
    "\n",
    "tr_imgs, va_imgs, tr_lbls, va_lbls = train_test_split(\n",
    "    images_train, labels_train, test_size=0.2, stratify=labels_train, random_state=42\n",
    ")\n",
    "\n",
    "print('Train/Val/Test sizes:', len(tr_imgs), len(va_imgs), len(images_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd5acc",
   "metadata": {},
   "source": [
    "## 2. 변환(Transform) — 모델별 입력 규격 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm의 기본 입력 크기/전처리를 사용\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from PIL import Image\n",
    "\n",
    "# MobileNetV3-Small & EfficientNet-B0 repo IDs on HF Hub via timm\n",
    "mobilenet_repo = 'timm/mobilenetv3_small_100.lamb_in1k'\n",
    "efficient_repo = 'timm/efficientnet_b0.ra_in1k'\n",
    "\n",
    "# 임시 모델로 config 확인 (나중에 실제 모델 생성 시에도 같은 입력)\n",
    "tmp_model = timm.create_model(f'hf_hub:{mobilenet_repo}', pretrained=True, num_classes=10)\n",
    "config = resolve_data_config({}, model=tmp_model)  # model의 기본 설정으로부터 transform 생성\n",
    "transform = create_transform(**config)\n",
    "\n",
    "IMG_SIZE = config.get('input_size', (3,224,224))[-1]\n",
    "print('Resolved input size:', IMG_SIZE)\n",
    "\n",
    "class NumpyCIFARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images; self.labels = labels; self.transform = transform\n",
    "    def __len__(self): return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.fromarray(self.images[idx])\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "train_ds = NumpyCIFARDataset(tr_imgs, tr_lbls, transform=transform)\n",
    "val_ds   = NumpyCIFARDataset(va_imgs, va_lbls, transform=transform)\n",
    "test_ds  = NumpyCIFARDataset(images_test, labels_test, transform=transform)\n",
    "\n",
    "BATCH=16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f897d",
   "metadata": {},
   "source": [
    "## 3. 백본 선택 & 전이학습 설정\n",
    "- 아래 스위치로 **MobileNetV3-Small** ↔ **EfficientNet-B0** 를 간단히 교체\n",
    "- **Feature Extraction**: 백본 파라미터를 `requires_grad=False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'mobilenet'  # 'mobilenet' or 'efficientnet'\n",
    "\n",
    "if BACKBONE == 'mobilenet':\n",
    "    repo_id = mobilenet_repo\n",
    "else:\n",
    "    repo_id = efficient_repo\n",
    "\n",
    "model = timm.create_model(f'hf_hub:{repo_id}', pretrained=True, num_classes=10)\n",
    "model.to(device)\n",
    "\n",
    "# Feature Extraction: 백본 동결 (classifier/head 제외)\n",
    "for name, p in model.named_parameters():\n",
    "    if 'classifier' in name or 'fc' in name or 'head' in name:  # timm 모델별 head 이름\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "def count_trainable(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print('Trainable params:', count_trainable(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270084d",
   "metadata": {},
   "source": [
    "## 4. 학습 루프 (Feature Extraction, CPU OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train(); tl=0; tc=0; n=0\n",
    "    for x,y in loader:\n",
    "        x,y=x.to(device),torch.tensor(y).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward(); optimizer.step()\n",
    "        tl += loss.item()*x.size(0)\n",
    "        tc += (out.argmax(1)==y).sum().item(); n += x.size(0)\n",
    "    return tl/n, tc/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval(); tl=0; tc=0; n=0\n",
    "    for x,y in loader:\n",
    "        x,y=x.to(device),torch.tensor(y).to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        tl += loss.item()*x.size(0)\n",
    "        tc += (out.argmax(1)==y).sum().item(); n += x.size(0)\n",
    "    return tl/n, tc/n\n",
    "\n",
    "EPOCHS=2  # 가벼운 설정\n",
    "hist={'tr_acc':[],'va_acc':[],'tr_loss':[],'va_loss':[]}\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    tr_l,tr_a = train_epoch(model, train_loader)\n",
    "    va_l,va_a = eval_epoch(model, val_loader)\n",
    "    hist['tr_loss'].append(tr_l); hist['va_loss'].append(va_l)\n",
    "    hist['tr_acc'].append(tr_a);  hist['va_acc'].append(va_a)\n",
    "    print(f'[Ep {ep}/{EPOCHS}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a439dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선\n",
    "plt.figure(figsize=(6,4)); plt.plot(hist['tr_acc'],label='train_acc'); plt.plot(hist['va_acc'],label='val_acc')\n",
    "plt.title('Accuracy (Feature Extraction)'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "plt.figure(figsize=(6,4)); plt.plot(hist['tr_loss'],label='train_loss'); plt.plot(hist['va_loss'],label='val_loss')\n",
    "plt.title('Loss (Feature Extraction)'); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a1c49",
   "metadata": {},
   "source": [
    "## 5. 평가 + 혼동행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def preds_and_labels(model, loader):\n",
    "    model.eval(); ys=[]; ps=[]\n",
    "    for x,y in loader:\n",
    "        x=x.to(device)\n",
    "        logits=model(x)\n",
    "        ps.append(logits.argmax(1).cpu().numpy())\n",
    "        ys.append(np.array(y))\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "y_true, y_pred = preds_and_labels(model, test_loader)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f'[TEST] Accuracy: {acc:.4f}')\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred,labels=list(range(10)))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest'); plt.title(f'Confusion Matrix ({BACKBONE})'); plt.colorbar()\n",
    "plt.xticks(range(10), class_names, rotation=45); plt.yticks(range(10), class_names)\n",
    "plt.tight_layout(); plt.xlabel('Pred'); plt.ylabel('True'); plt.show()\n",
    "\n",
    "print('\\n[Classification Report]\\n', classification_report(y_true,y_pred,target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfec41",
   "metadata": {},
   "source": [
    "## 6. (선택) Fine-tuning — GPU 권장\n",
    "\n",
    "- 백본 일부 블록을 `requires_grad=True`로 풀어서 추가 학습\n",
    "- CPU에서는 매우 느리므로 **GPU에서만 실행**을 권장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ac797",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_FINETUNE = False  # GPU면 True로 바꿔 실행\n",
    "if DO_FINETUNE:\n",
    "    # 예시: EfficientNet/MobileNet의 마지막 stage 파라미터 풀기\n",
    "    for name, p in model.named_parameters():\n",
    "        if any(k in name for k in ['blocks.5','blocks.6','stages.3','stages.4']):\n",
    "            p.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "    FT_EPOCHS=2\n",
    "    for ep in range(1, FT_EPOCHS+1):\n",
    "        tr_l,tr_a = train_epoch(model, train_loader)\n",
    "        va_l,va_a = eval_epoch(model, val_loader)\n",
    "        print(f'[FT Ep {ep}/{FT_EPOCHS}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8010db",
   "metadata": {},
   "source": [
    "## 7. 저장/로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ba642",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{BACKBONE}_fe_cifar10_light.pt')\n",
    "print('Saved:', f'{BACKBONE}_fe_cifar10_light.pt')\n",
    "\n",
    "# 로딩 예시\n",
    "m2 = timm.create_model(f'hf_hub:{repo_id}', pretrained=True, num_classes=10).to(device)\n",
    "# Feature extraction일 때 head 구조 동일해야 함\n",
    "m2.load_state_dict(torch.load(f'{BACKBONE}_fe_cifar10_light.pt', map_location=device))\n",
    "m2.eval(); print('Reload OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
