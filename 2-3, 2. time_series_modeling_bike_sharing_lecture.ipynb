{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2-3, 2. ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë§ â€” íšŒê·€(ì„ í˜•/ëœë¤í¬ë ˆìŠ¤íŠ¸), ì‹œê³„ì—´ ì „ìš©(SARIMA/Prophet), ë”¥ëŸ¬ë‹(LSTM) (Bike Sharing)\n",
    "\n",
    "- **ëª©í‘œ**: íšŒê·€(ì„ í˜•/ëœë¤í¬ë ˆìŠ¤íŠ¸), ì‹œê³„ì—´ ì „ìš©(SARIMA/Prophet), ë”¥ëŸ¬ë‹(LSTM) ëª¨ë¸ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµÂ·í‰ê°€í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "- **ë°ì´í„°**: Kaggle Bike Sharing Demand (ì‹œê°„ ë‹¨ìœ„, `count` ëŒ€ìƒ)\n",
    "- **ê·œì¹™(ê°•ì˜ìš©)**: `matplotlib`ë§Œ ì‚¬ìš© (seaborn X), ìƒ‰ìƒ ì§€ì • X, ì„œë¸Œí”Œë¡¯ X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### êµ¬ì„±\n",
    "- ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ì¤‘ìš”ì„±ê³¼ í™œìš©: ìˆ˜ìš”/ì¬ê³ /ì¸ë ¥/êµí†µ/ì—ë„ˆì§€ ë“±\n",
    "- ì¼ë°˜ íšŒê·€ vs ì‹œê³„ì—´ ëª¨ë¸ ì°¨ì´: IID ê°€ì • vs ìê¸°ìƒê´€/ê³„ì ˆì„±/ì¶”ì„¸\n",
    "- ì‹¤ìŠµì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í•™ìŠµ ë° í•´ì„\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. í™˜ê²½ ì¤€ë¹„ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "- ì‹œê°í™”ëŠ” `matplotlib`ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- íšŒê·€ ëª¨ë¸(`scikit-learn`), ì‹œê³„ì—´(`statsmodels`), Prophet(ì„ íƒ)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- í•œê¸€ í°íŠ¸ì™€ ê²½ê³  ì–µì œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    _HAS_PROPHET = True\n",
    "except Exception:\n",
    "    _HAS_PROPHET = False\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Glyph.*missing from font.*\", category=UserWarning)\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • ë° ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€ (í›„ë³´êµ°ì„ sans-serif ìš°ì„ ìˆœìœ„ë¡œ ì§€ì •)\n",
    "def _set_korean_font() -> None:\n",
    "    font_candidates = [\n",
    "        \"AppleGothic\",\n",
    "        \"NanumGothic\",\n",
    "        \"Malgun Gothic\",\n",
    "        \"Noto Sans CJK KR\",\n",
    "        \"Noto Sans KR\",\n",
    "        \"DejaVu Sans\",\n",
    "    ]\n",
    "    # ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë“±ë¡í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ í•­ëª©ìœ¼ë¡œ ëŒ€ì²´ë˜ë„ë¡ í•¨\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams[\"font.sans-serif\"] = font_candidates\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "_set_korean_font()\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬\n",
    "- `bike-sharing-demand/train.csv` ë¡œë“œ, `datetime` íŒŒì‹±/ì •ë ¬\n",
    "- ì‹œê°„ íŒŒìƒë³€ìˆ˜ ìƒì„±(ë…„/ì›”/ì¼/ìš”ì¼/ì‹œê°„)\n",
    "- ì‹œê°„ ìˆœì„œëŒ€ë¡œ train/val/test ë¶„í• \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hourly_data():\n",
    "    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
    "    path = 'bike-sharing-demand/train.csv'\n",
    "    \n",
    "    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ì—ëŸ¬ ë°œìƒ\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError('train.csv ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')\n",
    "    \n",
    "    # CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # datetime ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ì¸ë±ìŠ¤ ì¬ì„¤ì •\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    # ì‚¬ìš©í•  ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "    cols = ['datetime','season','holiday','workingday','weather',\n",
    "            'temp','atemp','humidity','windspeed',\n",
    "            'casual','registered','count']\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # ì…ë ¥ëœ DataFrameì„ ë³µì‚¬í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    out = df.copy()\n",
    "    \n",
    "    # datetimeì—ì„œ íŒŒìƒëœ ì‹œê°„ ê´€ë ¨ íŠ¹ì§• ì¶”ê°€\n",
    "    out['year'] = out['datetime'].dt.year        # ì—°ë„\n",
    "    out['month'] = out['datetime'].dt.month      # ì›”\n",
    "    out['day'] = out['datetime'].dt.day          # ì¼\n",
    "    out['dayofweek'] = out['datetime'].dt.dayofweek  # ìš”ì¼ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)\n",
    "    out['hour'] = out['datetime'].dt.hour        # ì‹œ(hour)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def split_by_time(df: pd.DataFrame, train_ratio: float = 0.8, val_ratio: float = 0.1):\n",
    "    # ì „ì²´ ë°ì´í„° í¬ê¸°\n",
    "    n = len(df)\n",
    "    \n",
    "    # í›ˆë ¨/ê²€ì¦ ë°ì´í„° í¬ê¸° ê³„ì‚°\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    \n",
    "    # ì‹œê°„ ìˆœìœ¼ë¡œ train/val/test ë‚˜ëˆ„ê¸°\n",
    "    train = df.iloc[:n_train]                    # ì•ë¶€ë¶„ â†’ train\n",
    "    val = df.iloc[n_train:n_train+n_val]         # ì¤‘ê°„ â†’ validation\n",
    "    test = df.iloc[n_train+n_val:]               # ë‚˜ë¨¸ì§€ â†’ test\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# --- ì‹¤í–‰ ë¶€ë¶„ ---\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = load_hourly_data()\n",
    "print('ë°ì´í„° í¬ê¸°:', df.shape, 'ê¸°ê°„:', df['datetime'].min(), 'â†’', df['datetime'].max())\n",
    "\n",
    "# ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€\n",
    "df_feat = add_time_features(df)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ë³´ê°„ (ì•/ë’¤ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°)\n",
    "df = df.ffill().bfill()\n",
    "\n",
    "# train, validation, test ë°ì´í„° ë¶„í• \n",
    "train_df, val_df, test_df = split_by_time(df_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. ì¼ë°˜ íšŒê·€ ë² ì´ìŠ¤ë¼ì¸: ì„ í˜• íšŒê·€ vs ëœë¤í¬ë ˆìŠ¤íŠ¸\n",
    "- ì…ë ¥: ì‹œê°„ íŒŒìƒ + ê¸°ìƒ/ìƒíƒœ ë³€ìˆ˜, íƒ€ê¹ƒ: `count`\n",
    "- í‰ê°€: MAE/MSE/RMSE/R^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised_features(df: pd.DataFrame, target: str = 'count'):\n",
    "    # ì‚¬ìš©í•  ê¸°ë³¸ íŠ¹ì§• ì»¬ëŸ¼ ì •ì˜\n",
    "    feature_cols = [\n",
    "        'season','holiday','workingday','weather',\n",
    "        'temp','atemp','humidity','windspeed',\n",
    "        'year','month','day','dayofweek','hour'\n",
    "    ]\n",
    "    \n",
    "    # íŠ¹ì§• ë³€ìˆ˜(X) ì„ íƒ\n",
    "    X = df[feature_cols].copy()\n",
    "    \n",
    "    # ë²”ì£¼í˜• ë³€ìˆ˜(season, holiday, workingday, weather, year, month, dayofweek, hour)ë¥¼ ì›-í•« ì¸ì½”ë”©\n",
    "    # drop_first=False : ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ë„ ìœ ì§€ (ëª¨ë“  ë”ë¯¸ ë³€ìˆ˜ë¥¼ í¬í•¨)\n",
    "    X = pd.get_dummies(\n",
    "        X, \n",
    "        columns=['season','holiday','workingday','weather','year','month','dayofweek','hour'], \n",
    "        drop_first=False\n",
    "    )\n",
    "    \n",
    "    # íƒ€ê²Ÿ ë³€ìˆ˜(y) ì„ íƒ (floatí˜•ìœ¼ë¡œ ë³€í™˜)\n",
    "    y = df[target].astype(float)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))  # í‰ê·  ì ˆëŒ€ ì˜¤ì°¨\n",
    "    mse  = float(mean_squared_error(y_true, y_pred))   # í‰ê·  ì œê³± ì˜¤ì°¨\n",
    "    rmse = float(np.sqrt(mse))                         # RMSE = MSEì˜ ì œê³±ê·¼\n",
    "    r2   = r2_score(y_true, y_pred)                    # ê²°ì •ê³„ìˆ˜ (ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨)\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"[Metrics]\")\n",
    "    print(f\"MAE  : {mae:.6f} - í‰ê·  ì ˆëŒ€ ì˜¤ì°¨\")\n",
    "    print(f\"MSE  : {mse:.6f} - í‰ê·  ì œê³± ì˜¤ì°¨\")\n",
    "    print(f\"RMSE : {rmse:.6f} - ì œê³±ê·¼ í‰ê·  ì œê³± ì˜¤ì°¨\")\n",
    "    print(f\"R^2  : {r2:.6f} - ê²°ì •ê³„ìˆ˜(ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨)\")\n",
    "    \n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "\n",
    "def plot_actual_vs_pred(dt_index, y_true, y_pred, title):\n",
    "    # ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‹œê³„ì—´ ë¹„êµ ì‹œê°í™”\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(dt_index, y_true, label='Actual')   # ì‹¤ì œê°’\n",
    "    plt.plot(dt_index, y_pred, label='Pred')     # ì˜ˆì¸¡ê°’\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Validation / Test ë°ì´í„°ë³„ íŠ¹ì§• ìƒì„±\n",
    "# -----------------------------\n",
    "X_train, y_train = to_supervised_features(train_df)  # í•™ìŠµ ë°ì´í„°\n",
    "X_val, y_val     = to_supervised_features(val_df)    # ê²€ì¦ ë°ì´í„°\n",
    "X_test, y_test   = to_supervised_features(test_df)   # í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì—ì„œ ìƒì„±ëœ ì»¬ëŸ¼ êµ¬ì¡°ì— ë§ì¶”ì–´ Validation/Test ë°ì´í„° ë³´ì •\n",
    "# â†’ í•™ìŠµ ë•Œ ë‚˜ì˜¨ ì»¬ëŸ¼ê³¼ Validation/Test ì»¬ëŸ¼ì´ ë™ì¼í•´ì•¼ í•¨\n",
    "# â†’ ì—†ëŠ” ì»¬ëŸ¼ì€ fill_value=0 ìœ¼ë¡œ ì±„ì›€\n",
    "X_val  = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ì„ í˜• íšŒê·€ (Linear Regression)\n",
    "# -----------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ (train ë°ì´í„°ë¡œ ì í•©)\n",
    "lin = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡\n",
    "lin_val_pred  = lin.predict(X_val)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "lin_test_pred = lin.predict(X_test)\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€ (ê²€ì¦/í…ŒìŠ¤íŠ¸)\n",
    "print('Linear/VAL', compute_metrics(y_val.to_numpy(), lin_val_pred))   # ê²€ì¦ ë°ì´í„° ì§€í‘œ\n",
    "print('Linear/TEST', compute_metrics(y_test.to_numpy(), lin_test_pred)) # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì§€í‘œ\n",
    "\n",
    "# ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‹œê°í™” (ê²€ì¦)\n",
    "plot_actual_vs_pred(\n",
    "    val_df['datetime'].values, y_val.to_numpy(), lin_val_pred, \n",
    "    'ì„ í˜•íšŒê·€ ê²€ì¦ ì˜ˆì¸¡ vs ì‹¤ì œ'\n",
    ")\n",
    "\n",
    "# ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‹œê°í™” (í…ŒìŠ¤íŠ¸)\n",
    "plot_actual_vs_pred(\n",
    "    test_df['datetime'].values, y_test.to_numpy(), lin_test_pred, \n",
    "    'ì„ í˜•íšŒê·€ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ'\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ (RandomForestRegressor)\n",
    "# -----------------------------\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,   # íŠ¸ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ê°’ë³´ë‹¤ í¬ê²Œ ì„¤ì • â†’ ì„±ëŠ¥ ì•ˆì •í™”)\n",
    "    n_jobs=-1,          # CPU ì½”ì–´ ëª¨ë‘ ì‚¬ìš©\n",
    "    random_state=42     # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡\n",
    "rf_val_pred = rf.predict(X_val)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€ (ê²€ì¦/í…ŒìŠ¤íŠ¸)\n",
    "print('RF/VAL', compute_metrics(y_val.to_numpy(), rf_val_pred))   # ê²€ì¦ ë°ì´í„° ì§€í‘œ\n",
    "print('RF/TEST', compute_metrics(y_test.to_numpy(), rf_test_pred)) # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì§€í‘œ\n",
    "\n",
    "# ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‹œê°í™” (ê²€ì¦)\n",
    "plot_actual_vs_pred(\n",
    "    val_df['datetime'].values, y_val.to_numpy(), rf_val_pred, \n",
    "    'ëœë¤í¬ë ˆìŠ¤íŠ¸ ê²€ì¦ ì˜ˆì¸¡ vs ì‹¤ì œ'\n",
    ")\n",
    "\n",
    "# ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‹œê°í™” (í…ŒìŠ¤íŠ¸)\n",
    "plot_actual_vs_pred(\n",
    "    test_df['datetime'].values, y_test.to_numpy(), rf_test_pred, \n",
    "    'ëœë¤í¬ë ˆìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ'\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ëœë¤ í¬ë ˆìŠ¤íŠ¸ íŠ¹ì§• ì¤‘ìš”ë„ í™•ì¸\n",
    "# -----------------------------\n",
    "try:\n",
    "    # feature_importances_ ì†ì„±ì„ ì´ìš©í•´ ë³€ìˆ˜ ì¤‘ìš”ë„ ê³„ì‚°\n",
    "    importances = (\n",
    "        pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    # ìƒìœ„ 15ê°œ íŠ¹ì§• ì¶œë ¥\n",
    "    print('RF ì¤‘ìš”ë„ TOP 15:\\n', importances.head(15))\n",
    "except Exception:\n",
    "    # ë§Œì•½ feature_importances_ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‹œ\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” & ì‹œê³„ì—´ êµì°¨ê²€ì¦\n",
    "- `TimeSeriesSplit`ì„ ì´ìš©í•œ êµì°¨ê²€ì¦\n",
    "- ëœë¤í¬ë ˆìŠ¤íŠ¸ ê°„ë‹¨ Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# 0) ì „ì²´ í”¼ì²˜/íƒ€ê¹ƒ ìƒì„±\n",
    "# -----------------------------\n",
    "X_all_ts, y_all_ts = to_supervised_features(df_feat)  # ì „ì²´ ë°ì´í„°ì—ì„œ íŠ¹ì§•(X), íƒ€ê¹ƒ(y) ìƒì„±\n",
    "\n",
    "# (ì„ íƒ) ì¸ë±ìŠ¤ê°€ ì„ì—¬ ìˆë‹¤ë©´ ì‹œê°„ìˆœ ì •ë ¬ í•„ìš”\n",
    "# X_all_ts = X_all_ts.sort_index()\n",
    "# y_all_ts = y_all_ts.loc[X_all_ts.index]\n",
    "\n",
    "# -----------------------------\n",
    "# 1) í™€ë“œì•„ì›ƒ ë¶„í•  (ë§ˆì§€ë§‰ 20%ë¥¼ ìµœì¢… í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë³´ì¡´)\n",
    "# -----------------------------\n",
    "split_idx = int(len(X_all_ts) * 0.8)   # 80% ì‹œì  ê¸°ì¤€ ë¶„í• \n",
    "\n",
    "# êµì°¨ê²€ì¦(CV)ì— ì‚¬ìš©í•  êµ¬ê°„\n",
    "X_cv_ts,  y_cv_ts   = X_all_ts.iloc[:split_idx], y_all_ts.iloc[:split_idx]\n",
    "\n",
    "# ìµœì¢… í…ŒìŠ¤íŠ¸ êµ¬ê°„ (í•™ìŠµ/íŠœë‹ì— ì‚¬ìš©í•˜ì§€ ì•ŠìŒ â†’ ì™„ì „ unseen data)\n",
    "X_test_ts, y_test_ts = X_all_ts.iloc[split_idx:], y_all_ts.iloc[split_idx:]\n",
    "\n",
    "print(f\"[SHAPE] X_cv_ts={X_cv_ts.shape}, y_cv_ts={y_cv_ts.shape}, \"\n",
    "      f\"X_test_ts={X_test_ts.shape}, y_test_ts={y_test_ts.shape}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) ì‹œê³„ì—´ êµì°¨ê²€ì¦ ë¶„í• ê¸° (TimeSeriesSplit)\n",
    "# -----------------------------\n",
    "# ì¼ë°˜ KFoldì™€ ë‹¬ë¦¬, ë¯¸ë˜ ë°ì´í„°ë¥¼ í•™ìŠµì— ì“°ì§€ ì•Šë„ë¡ ì‹œê°„ ìˆœì„œë¥¼ ë³´ì¡´í•˜ëŠ” ë°©ì‹\n",
    "tscv_cv = TimeSeriesSplit(n_splits=5)  # 5ê²¹ ì‹œê³„ì—´ êµì°¨ê²€ì¦\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ & GridSearchCV ê°ì²´ ìƒì„±\n",
    "# -----------------------------\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [200, 300],   # íŠ¸ë¦¬ ê°œìˆ˜\n",
    "    \"max_depth\": [None, 12, 24]   # íŠ¸ë¦¬ ê¹Šì´\n",
    "}\n",
    "\n",
    "# ëœë¤í¬ë ˆìŠ¤íŠ¸ ê¸°ë³¸ ëª¨ë¸\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "rf_gs_cv = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=tscv_cv,                          # ì‹œê³„ì—´ ë¶„í• ê¸°ë¡œ êµì°¨ê²€ì¦\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE (scikit-learnì€ ìŒìˆ˜ë¡œ ë°˜í™˜)\n",
    "    n_jobs=-1                            # ë³‘ë ¬ ì²˜ë¦¬\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) CV êµ¬ê°„ ë°ì´í„°ë¡œ ê·¸ë¦¬ë“œì„œì¹˜ ì‹¤í–‰\n",
    "# -----------------------------\n",
    "rf_gs_cv.fit(X_cv_ts, y_cv_ts)\n",
    "\n",
    "# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° êµì°¨ê²€ì¦ RMSE ì¶œë ¥\n",
    "print('ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°(CV):', rf_gs_cv.best_params_)\n",
    "print('CV RMSE:', -rf_gs_cv.best_score_)  # neg RMSE â†’ ë¶€í˜¸ ë°˜ì „\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) ìµœì  ëª¨ë¸ì„ CV êµ¬ê°„ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ\n",
    "# -----------------------------\n",
    "rf_best_cv_ts = rf_gs_cv.best_estimator_\n",
    "rf_best_cv_ts.fit(X_cv_ts, y_cv_ts)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) ìµœì¢… í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ (í™€ë“œì•„ì›ƒ)\n",
    "# -----------------------------\n",
    "y_pred_test_ts = rf_best_cv_ts.predict(X_test_ts)\n",
    "\n",
    "# í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "mae_ts  = mean_absolute_error(y_test_ts, y_pred_test_ts)\n",
    "mse_ts  = mean_squared_error(y_test_ts, y_pred_test_ts)\n",
    "rmse_ts = np.sqrt(mse_ts)\n",
    "r2_ts   = r2_score(y_test_ts, y_pred_test_ts)\n",
    "\n",
    "print(\"\\n[TEST Metrics - Time Series Holdout]\")\n",
    "print(f\"MAE  : {mae_ts:.3f}\")\n",
    "print(f\"MSE  : {mse_ts:.3f}\")\n",
    "print(f\"RMSE : {rmse_ts:.3f}\")\n",
    "print(f\"R^2  : {r2_ts:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries_split(n_samples=30, n_splits=5):\n",
    "    \"\"\"\n",
    "    TimeSeriesSplitì´ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë¶„í• í•˜ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Parameters%\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        ì „ì²´ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (ì˜ˆ: len(X_all))\n",
    "    n_splits : int\n",
    "        TimeSeriesSplitì˜ ë¶„í•  ê°œìˆ˜ (fold ìˆ˜)\n",
    "    \"\"\"\n",
    "    # ìƒ˜í”Œ ì¸ë±ìŠ¤ (0 ~ n_samples-1 ê¹Œì§€)\n",
    "    X = range(n_samples)\n",
    "    \n",
    "    # ì‹œê³„ì—´ êµì°¨ê²€ì¦ ë¶„í• ê¸° ìƒì„±\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # ê·¸ë˜í”„ í¬ê¸° ì§€ì •\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # TimeSeriesSplitì—ì„œ foldë³„ train/test ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "        # ğŸ”¹ Train ë¶€ë¶„ (íŒŒë€ìƒ‰ ì›)\n",
    "        plt.scatter(\n",
    "            train_idx,                # í›ˆë ¨ ì¸ë±ìŠ¤\n",
    "            [fold]*len(train_idx),    # yì¶•ì€ fold ë²ˆí˜¸ë¡œ ê³ ì •\n",
    "            marker='o', color='blue', s=40,\n",
    "            label='Train' if fold == 1 else \"\"  # ì²« ë²ˆì§¸ foldë§Œ ë²”ë¡€ ì¶”ê°€\n",
    "        )\n",
    "        \n",
    "        # ğŸ”¹ Test ë¶€ë¶„ (ë¹¨ê°„ìƒ‰ ì›)\n",
    "        plt.scatter(\n",
    "            test_idx, \n",
    "            [fold]*len(test_idx), \n",
    "            marker='o', color='red', s=40,\n",
    "            label='Test' if fold == 1 else \"\"   # ì²« ë²ˆì§¸ foldë§Œ ë²”ë¡€ ì¶”ê°€\n",
    "        )\n",
    "\n",
    "    # yì¶•: Fold ë²ˆí˜¸ í‘œì‹œ\n",
    "    plt.yticks(range(1, n_splits+1), [f\"Fold {i}\" for i in range(1, n_splits+1)])\n",
    "    \n",
    "    # xì¶•/ yì¶• ë¼ë²¨\n",
    "    plt.xlabel(\"Index (ì‹œê°„ ìˆœì„œ)\")\n",
    "    plt.ylabel(\"CV Fold\")\n",
    "    \n",
    "    # ê·¸ë˜í”„ ì œëª©\n",
    "    plt.title(f\"TimeSeriesSplit ì‹œê°í™” (ìƒ˜í”Œ {n_samples}, ë¶„í•  {n_splits})\")\n",
    "    \n",
    "    # ë²”ë¡€ ë° ê·¸ë¦¬ë“œ í‘œì‹œ\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # ì¶œë ¥\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•  ê²½ìš°\n",
    "plot_timeseries_split(n_samples=X_all_ts.shape[0], n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. ì•™ìƒë¸” ëª¨ë¸ë§ (ê°€ì¤‘ í‰ê· )\n",
    "- ì„ í˜•íšŒê·€ì™€ ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ ê²€ì¦ ì˜ˆì¸¡ì„ ê°€ì¤‘ í‰ê· \n",
    "- ê²€ì¦ ì„±ëŠ¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì„ íƒ í›„ í…ŒìŠ¤íŠ¸ í‰ê°€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ensemble(y1, y2, w):\n",
    "    \"\"\"\n",
    "    ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ ì•™ìƒë¸”\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y1 : array-like\n",
    "        ì²« ë²ˆì§¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ (ì˜ˆ: ì„ í˜•íšŒê·€)\n",
    "    y2 : array-like\n",
    "        ë‘ ë²ˆì§¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ (ì˜ˆ: ëœë¤í¬ë ˆìŠ¤íŠ¸)\n",
    "    w : float\n",
    "        ì²« ë²ˆì§¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ (0~1 ì‚¬ì´ ê°’)\n",
    "        ë‘ ë²ˆì§¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ìë™ìœ¼ë¡œ (1 - w)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        ê°€ì¤‘ í‰ê· ëœ ì˜ˆì¸¡ê°’\n",
    "    \"\"\"\n",
    "    return w*y1 + (1-w)*y2\n",
    "\n",
    "\n",
    "def search_weight_and_report(lin_val_pred, rf_val_pred, y_val,\n",
    "                             lin_test_pred, rf_test_pred, y_test,\n",
    "                             weights=np.linspace(0, 1, 21),\n",
    "                             title_prefix='ì•™ìƒë¸”'):\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ì¡°í•©ìœ¼ë¡œ ê²€ì¦ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ ,\n",
    "    ìµœì  ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ê¹Œì§€ ë³´ê³ í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lin_val_pred, rf_val_pred : array-like\n",
    "        ê²€ì¦ì…‹ì—ì„œ ì„ í˜•íšŒê·€/ëœë¤í¬ë ˆìŠ¤íŠ¸ ì˜ˆì¸¡ê°’\n",
    "    y_val : Series or array-like\n",
    "        ê²€ì¦ì…‹ ì‹¤ì œê°’\n",
    "    lin_test_pred, rf_test_pred : array-like\n",
    "        í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ì„ í˜•íšŒê·€/ëœë¤í¬ë ˆìŠ¤íŠ¸ ì˜ˆì¸¡ê°’\n",
    "    y_test : Series or array-like\n",
    "        í…ŒìŠ¤íŠ¸ì…‹ ì‹¤ì œê°’\n",
    "    weights : array-like\n",
    "        íƒìƒ‰í•  ê°€ì¤‘ì¹˜ í›„ë³´ ë¦¬ìŠ¤íŠ¸ (0~1)\n",
    "    title_prefix : str\n",
    "        ì‹œê°í™” ë° ì¶œë ¥ ì œëª© prefix\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    best_w, best_rmse = None, float('inf')\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) ê²€ì¦ ë°ì´í„°ì—ì„œ ê°€ì¤‘ì¹˜ë³„ ì„±ëŠ¥ ì¸¡ì •\n",
    "    # -----------------------------\n",
    "    for w in weights:\n",
    "        # í˜„ì¬ ê°€ì¤‘ì¹˜ ì¡°í•©ìœ¼ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡\n",
    "        ens_val = weighted_ensemble(lin_val_pred, rf_val_pred, w)\n",
    "        \n",
    "        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "        mae, mse, rmse, r2 = compute_metrics(y_val.to_numpy(), ens_val)\n",
    "        \n",
    "        # ê¸°ë¡ ì €ì¥\n",
    "        rows.append({\n",
    "            'weight_linear': float(w),\n",
    "            'weight_rf': float(1-w),\n",
    "            'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2\n",
    "        })\n",
    "        \n",
    "        # ìµœì  ê°€ì¤‘ì¹˜ ê°±ì‹  (RMSE ìµœì†Œ ê¸°ì¤€)\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_w = w\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) DataFrame ì •ë¦¬\n",
    "    # -----------------------------\n",
    "    df = pd.DataFrame(rows)\n",
    "    df_sorted = df.sort_values('weight_linear').reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) ê°€ì¤‘ì¹˜ vs RMSE ê³¡ì„  ì‹œê°í™”\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    plt.plot(df_sorted['weight_linear'], df_sorted['RMSE'], marker='o')\n",
    "    \n",
    "    # ìµœì  ì§€ì  í‘œì‹œ\n",
    "    plt.scatter([best_w], [best_rmse], s=80, color=\"red\")\n",
    "    plt.title(f'{title_prefix} (ê²€ì¦) â€” ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ RMSE')\n",
    "    plt.xlabel('ì„ í˜•íšŒê·€ ê°€ì¤‘ì¹˜ w  (ëœë¤í¬ë ˆìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ = 1 - w)')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) ê²€ì¦ ê²°ê³¼ ìƒìœ„ 5ê°œ (RMSE ê¸°ì¤€)\n",
    "    # -----------------------------\n",
    "    topk = df.nsmallest(5, 'RMSE').copy()\n",
    "    topk['linear%'] = (topk['weight_linear']*100).round(0).astype(int)\n",
    "    topk['rf%'] = (topk['weight_rf']*100).round(0).astype(int)\n",
    "    \n",
    "    print('\\n[ê²€ì¦ ê²°ê³¼ ìƒìœ„ 5ê°œ (RMSE ê¸°ì¤€)]')\n",
    "    print(\n",
    "        topk[['weight_linear', 'weight_rf', 'linear%', 'rf%', 'MAE', 'RMSE', 'R2']]\n",
    "        .rename(columns={'weight_linear':'w(linear)','weight_rf':'w(rf)'})\n",
    "        .to_string(index=False, float_format=lambda x: f'{x:,.3f}')\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) ìµœì  ê°€ì¤‘ì¹˜ë¡œ í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥ í‰ê°€\n",
    "    # -----------------------------\n",
    "    ens_test = weighted_ensemble(lin_test_pred, rf_test_pred, best_w)\n",
    "    mae_t, mse_t, rmse_t, r2_t = compute_metrics(y_test.to_numpy(), ens_test)\n",
    "\n",
    "    print('\\n[ìš”ì•½]')\n",
    "    print(f\"- ìµœì  ê°€ì¤‘ì¹˜ w = {best_w:.2f}  \"\n",
    "          f\"(ì„ í˜•íšŒê·€ {best_w*100:.0f}%, ëœë¤í¬ë ˆìŠ¤íŠ¸ {(1-best_w)*100:.0f}%)\")\n",
    "    print(f\"- ê²€ì¦ RMSE = {best_rmse:.3f}\")\n",
    "    print(f\"- í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ â†’ MAE={mae_t:.3f}, RMSE={rmse_t:.3f}, R^2={r2_t:.3f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) í…ŒìŠ¤íŠ¸ì…‹ ì‹¤ì œê°’ vs ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ì‹œê°í™”\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        plot_actual_vs_pred(\n",
    "            test_df['datetime'].values,   # ê¸°ì¡´ ì½”ë“œì˜ test_df í™œìš©\n",
    "            y_test.to_numpy(),\n",
    "            ens_test,\n",
    "            f'{title_prefix} í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ (w={best_w:.2f})'\n",
    "        )\n",
    "    except NameError:\n",
    "        # test_dfê°€ ì™¸ë¶€ì—ì„œ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„\n",
    "        pass\n",
    "\n",
    "    return best_w, df_sorted\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "# ---------------------------\n",
    "best_w, df_weights = search_weight_and_report(\n",
    "    lin_val_pred, rf_val_pred, y_val,\n",
    "    lin_test_pred, rf_test_pred, y_test,\n",
    "    weights=np.linspace(0, 1, 21),  # 0.00 ~ 1.00 (0.05 ê°„ê²©)\n",
    "    title_prefix='ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. ì”ì°¨ ë¶„ì„\n",
    "- í…ŒìŠ¤íŠ¸ êµ¬ê°„ ì”ì°¨ ì‹œê°í™” ë° ë¶„í¬ í™•ì¸\n",
    "- íŒ¨í„´/ì´ë¶„ì‚°/ë¹„ì •ìƒì„± ì—¬ë¶€ ì ê²€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) ì”ì°¨ ê³„ì‚°\n",
    "# -----------------------------\n",
    "# ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’ = ì”ì°¨(residual)\n",
    "residual = y_test.to_numpy() - rf_test_pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) ì”ì°¨ ì‹œê³„ì—´ í”Œë¡¯\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,3.5))\n",
    "plt.plot(test_df['datetime'].values, residual)   # xì¶•: ì‹œê°„(datetime), yì¶•: ì”ì°¨\n",
    "plt.title('ì”ì°¨ ì‹œê³„ì—´ (RF)')                   # ì œëª©\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í•´ì„: ì‹œê°„ì— ë”°ë¼ ì”ì°¨ê°€ ì¼ì • íŒ¨í„´ì„ ë³´ì´ëŠ”ì§€ í™•ì¸\n",
    "#       (ëœë¤í•œ ë¶„í¬ë¼ë©´ ëª¨ë¸ì´ ì‹œê³„ì—´ êµ¬ì¡°ë¥¼ ì˜ ì„¤ëª…í•˜ê³  ìˆë‹¤ëŠ” ì˜ë¯¸)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) ì”ì°¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(residual, bins=30)                     # êµ¬ê°„ì„ 30ê°œë¡œ ë‚˜ëˆ  ë¶„í¬ í™•ì¸\n",
    "plt.title('ì”ì°¨ ë¶„í¬ (RF)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í•´ì„: ì”ì°¨ê°€ ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´ ì¢… ëª¨ì–‘(Bell-shape)ì´ë©´ ëª¨ë¸ ì í•©ì´ ì–‘í˜¸í•¨\n",
    "#       í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì³ ìˆê±°ë‚˜ ê¸´ ê¼¬ë¦¬ê°€ ìˆìœ¼ë©´ í¸í–¥(bias) ë˜ëŠ” ê³¼ëŒ€/ê³¼ì†Œ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ìˆìŒ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. ì”ì°¨ ë¶„ì„ ê²°ê³¼ í•´ì„ (Random Forest)\n",
    "\n",
    "#### 1). ì”ì°¨ ì‹œê³„ì—´ (Residual Time Series)\n",
    "- ì‹œê°„ ì¶•ì— ë”°ë¼ ì˜ˆì¸¡ ì˜¤ì°¨(ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)ë¥¼ í‘œì‹œí•œ ê·¸ë˜í”„  \n",
    "- **íŠ¹ì§•**:\n",
    "  - ëŒ€ë¶€ë¶„ ì”ì°¨ê°€ 0 ê·¼ì²˜ì— ë¶„í¬í•˜ì§€ë§Œ, êµ¬ê°„ë³„ë¡œ ë­‰ì¹¨ í˜„ìƒì´ ë³´ì„\n",
    "  - ì¼ë¶€ ì‹œì ì—ì„œ í° ì–‘ìˆ˜(ê³¼ì†Œì˜ˆì¸¡) ë˜ëŠ” ìŒìˆ˜(ê³¼ëŒ€ì˜ˆì¸¡) í”¼í¬ ë°œìƒ\n",
    "  - ì™„ì „íˆ ë¬´ì‘ìœ„(white noise) í˜•íƒœê°€ ì•„ë‹ˆë©°, ì‹œê°„ì— ë”°ë¥¸ êµ¬ì¡°ì  íŒ¨í„´ì´ ë‚¨ì•„ ìˆìŒ  \n",
    "\n",
    " **í•´ì„**:  \n",
    "ëª¨ë¸ì´ í‰ê· ì ìœ¼ë¡œëŠ” ì˜ ë§ì¶”ì§€ë§Œ, **íŠ¹ì • ì‹œì (ê¸‰ë“±/ê¸‰ë½ êµ¬ê°„ ë“±)**ì—ì„œëŠ” ì˜ˆì¸¡ë ¥ì´ ë–¨ì–´ì§€ë©°,  \n",
    "ì´ëŠ” ëª¨ë¸ì´ **ê³„ì ˆì„±/ì¶”ì„¸ ê°™ì€ ì¼ë¶€ íŒ¨í„´ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í–ˆìŒì„ ì‹œì‚¬**í•¨.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) ì”ì°¨ ë¶„í¬ (Residual Histogram)\n",
    "- ì”ì°¨ ê°’ì˜ ë¹ˆë„ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„  \n",
    "- **íŠ¹ì§•**:\n",
    "  - ì¤‘ì‹¬ì´ 0 ê·¼ì²˜ì— ìˆìœ¼ë©°, ëŒ€ì²´ë¡œ ì¢Œìš° ëŒ€ì¹­ì— ê°€ê¹Œìš´ í˜•íƒœ\n",
    "  - ë‹¤ë§Œ ì˜¤ë¥¸ìª½ ê¼¬ë¦¬(ì–‘ì˜ ì”ì°¨)ê°€ ì¡°ê¸ˆ ë” ê¸¸ì–´ ì‹¤ì œë³´ë‹¤ ì‘ê²Œ ì˜ˆì¸¡í•˜ëŠ” ê²½í–¥ì´ ì¼ë¶€ ì¡´ì¬\n",
    "  - ëŒ€ë¶€ë¶„ ì”ì°¨ëŠ” -100 ~ +200 êµ¬ê°„ì— ë¶„í¬, ì†Œìˆ˜ì˜ ê·¹ë‹¨ê°’(Â±200~400) ì¡´ì¬  \n",
    "\n",
    " **í•´ì„**:  \n",
    "ì”ì°¨ ë¶„í¬ëŠ” ëŒ€ì²´ë¡œ ì •ê·œë¶„í¬ì— ê°€ê¹Œì›Œ **í° í¸í–¥ì€ ì—†ì§€ë§Œ**,  \n",
    "ì•½ê°„ì˜ ì˜¤ë¥¸ìª½ ë¹„ëŒ€ì¹­ì„±ìœ¼ë¡œ ì¸í•´ **ê³¼ì†Œì˜ˆì¸¡ ê²½í–¥**ì´ í™•ì¸ë¨.  \n",
    "ë˜í•œ ì¼ë¶€ **ì´ìƒì¹˜(Outlier)** ì˜¤ì°¨ê°€ ì¡´ì¬í•˜ì—¬ ê°œì„  ì—¬ì§€ê°€ ìˆìŒ.\n",
    "\n",
    "---\n",
    "\n",
    "#### ì¢…í•© ê²°ë¡ \n",
    "- RF(RandomForest) ëª¨ë¸ì€ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ì–‘í˜¸í•˜ë‚˜,  \n",
    "  1. ì”ì°¨ ì‹œê³„ì—´ì—ì„œ **ì‹œê°„ êµ¬ì¡°ì  íŒ¨í„´**ì´ ë‚¨ì•„ ìˆìŒ  \n",
    "  2. ì”ì°¨ ë¶„í¬ì—ì„œ **ê³¼ì†Œì˜ˆì¸¡ ê²½í–¥ ë° ì´ìƒì¹˜**ê°€ í™•ì¸ë¨  \n",
    "\n",
    " **ê°œì„  ë°©í–¥**:\n",
    "- ì‹œê³„ì—´ íŠ¹ì„±ì„ ë°˜ì˜í•œ í”¼ì²˜ ì¶”ê°€ (ìš”ì¼, ì£¼ê¸°ì„±, Lag, Rolling Mean ë“±)\n",
    "- ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜(XGBoost, LightGBM, LSTM ë“±) ë¹„êµ ì ìš©\n",
    "- ì”ì°¨ ê¸°ë°˜ ì¶”ê°€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì§„í–‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. ì‹œê³„ì—´ ì „ìš© ëª¨ë¸: SARIMA, Prophet(ì„ íƒ)\n",
    "- ì¼ ë‹¨ìœ„ í‰ê· ìœ¼ë¡œ ì§‘ê³„ í›„ ì˜ˆì¸¡\n",
    "- SARIMA ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ì‹œì—°, Prophetì€ ì„¤ì¹˜ ì‹œ ì‹¤í–‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_daily(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    ì‹œê°„ ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì¼ ë‹¨ìœ„ë¡œ ì§‘ê³„í•˜ëŠ” í•¨ìˆ˜\n",
    "    - datetimeì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\n",
    "    - count ì»¬ëŸ¼ì„ ì¼ë³„ í‰ê· ìœ¼ë¡œ ì§‘ê³„\n",
    "    \"\"\"\n",
    "    return df.set_index('datetime')['count'].resample('D').mean()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) ì¼ ë‹¨ìœ„ ë°ì´í„° ì¤€ë¹„\n",
    "# -----------------------------\n",
    "daily = aggregate_daily(df)\n",
    "\n",
    "# ë¹ˆ ë‚ ì§œ ì±„ìš°ê¸° + ë³´ê°„ ì²˜ë¦¬\n",
    "daily = (\n",
    "    daily.asfreq('D')        # ëª¨ë“  ë‚ ì§œë¥¼ ì—°ì†ì ìœ¼ë¡œ ì±„ì›€ (ê²°ì¸¡ì¹˜ ë°œìƒ)\n",
    "         .interpolate()      # ì„ í˜• ë³´ê°„ (ì—°ì†ëœ ê°’ ë³´ì •)\n",
    "         .ffill().bfill()    # ì•/ë’¤ ê°’ ë³´ê°„ìœ¼ë¡œ ì”ì—¬ ê²°ì¸¡ì¹˜ ë³´ì •\n",
    ")\n",
    "\n",
    "# train/val/test ë¶„í•  (80% / 10% / 10%)\n",
    "N = len(daily)\n",
    "N_tr, N_va = int(N*0.8), int(N*0.1)\n",
    "daily_tr = daily.iloc[:N_tr]\n",
    "daily_va = daily.iloc[N_tr:N_tr+N_va]\n",
    "daily_te = daily.iloc[N_tr+N_va:]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) SARIMA ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡\n",
    "# -----------------------------\n",
    "# (1,1,1) ARIMA + (1,1,1,7) ì£¼ê°„ ê³„ì ˆì„± ê³ ë ¤\n",
    "sar = SARIMAX(\n",
    "    daily_tr,\n",
    "    order=(1,1,1),\n",
    "    seasonal_order=(1,1,1,7),\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ").fit(disp=False)\n",
    "\n",
    "# ê²€ì¦ êµ¬ê°„ ì˜ˆì¸¡\n",
    "va_fc = sar.forecast(steps=len(daily_va)).to_numpy()\n",
    "\n",
    "# í•™ìŠµ+ê²€ì¦ ë°ì´í„° ì „ì²´ë¥¼ í™œìš©í•˜ì—¬ test êµ¬ê°„ ì˜ˆì¸¡\n",
    "combined = pd.concat([daily_tr, daily_va])\n",
    "te_fc = (\n",
    "    SARIMAX(\n",
    "        combined,\n",
    "        order=(1,1,1),\n",
    "        seasonal_order=(1,1,1,7),\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    .fit(disp=False)\n",
    "    .forecast(steps=len(daily_te))\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€ ì¶œë ¥\n",
    "print('SARIMA/VAL', compute_metrics(daily_va.to_numpy(), va_fc))\n",
    "print('SARIMA/TEST', compute_metrics(daily_te.to_numpy(), te_fc))\n",
    "\n",
    "# ì‹¤ì œ vs ì˜ˆì¸¡ ì‹œê°í™”\n",
    "plot_actual_vs_pred(daily_va.index, daily_va.to_numpy(), va_fc, 'SARIMA ê²€ì¦ ì˜ˆì¸¡ vs ì‹¤ì œ (ì¼)')\n",
    "plot_actual_vs_pred(daily_te.index, daily_te.to_numpy(), te_fc, 'SARIMA í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ (ì¼)')\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Prophet ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡ (ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸)\n",
    "# -----------------------------\n",
    "if _HAS_PROPHET:\n",
    "    # Prophetì€ ds, y ì»¬ëŸ¼ í•„ìš”\n",
    "    dprop = pd.DataFrame({'ds': daily_tr.index, 'y': daily_tr.values})\n",
    "    \n",
    "    # Prophet ëª¨ë¸ ìƒì„± (ì£¼ê°„ seasonalityë§Œ ì‚¬ìš©)\n",
    "    m = Prophet(\n",
    "        seasonality_mode='additive',\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        yearly_seasonality=False\n",
    "    )\n",
    "    m.fit(dprop)\n",
    "    \n",
    "    # ê²€ì¦ êµ¬ê°„ ê¸¸ì´ë§Œí¼ ë¯¸ë˜ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    future = pd.DataFrame({\n",
    "        'ds': pd.date_range(\n",
    "            start=daily_tr.index[-1] + pd.Timedelta(days=1),\n",
    "            periods=len(daily_va),\n",
    "            freq='D'\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    yhat_val = m.predict(future)['yhat'].to_numpy()\n",
    "    \n",
    "    # ì„±ëŠ¥ í‰ê°€\n",
    "    print('Prophet/VAL', compute_metrics(daily_va.to_numpy(), yhat_val))\n",
    "\n",
    "else:\n",
    "    print('Prophet ë¯¸ì„¤ì¹˜: ê±´ë„ˆëœ€')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. ì™œ SARIMAì™€ Prophetì€ ì„±ëŠ¥ì´ ë‚®ì„ê¹Œ?\n",
    "\n",
    "#### 1) ë°ì´í„° íŠ¹ì„±ê³¼ì˜ ë¶€ì í•©ì„±\n",
    "- SARIMA/Prophetì€ **ì¶”ì„¸ + ê³„ì ˆì„±** íŒ¨í„´ì´ ëšœë ·í•˜ê³  ë¹„êµì  ì•ˆì •ì ì¸ ì‹œê³„ì—´ ë°ì´í„°ì— ê°•ì ì´ ìˆìŒ.  \n",
    "- í•˜ì§€ë§Œ ìì „ê±° ëŒ€ì—¬ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì„±ì´ ì¡´ì¬:\n",
    "  - **ì™¸ë¶€ ìš”ì¸**(ê¸°ì˜¨, ìŠµë„, ë‚ ì”¨, ìš”ì¼, ê·¼ë¬´ì¼ ì—¬ë¶€, ì‹œê°„ëŒ€ ë“±)ì— í¬ê²Œ ì˜ì¡´  \n",
    "  - ë‚ ì”¨ ë³€í™”ë‚˜ íœ´ì¼ ì´ë²¤íŠ¸ ë“±ìœ¼ë¡œ **ê¸‰ê²©í•œ ìˆ˜ìš” ë³€ë™(ìŠ¤íŒŒì´í¬)** ë°œìƒ  \n",
    "- ë”°ë¼ì„œ ë‹¨ìˆœíˆ ê³¼ê±° ì‹œê³„ì—´ íŒ¨í„´ë§Œ ë³´ëŠ” SARIMA/Prophetì€ ì´ë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•¨.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2) ëª¨ë¸ì˜ êµ¬ì¡°ì  í•œê³„\n",
    "##### SARIMA\n",
    "- ìê¸°íšŒê·€(AR) + ì°¨ë¶„(I) + ì´ë™í‰ê· (MA) + ê³„ì ˆì„±(S)ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™.  \n",
    "- **ë‹¨ì¼/ê³ ì •ëœ ê³„ì ˆì„±**ì—ëŠ” ì˜ ë§ì§€ë§Œ, ìì „ê±° ëŒ€ì—¬ ë°ì´í„°ì²˜ëŸ¼ **í•˜ë£¨ ì£¼ê¸° + ì£¼ê°„ ì£¼ê¸° + ë‚ ì”¨ ì˜í–¥**ì´ ë™ì‹œì— ì¡´ì¬í•˜ëŠ” ê²½ìš° í•œê³„ê°€ ìˆìŒ.  \n",
    "- íŠ¹íˆ ì‹œê°„ëŒ€ë³„ ë³€ë™ì„±ì´ í° íŒ¨í„´ì„ í¬ì°©í•˜ê¸° ì–´ë ¤ì›€.  \n",
    "\n",
    "##### Prophet\n",
    "- êµ¬ì¡°: **íŠ¸ë Œë“œ + ì£¼ê°„/ì—°ê°„ ê³„ì ˆì„± + íœ´ì¼ íš¨ê³¼**  \n",
    "- ì¥ì : íœ´ì¼ íš¨ê³¼ ë°˜ì˜, ë¹„ì„ í˜• íŠ¸ë Œë“œ ì²˜ë¦¬  \n",
    "- ë‹¨ì : **ë‹¨ê¸° ë³€ë™ì„±**ê³¼ **ë¹„ì„ í˜• ì™¸ë¶€ ë³€ìˆ˜ íš¨ê³¼**(ì˜ˆ: íŠ¹ì • ê¸°ì˜¨ êµ¬ê°„ì—ì„œ ìˆ˜ìš” ê¸‰ì¦)ë¥¼ ë°˜ì˜í•˜ê¸° ì–´ë ¤ì›€.  \n",
    "- ì´ë²ˆ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ê¸°ìƒÂ·ì‹œê°„ ìš”ì¸ì„ ì§ì ‘ ë°˜ì˜í•  ìˆ˜ ì—†ì—ˆìŒ.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3) ë°ì´í„° ì „ì²˜ë¦¬/ë¶„í•´ í•œê³„\n",
    "- ì‹¤ì œ ë°ì´í„°ì—ëŠ” ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ê·¹ë‹¨ì  ì´ë²¤íŠ¸(í­ìš°, í­ì„¤ ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆìŒ.  \n",
    "- SARIMA/Prophetì€ ì´ëŸ° ì´ìƒ ë³€ë™ì„ **ì¶”ì„¸/ê³„ì ˆì„±ìœ¼ë¡œ ì˜ëª» í¡ìˆ˜**í•˜ì—¬ ì˜¤ì°¨ê°€ ì»¤ì§.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4) ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ\n",
    "- **Linear Regression, RandomForest** ë“± ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ **ì™¸ë¶€ í”¼ì²˜**ë¥¼ ì§ì ‘ í™œìš© ê°€ëŠ¥:\n",
    "  - ê¸°ì˜¨ì´ ë†’ì„ ë•Œ ìˆ˜ìš” ì¦ê°€  \n",
    "  - í‡´ê·¼ ì‹œê°„ëŒ€(17~19ì‹œ) ìˆ˜ìš” í­ì¦  \n",
    "- ë”°ë¼ì„œ ë¹„ì„ í˜•Â·ë³µí•©ì ì¸ ìš”ì¸ì„ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì´ í–¥ìƒë¨.  \n",
    "- ë°˜ë©´ SARIMA/Prophetì€ **ì™¸ë¶€ ìš”ì¸ ì—†ì´ ì˜¤ì§ ê³¼ê±° ìˆ˜ìš” ë°ì´í„°ë§Œ** ì‚¬ìš© â†’ ì„±ëŠ¥ì´ ë‚®ê²Œ ë‚˜íƒ€ë‚¨.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ê²°ë¡ \n",
    "- SARIMA/Prophetì˜ ë‚®ì€ ì„±ëŠ¥ ì´ìœ :\n",
    "  1. **ì™¸ë¶€ ìš”ì¸(ë‚ ì”¨Â·ìš”ì¼Â·ì‹œê°„ëŒ€)ì„ ë°˜ì˜í•˜ì§€ ëª»í•¨**  \n",
    "  2. **ê¸‰ê²©í•œ ìˆ˜ìš” ë³€ë™(ìŠ¤íŒŒì´í¬)ì— ì·¨ì•½**  \n",
    "  3. **ë‹¤ì¤‘ ê³„ì ˆì„±ê³¼ ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ ì¶©ë¶„íˆ ì„¤ëª…í•˜ì§€ ëª»í•¨**  \n",
    "\n",
    "ì´ ë°ì´í„°ì—ì„œëŠ” **ëœë¤í¬ë ˆìŠ¤íŠ¸ ê°™ì€ í”¼ì²˜ ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸**ì´ í›¨ì”¬ ë” ì í•©í•˜ë©°,  \n",
    "ì¶”ê°€ë¡œ Gradient Boosting ê³„ì—´(XGBoost, LightGBM)ì´ë‚˜ ì‹œê³„ì—´ íŠ¹í™” ë”¥ëŸ¬ë‹(LSTM ë“±)ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± ë° LSTM ëª¨ë¸ ì •ì˜\n",
    "- ìœˆë„ìš° í¬ê¸° 24(í•˜ë£¨)ë¡œ ì‹œí€€ìŠ¤ êµ¬ì„±, horizon=1\n",
    "- ë‹¨ìˆœ LSTM íšŒê·€ í—¤ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def _set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ ì¬í˜„ì„±(reproducibility)ì„ í™•ë³´í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int, default=42\n",
    "        ë‚œìˆ˜ ì‹œë“œ ê°’ (ê³ ì •í•  ìˆ«ì)\n",
    "    \n",
    "    ì„¤ëª…\n",
    "    ----\n",
    "    - np.random.seed(seed): NumPy ë‚œìˆ˜ ì‹œë“œ ê³ ì •\n",
    "    - torch.manual_seed(seed): CPU ì—°ì‚°ìš© PyTorch ë‚œìˆ˜ ì‹œë“œ ê³ ì •\n",
    "    - torch.cuda.manual_seed_all(seed): GPU ì—°ì‚°ìš© PyTorch ë‚œìˆ˜ ì‹œë“œ ê³ ì •\n",
    "      (ë©€í‹° GPU í™˜ê²½ì—ì„œë„ ë™ì¼í•œ ì‹œë“œê°€ ì ìš©ë¨)\n",
    "    \"\"\"\n",
    "    # NumPy ë‚œìˆ˜ ì‹œë“œ ê³ ì •\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch CPU ë‚œìˆ˜ ì‹œë“œ ê³ ì •\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # PyTorch GPU ë‚œìˆ˜ ì‹œë“œ ê³ ì • (ëª¨ë“  GPUì— ì ìš©)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    DataFrameì—ì„œ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  íŠ¹ì§• í–‰ë ¬(X)ê³¼ íƒ€ê¹ƒ ë²¡í„°(y)ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        ì…ë ¥ ë°ì´í„° (datetime í¬í•¨, bike sharing demand ë°ì´í„°ì…‹)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (shape = [n_samples, n_features])\n",
    "        íŠ¹ì§• í–‰ë ¬ (float32 íƒ€ì…)\n",
    "    y : np.ndarray (shape = [n_samples])\n",
    "        íƒ€ê¹ƒ ë²¡í„° (count, float32 íƒ€ì…)\n",
    "    feature_cols : list of str\n",
    "        ì‚¬ìš©ëœ íŠ¹ì§• ë³€ìˆ˜ë“¤ì˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ì‚¬ìš©í•  í”¼ì²˜ ì»¬ëŸ¼ ì •ì˜\n",
    "    feature_cols = [\n",
    "        'temp','atemp','humidity','windspeed',  # ê¸°ìƒ ê´€ë ¨\n",
    "        'season','holiday','workingday','weather',  # ì¹´í…Œê³ ë¦¬/ìƒí™© ë³€ìˆ˜\n",
    "        'year','month','dayofweek','hour'      # ì‹œê°„ ê´€ë ¨ íŒŒìƒ ë³€ìˆ˜\n",
    "    ]\n",
    "    \n",
    "    # X: íŠ¹ì§• í–‰ë ¬ (NumPy ë°°ì—´, float32ë¡œ ë³€í™˜ â†’ PyTorch/TF í•™ìŠµ í˜¸í™˜)\n",
    "    X = df[feature_cols].to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # y: íƒ€ê¹ƒ ë²¡í„° (ëŒ€ì—¬ ìˆ˜ìš” count, float32ë¡œ ë³€í™˜)\n",
    "    y = df['count'].astype(np.float32).to_numpy()\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(X: np.ndarray, y: np.ndarray, window: int, horizon: int = 1):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ ë°ì´í„°ë¥¼ (ì…ë ¥ ì‹œí€€ìŠ¤, ì˜ˆì¸¡ ëŒ€ìƒ ì‹œí€€ìŠ¤) ìœˆë„ìš° í˜•íƒœë¡œ ë³€í™˜\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape = [n_samples, n_features]\n",
    "        ì…ë ¥ íŠ¹ì§• ë°ì´í„° (ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ìƒíƒœ)\n",
    "    y : np.ndarray, shape = [n_samples]\n",
    "        íƒ€ê¹ƒ ë²¡í„° (ì˜ˆ: count)\n",
    "    window : int\n",
    "        ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ (ê³¼ê±° ê´€ì¸¡ì¹˜ ê°œìˆ˜)\n",
    "    horizon : int, default=1\n",
    "        ì˜ˆì¸¡í•  ë¯¸ë˜ ì‹œì ì˜ ê°œìˆ˜ (1ì´ë©´ ë‹¤ìŒ ì‹œì ë§Œ ì˜ˆì¸¡)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xs : np.ndarray, shape = [n_windows, window, n_features]\n",
    "        ëª¨ë¸ ì…ë ¥ (ê³¼ê±° window ê¸¸ì´ë§Œí¼ì˜ ì‹œí€€ìŠ¤)\n",
    "    ys : np.ndarray, shape = [n_windows, horizon]\n",
    "        ëª¨ë¸ ì¶œë ¥ (horizon ê¸¸ì´ë§Œí¼ì˜ ë¯¸ë˜ ê°’)\n",
    "    \n",
    "    ì˜ˆì‹œ\n",
    "    ----\n",
    "    X = [[x1], [x2], [x3], [x4], [x5]], y = [y1, y2, y3, y4, y5]\n",
    "    window=2, horizon=1 â†’ \n",
    "        xs = [[[x1],[x2]], [[x2],[x3]], [[x3],[x4]]]\n",
    "        ys = [[y3], [y4], [y5]]\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # i ìœ„ì¹˜ë¶€í„° window ê¸¸ì´ ë§Œí¼ Xë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ ,\n",
    "    # ê·¸ ë‹¤ìŒ horizon ê¸¸ì´ ë§Œí¼ yë¥¼ ì˜ˆì¸¡ ëŒ€ìƒìœ¼ë¡œ ì„¤ì •\n",
    "    for i in range(len(X) - window - horizon + 1):\n",
    "        xs.append(X[i:i+window])                    # ì…ë ¥ êµ¬ê°„ (window ê¸¸ì´)\n",
    "        ys.append(y[i+window:i+window+horizon])     # ì¶œë ¥ êµ¬ê°„ (horizon ê¸¸ì´)\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ (float32 â†’ PyTorch í˜¸í™˜)\n",
    "    return np.asarray(xs, dtype=np.float32), np.asarray(ys, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸\n",
    "- í‘œì¤€í™”: train ê¸°ì¤€ í‰ê· /í‘œì¤€í¸ì°¨\n",
    "- í•™ìŠµ: Adam + MSE, ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ\n",
    "- ì˜ˆì¸¡ vs ì‹¤ì œ ì‹œê°í™”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# LSTM + Linear head ì¡°í•©ì„ ì‰½ê²Œ ìƒì„±í•˜ëŠ” í—¬í¼\n",
    "# -----------------------------------------\n",
    "def make_lstm_pair(input_size: int, hidden_size: int = 64, num_layers: int = 2, dropout: float = 0.1, device=None):\n",
    "    \"\"\"\n",
    "    LSTM + Linear ì¡°í•© ëª¨ë¸ì„ ê°„ë‹¨íˆ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        ì…ë ¥ íŠ¹ì§•(feature) ê°œìˆ˜\n",
    "    hidden_size : int, default=64\n",
    "        LSTM ì€ë‹‰ ìƒíƒœ(hidden state) ì°¨ì› í¬ê¸°\n",
    "    num_layers : int, default=2\n",
    "        LSTM ë ˆì´ì–´(ì¸µ) ê°œìˆ˜ (ìŠ¤íƒ êµ¬ì¡°)\n",
    "    dropout : float, default=0.1\n",
    "        LSTM ë ˆì´ì–´ ì‚¬ì´ì— ì ìš©í•  ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "    device : torch.device or None\n",
    "        ëª¨ë¸ì„ ì˜¬ë¦´ ì¥ì¹˜ (Noneì´ë©´ GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ ìë™ ì„ íƒ)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lstm : nn.LSTM\n",
    "        ì‹œê³„ì—´ íŠ¹ì§• ì¶”ì¶œìš© LSTM ë„¤íŠ¸ì›Œí¬\n",
    "    head : nn.Linear\n",
    "        ë§ˆì§€ë§‰ hidden stateë¥¼ ë°›ì•„ ìµœì¢… ì˜ˆì¸¡ì„ ì¶œë ¥í•˜ëŠ” Linear ë ˆì´ì–´\n",
    "    device : torch.device\n",
    "        ëª¨ë¸ì´ ì˜¬ë¼ê°„ ì¥ì¹˜ ì •ë³´\n",
    "    \"\"\"\n",
    "    # í•™ìŠµ ì¥ì¹˜ ì„ íƒ (GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ cuda, ì•„ë‹ˆë©´ cpu)\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # LSTM ì •ì˜\n",
    "    lstm = nn.LSTM(\n",
    "        input_size=input_size,   # ì…ë ¥ íŠ¹ì§• ì°¨ì›\n",
    "        hidden_size=hidden_size, # ì€ë‹‰ ìƒíƒœ ì°¨ì›\n",
    "        num_layers=num_layers,   # LSTM ì¸µ ê°œìˆ˜\n",
    "        batch_first=True,        # ì…ë ¥ shape = (batch, seq_len, input_size)\n",
    "        dropout=dropout          # ë ˆì´ì–´ ì‚¬ì´ dropout ì ìš©\n",
    "    ).to(device)\n",
    "    \n",
    "    # LSTMì˜ ë§ˆì§€ë§‰ hidden state â†’ ìµœì¢… 1ì°¨ì› ì¶œë ¥ê°’ìœ¼ë¡œ ë§¤í•‘\n",
    "    head = nn.Linear(hidden_size, 1).to(device)\n",
    "    \n",
    "    return lstm, head, device\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# LSTM forward í•¨ìˆ˜\n",
    "# -----------------------------------------\n",
    "def lstm_forward(lstm: nn.LSTM, head: nn.Linear, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    LSTM + Linear head ì¡°í•©ì˜ forward íŒ¨ìŠ¤\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lstm : nn.LSTM\n",
    "        LSTM ë„¤íŠ¸ì›Œí¬\n",
    "    head : nn.Linear\n",
    "        ìµœì¢… ì¶œë ¥ ë ˆì´ì–´\n",
    "    x : torch.Tensor, shape = (batch, seq_len, input_size)\n",
    "        ì…ë ¥ ì‹œí€€ìŠ¤\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor, shape = (batch,)\n",
    "        ì˜ˆì¸¡ëœ ì¶œë ¥ê°’ (ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€)\n",
    "    \"\"\"\n",
    "    # LSTM í†µê³¼ â†’ out: (batch, seq_len, hidden_size)\n",
    "    out, _ = lstm(x)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì‹œì ì˜ hidden state ì„ íƒ (seq_lenì˜ ë§ˆì§€ë§‰ ìœ„ì¹˜)\n",
    "    last = out[:, -1, :]   # shape = (batch, hidden_size)\n",
    "    \n",
    "    # Linear head í†µê³¼ â†’ ìµœì¢… ì¶œë ¥\n",
    "    return head(last).squeeze(-1)   # shape = (batch,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# í•™ìŠµ ë£¨í”„ (í´ë˜ìŠ¤ ë²„ì „ì´ ì•„ë‹Œ í•¨ìˆ˜ ë²„ì „)\n",
    "# -----------------------------\n",
    "def train_model(model, train_loader, val_loader, epochs: int, lr: float, device):\n",
    "    \"\"\"\n",
    "    ì¼ë°˜ nn.Module ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜\n",
    "    - MSELoss, Adam, ReduceLROnPlateau, EarlyStopping í¬í•¨\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, factor=0.5, patience=3)\n",
    "    \n",
    "    patience = 8           # ì¡°ê¸° ì¢…ë£Œ patience\n",
    "    no_improve = 0         # ê°œì„  ì—†ëŠ” epoch ì¹´ìš´í„°\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    \n",
    "    for ep in range(1, epochs+1):\n",
    "        # ----- í•™ìŠµ ë‹¨ê³„ -----\n",
    "        model.train()\n",
    "        tr_sum, n = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "            optim.step()\n",
    "            \n",
    "            tr_sum += float(loss.item()) * len(xb)\n",
    "            n += len(xb)\n",
    "        tr_loss = tr_sum / max(n, 1)\n",
    "\n",
    "        # ----- ê²€ì¦ ë‹¨ê³„ -----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            va_sum, n2 = 0.0, 0\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                va_sum += float(loss.item()) * len(xb)\n",
    "                n2 += len(xb)\n",
    "        va_loss = va_sum / max(n2, 1)\n",
    "        \n",
    "        # ë¡œê·¸ ì¶œë ¥\n",
    "        print(f'Epoch {ep:03d} - train MSE: {tr_loss:.4f}, val MSE: {va_loss:.4f}')\n",
    "        \n",
    "        # LR scheduler\n",
    "        scheduler.step(va_loss)\n",
    "        \n",
    "        # Early stopping ì²´í¬\n",
    "        if va_loss + 1e-9 < best_val:\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f'[EarlyStop] epoch={ep} val={va_loss:.5f}')\n",
    "                break\n",
    "        \n",
    "        # Best ëª¨ë¸ ì €ì¥\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    # í•™ìŠµ ì¢…ë£Œ í›„ best ê°€ì¤‘ì¹˜ ë³µì›\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "# -----------------------------\n",
    "def predict_all(model, loader, device):\n",
    "    \"\"\"\n",
    "    ì „ì²´ DataLoaderì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ í›„ numpy ë°°ì—´ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb)\n",
    "            outs.append(pred.cpu().numpy())\n",
    "    return np.concatenate(outs, axis=0)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ì‹œê°í™” í•¨ìˆ˜\n",
    "# -----------------------------\n",
    "def plot_series(dt_index, y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    ì‹œê³„ì—´ ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ê·¸ë˜í”„\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(dt_index, y_true, label='Actual')\n",
    "    plt.plot(dt_index, y_pred, label='Pred')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ë°ì´í„° êµ¬ì„± ë‹¨ê³„\n",
    "# -----------------------------\n",
    "# ê°œì„  3ê°€ì§€:\n",
    "# (1) ì£¼ê°„ ì°½(window=168)\n",
    "# (2) ì‹œê°„ ì›í˜• ì¸ì½”ë”©(hour_sin, hour_cos)\n",
    "# (3) ë¡œê·¸ ë³€í™˜ íƒ€ê¹ƒ\n",
    "def add_cyclical_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out['hour_sin'] = np.sin(2*np.pi*out['hour']/24)  # ì£¼ê¸°ì„± ë³´ì¡´\n",
    "    out['hour_cos'] = np.cos(2*np.pi*out['hour']/24)\n",
    "    return out\n",
    "\n",
    "# ì‹œê°„ ì›í˜• ì¸ì½”ë”© ì¶”ê°€\n",
    "train_df = add_cyclical_cols(train_df)\n",
    "val_df   = add_cyclical_cols(val_df)\n",
    "test_df  = add_cyclical_cols(test_df)\n",
    "\n",
    "# X, y ìƒì„±\n",
    "X_train, y_train, feat_cols = build_feature_matrix(train_df)\n",
    "X_val,   y_val,   _         = build_feature_matrix(val_df)\n",
    "X_test,  y_test,  _         = build_feature_matrix(test_df)\n",
    "\n",
    "# Xì— hour_sin/cos ì´ì–´ë¶™ì´ê¸°\n",
    "def append_cols(X: np.ndarray, df: pd.DataFrame, cols: list[str]) -> np.ndarray:\n",
    "    return np.concatenate([X, df[cols].to_numpy(dtype=np.float32)], axis=1)\n",
    "\n",
    "X_train = append_cols(X_train, train_df, ['hour_sin','hour_cos'])\n",
    "X_val   = append_cols(X_val,   val_df,   ['hour_sin','hour_cos'])\n",
    "X_test  = append_cols(X_test,  test_df,  ['hour_sin','hour_cos'])\n",
    "\n",
    "# í‘œì¤€í™” (train ê¸°ì¤€)\n",
    "mean, std = X_train.mean(axis=0, keepdims=True), X_train.std(axis=0, keepdims=True) + 1e-8\n",
    "X_train, X_val, X_test = (X_train-mean)/std, (X_val-mean)/std, (X_test-mean)/std\n",
    "\n",
    "# ë¡œê·¸ íƒ€ê¹ƒ ì ìš© (ì •ê·œì„± ê°œì„ )\n",
    "use_log_target = True\n",
    "if use_log_target:\n",
    "    y_train, y_val, y_test = np.log1p(y_train), np.log1p(y_val), np.log1p(y_test)\n",
    "\n",
    "# -----------------------------\n",
    "# ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (LSTM ì…ë ¥ìš©)\n",
    "# -----------------------------\n",
    "window = 168  # 1ì£¼ì¼ = 24*7\n",
    "Xtr_seq, ytr_seq = make_windows(X_train, y_train, window)\n",
    "Xva_seq, yva_seq = make_windows(X_val,   y_val,   window)\n",
    "Xte_seq, yte_seq = make_windows(X_test,  y_test,  window)\n",
    "\n",
    "# íƒ€ê¹ƒ ì°¨ì› ì •ë¦¬\n",
    "ytr_1d = ytr_seq.squeeze(-1) if ytr_seq.ndim > 1 else ytr_seq\n",
    "yva_1d = yva_seq.squeeze(-1) if yva_seq.ndim > 1 else yva_seq\n",
    "yte_1d = yte_seq.squeeze(-1) if yte_seq.ndim > 1 else yte_seq\n",
    "\n",
    "# Tensor ë³€í™˜\n",
    "a2_Xtr, a2_ytr = torch.as_tensor(Xtr_seq, dtype=torch.float32), torch.as_tensor(ytr_1d, dtype=torch.float32)\n",
    "a2_Xva, a2_yva = torch.as_tensor(Xva_seq, dtype=torch.float32), torch.as_tensor(yva_1d, dtype=torch.float32)\n",
    "a2_Xte, a2_yte = torch.as_tensor(Xte_seq, dtype=torch.float32), torch.as_tensor(yte_1d, dtype=torch.float32)\n",
    "\n",
    "# (x, y) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ê¸°\n",
    "train_data, val_data, test_data = list(zip(a2_Xtr, a2_ytr)), list(zip(a2_Xva, a2_yva)), list(zip(a2_Xte, a2_yte))\n",
    "\n",
    "# DataLoader êµ¬ì„±\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=False, drop_last=True)\n",
    "val_loader   = DataLoader(val_data,   batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_data,  batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM + Linear í•™ìŠµ\n",
    "# -----------------------------\n",
    "_set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# LSTM + Linear ìƒì„±\n",
    "input_size = Xtr_seq.shape[-1]\n",
    "lstm_nc, head_nc, _ = make_lstm_pair(input_size=input_size, hidden_size=64, num_layers=2, dropout=0.1, device=device)\n",
    "\n",
    "# ìµœì í™” ì„¸íŒ…\n",
    "params_nc = list(lstm_nc.parameters()) + list(head_nc.parameters())\n",
    "optim_nc = torch.optim.Adam(params_nc, lr=1e-3)\n",
    "crit_nc = nn.MSELoss()\n",
    "scheduler_nc = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_nc, factor=0.5, patience=3)\n",
    "\n",
    "# EarlyStopping ë³€ìˆ˜\n",
    "best_val, best_state, patience, no_improve = float('inf'), None, 8, 0\n",
    "\n",
    "# -----------------------------\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "# -----------------------------\n",
    "for ep in range(1, 21):\n",
    "    # ----- í•™ìŠµ -----\n",
    "    lstm_nc.train(); head_nc.train()\n",
    "    tr_sum, n = 0.0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = lstm_forward(lstm_nc, head_nc, xb)\n",
    "        loss = crit_nc(pred, yb)\n",
    "        optim_nc.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params_nc, max_norm=1.0)\n",
    "        optim_nc.step()\n",
    "        tr_sum += float(loss.item()) * len(xb); n += len(xb)\n",
    "    tr_loss = tr_sum / max(n, 1)\n",
    "\n",
    "    # ----- ê²€ì¦ -----\n",
    "    lstm_nc.eval(); head_nc.eval()\n",
    "    with torch.no_grad():\n",
    "        va_sum, n2 = 0.0, 0\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = lstm_forward(lstm_nc, head_nc, xb)\n",
    "            loss = crit_nc(pred, yb)\n",
    "            va_sum += float(loss.item()) * len(xb); n2 += len(xb)\n",
    "    va_loss = va_sum / max(n2, 1)\n",
    "\n",
    "    print(f'Epoch {ep:03d} - train MSE: {tr_loss:.4f}, val MSE: {va_loss:.4f}')\n",
    "    scheduler_nc.step(va_loss)\n",
    "\n",
    "    # EarlyStopping ì²´í¬\n",
    "    if va_loss + 1e-9 < best_val: no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f'[EarlyStop] epoch={ep} val={va_loss:.5f}'); break\n",
    "\n",
    "    # Best ëª¨ë¸ ì €ì¥\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        best_state = {\n",
    "            'lstm': {k: v.detach().cpu().clone() for k,v in lstm_nc.state_dict().items()},\n",
    "            'head': {k: v.detach().cpu().clone() for k,v in head_nc.state_dict().items()},\n",
    "        }\n",
    "\n",
    "# í•™ìŠµ ì¢…ë£Œ í›„ best ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "if best_state is not None:\n",
    "    lstm_nc.load_state_dict(best_state['lstm'])\n",
    "    head_nc.load_state_dict(best_state['head'])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ì˜ˆì¸¡\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    lstm_nc.eval(); head_nc.eval()\n",
    "    val_pred = np.concatenate([lstm_forward(lstm_nc, head_nc, xb.to(device)).cpu().numpy() for xb,_ in val_loader], axis=0)\n",
    "    test_pred = np.concatenate([lstm_forward(lstm_nc, head_nc, xb.to(device)).cpu().numpy() for xb,_ in test_loader], axis=0)\n",
    "\n",
    "val_idx, test_idx = val_df['datetime'].iloc[window:].values, test_df['datetime'].iloc[window:].values\n",
    "\n",
    "# ë¡œê·¸ ë³€í™˜ ë³µì› í›„ ì‹œê°í™”\n",
    "if 'use_log_target' in globals() and use_log_target:\n",
    "    plot_series(val_idx, np.expm1(yva_seq.squeeze(-1)), np.expm1(val_pred), 'LSTM ê²€ì¦ ì˜ˆì¸¡ vs ì‹¤ì œ')\n",
    "    plot_series(test_idx, np.expm1(yte_seq.squeeze(-1)), np.expm1(test_pred), 'LSTM í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ')\n",
    "else:\n",
    "    plot_series(val_idx, yva_seq.squeeze(-1), val_pred, 'LSTM ê²€ì¦ ì˜ˆì¸¡ vs ì‹¤ì œ')\n",
    "    plot_series(test_idx, yte_seq.squeeze(-1), test_pred, 'LSTM í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. ì„±ëŠ¥ í‰ê°€ ë° ì”ì°¨ ë¶„ì„\n",
    "- MAE/MSE/RMSE/R^2\n",
    "- ì˜ˆì¸¡ vs ì‹¤ì œ í”Œë¡¯, ì”ì°¨ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ë¡œê·¸ ë³€í™˜ëœ íƒ€ê¹ƒì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›\n",
    "# -----------------------------\n",
    "if 'use_log_target' in globals() and use_log_target:\n",
    "    # ë¡œê·¸ ë³€í™˜ ì „ ê°’ìœ¼ë¡œ ë³µì› (expm1 = exp(x) - 1)\n",
    "    yva_true = np.expm1(yva_seq.squeeze(-1))   # ê²€ì¦ ì‹¤ì œê°’\n",
    "    yte_true = np.expm1(yte_seq.squeeze(-1))   # í…ŒìŠ¤íŠ¸ ì‹¤ì œê°’\n",
    "    val_pred_eval = np.expm1(val_pred)         # ê²€ì¦ ì˜ˆì¸¡ê°’\n",
    "    test_pred_eval = np.expm1(test_pred)       # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’\n",
    "else:\n",
    "    # ë¡œê·¸ ë³€í™˜ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    yva_true = yva_seq.squeeze(-1)\n",
    "    yte_true = yte_seq.squeeze(-1)\n",
    "    val_pred_eval = val_pred\n",
    "    test_pred_eval = test_pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ì„±ëŠ¥ í‰ê°€ (ê²€ì¦/í…ŒìŠ¤íŠ¸)\n",
    "# -----------------------------\n",
    "print('LSTM/VAL', compute_metrics(yva_true, val_pred_eval))   # ê²€ì¦ ì„±ëŠ¥\n",
    "print('LSTM/TEST', compute_metrics(yte_true, test_pred_eval)) # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥\n",
    "# â†’ MAE, MSE, RMSE, RÂ² ì¶œë ¥\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ì”ì°¨ ë¶„ì„ (Residual Analysis)\n",
    "# -----------------------------\n",
    "# ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’ = ì”ì°¨\n",
    "residual = yte_true - test_pred_eval\n",
    "\n",
    "# 1) ì”ì°¨ ì‹œê³„ì—´ í”Œë¡¯\n",
    "plt.figure(figsize=(12,3.5))\n",
    "plt.plot(test_idx, residual)\n",
    "plt.title('ì”ì°¨ ì‹œê³„ì—´ (LSTM)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# â†’ ì‹œê°„ì— ë”°ë¼ ì”ì°¨ê°€ íŒ¨í„´ ì—†ì´ ëœë¤í•˜ê²Œ ë¶„í¬í•˜ë©´ ëª¨ë¸ ì í•©ì´ ì–‘í˜¸\n",
    "# â†’ íŠ¹ì • êµ¬ê°„ì—ì„œ í¸í–¥ì´ ë‚˜íƒ€ë‚˜ë©´ í•´ë‹¹ ì‹œì  íŒ¨í„´ì„ ëª¨ë¸ì´ í•™ìŠµ ëª»í–ˆì„ ê°€ëŠ¥ì„±\n",
    "\n",
    "# 2) ì”ì°¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(residual, bins=30)\n",
    "plt.title('ì”ì°¨ ë¶„í¬ (LSTM)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# â†’ ì”ì°¨ê°€ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´ ì¢… ëª¨ì–‘ì´ë©´ ì´ìƒì \n",
    "# â†’ ì¹˜ìš°ì¹¨(skew)ì´ë‚˜ ê¼¬ë¦¬(tail)ê°€ ê¸¸ë©´ íŠ¹ì • êµ¬ê°„ì—ì„œ ì˜ˆì¸¡ ì˜¤ì°¨ê°€ í¼\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4 LSTM ì”ì°¨ ë¶„ì„ ê²°ê³¼\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) ì”ì°¨ ì‹œê³„ì—´ (ì²« ë²ˆì§¸ ê·¸ë˜í”„)  \n",
    "- ì”ì°¨ê°€ **0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìœ„ì•„ë˜ë¡œ ìš”ë™**í•¨.  \n",
    "- íŠ¹ì • ì‹œì (ì˜ˆ: 11ì›” ì´ˆ, 12ì›” ì´ˆ)ì— **ì˜ˆì¸¡ ì˜¤ì°¨ê°€ í¬ê²Œ íŠ€ëŠ” êµ¬ê°„** ì¡´ì¬ â†’ ëª¨ë¸ì´ íŠ¹ìˆ˜ ì´ë²¤íŠ¸(ë‚ ì”¨ ê¸‰ë³€, íœ´ì¼, í”„ë¡œëª¨ì…˜ ë“±)ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í–ˆì„ ê°€ëŠ¥ì„±.  \n",
    "- ì”ì°¨ê°€ ì™„ì „íˆ ëœë¤í•˜ì§€ ì•Šê³  êµ¬ê°„ë³„ë¡œ ë¬¶ì—¬ ë³´ì„ â†’ ì‹œê³„ì—´ì  íŒ¨í„´(ì£¼ê°„/ìš”ì¼ íš¨ê³¼ ë“±)ì„ ì¶©ë¶„íˆ í•™ìŠµí•˜ì§€ ëª»í–ˆìŒì„ ì‹œì‚¬.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2) ì”ì°¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (ë‘ ë²ˆì§¸ ê·¸ë˜í”„)  \n",
    "- **0ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì¢Œìš° ëŒ€ì¹­ ì¢… ëª¨ì–‘(ì •ê·œë¶„í¬í˜•)**ì— ê°€ê¹Œì›€.  \n",
    "- ê¼¬ë¦¬ê°€ ë‹¤ì†Œ ë‘êº¼ì›€(Â±200 ì´ìƒ êµ¬ê°„ì—ë„ ì”ì°¨ ì¡´ì¬) â†’ íŠ¹ì • extreme outlier ìƒí™©ì—ì„œ í° ì˜¤ì°¨ ë°œìƒ.  \n",
    "- ì¤‘ì•™ì— ë°ì´í„°ê°€ ëª°ë ¤ ìˆìŒ â†’ ëŒ€ë¶€ë¶„ êµ¬ê°„ì—ì„œëŠ” ì˜ˆì¸¡ ì •í™•ë„ê°€ ê´œì°®ìŒ.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3) ì¢…í•© í•´ì„  \n",
    "- **ì¥ì **  \n",
    "  - í‰ê· ì ìœ¼ë¡œëŠ” ì˜ˆì¸¡ì„ ì˜í•¨ (ì”ì°¨ê°€ 0 ê·¼ì²˜ì— ì§‘ì¤‘).  \n",
    "  - ì¢Œìš° ëŒ€ì¹­ ë¶„í¬ â†’ ì²´ê³„ì ì¸ í¸í–¥(bias)ì€ í¬ì§€ ì•ŠìŒ.  \n",
    "\n",
    "- **í•œê³„**  \n",
    "  - íŠ¹ì • ê¸°ê°„(ì´ë²¤íŠ¸ì„± ë‚ ì”¨, íœ´ì¼ ë“±)ì—ì„œ ì˜¤ì°¨ê°€ í¬ê²Œ ë°œìƒ.  \n",
    "  - ì”ì°¨ê°€ ë°±ìƒ‰ì¡ìŒ(white noise) ìˆ˜ì¤€ì€ ì•„ë‹˜ â†’ íŒ¨í„´ì´ ë‚¨ì•„ ìˆìŒ.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Metacode Lecture (Python 3.13)",
   "language": "python",
   "name": "metacode-lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
