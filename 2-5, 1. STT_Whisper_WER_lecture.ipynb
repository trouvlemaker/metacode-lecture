{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2-5, 1강 STT(음성→텍스트) 기초와 데이터 핸들링 — Whisper 실습\n",
        "\n",
        "- 목표: 음성→텍스트(STT) 기본 개념과 워크플로우 이해, Whisper로 전사 및 정확도(WER) 측정\n",
        "- 데이터: 짧은 음성-텍스트 pair 샘플(예: Common Voice ko 소규모 발화, 또는 제공된 예제 파일)\n",
        "- 규칙(강의용): 시각화는 `matplotlib`만 사용, 불필요한 외부 의존 최소화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 구성\n",
        "- STT 개요 및 워크플로우\n",
        "- 환경 준비(`ffmpeg`, 라이브러리)와 폰트/경고 설정\n",
        "- 데이터 준비: 오디오 로딩, 16kHz/모노 전처리\n",
        "- Whisper 전사 데모(옵션: 언어/모델 크기)\n",
        "- 정확도 평가지표(WER) 계산(참고용)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. 환경 준비 및 라이브러리 임포트\n",
        "\n",
        "- 시각화는 `matplotlib`만 사용합니다.\n",
        "- STT는 `openai-whisper`, 오디오 I/O/변환은 `torchaudio`, 평가는 `jiwer`를 사용합니다.\n",
        "- 한글 폰트와 경고 억제를 설정합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "try:\n",
        "    import whisper\n",
        "    _HAS_WHISPER = True\n",
        "except Exception:\n",
        "    _HAS_WHISPER = False\n",
        "\n",
        "try:\n",
        "    import jiwer\n",
        "    _HAS_JIWER = True\n",
        "except Exception:\n",
        "    _HAS_JIWER = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=r\"Glyph.*missing from font.*\", category=UserWarning)\n",
        "\n",
        "# 한글 폰트 설정 및 마이너스 기호 깨짐 방지\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 데이터 준비 및 전처리\n",
        "- 예제 오디오 파일 경로 지정, 로드, 16kHz 리샘플, 모노 변환\n",
        "- 긴 파일은 일부 구간만 사용(강의 시간 고려, 10–15초 권장)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio_mono_16k(path: str, offset_sec: float = 0.0, duration_sec: float | None = None):\n",
        "    \"\"\"\n",
        "    오디오를 로드하여 16kHz 모노로 변환합니다.\n",
        "    offset_sec부터 duration_sec 길이만 잘라 사용합니다.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError('오디오 파일을 찾을 수 없습니다: ' + path)\n",
        "\n",
        "    wav, sr = torchaudio.load(path)  # (channels, time)\n",
        "    # 모노 변환: 여러 채널이면 평균\n",
        "    if wav.size(0) > 1:\n",
        "        wav = wav.mean(dim=0, keepdim=True)\n",
        "    # 리샘플\n",
        "    if sr != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "        wav = resampler(wav)\n",
        "        sr = 16000\n",
        "    # offset/duration 적용\n",
        "    if offset_sec > 0 or duration_sec is not None:\n",
        "        start = int(offset_sec * sr)\n",
        "        end = start + int(duration_sec * sr) if duration_sec is not None else wav.size(1)\n",
        "        wav = wav[:, start:end]\n",
        "    return wav.squeeze(0), sr  # (time,), 16000\n",
        "\n",
        "# 예시 경로 (수강생은 자신의 파일로 교체 가능)\n",
        "EXAMPLE_WAV = 'example.wav'  # 존재하지 않을 경우 에러 메시지 안내\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Whisper 전사 데모\n",
        "- 모델 선택(`tiny`~`large-v3`/`large-v3-turbo`), 언어 지정, CPU/GPU 동작\n",
        "- 길이 긴 파일은 부분 전사 또는 슬라이딩 윈도우 전략을 언급\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transcribe_with_whisper(path: str, model_name: str = 'base', language: str | None = None):\n",
        "    if not _HAS_WHISPER:\n",
        "        raise ImportError('whisper 패키지가 설치되어 있지 않습니다. 설치: pip install -U openai-whisper')\n",
        "    # Whisper는 내부적으로 ffmpeg를 사용하므로, ffmpeg 설치 필요\n",
        "    model = whisper.load_model(model_name, device=DEVICE)\n",
        "    # Whisper 내장 로더를 사용 (자동 리샘플/패딩 포함)\n",
        "    result = model.transcribe(path, language=language)\n",
        "    return result\n",
        "\n",
        "# 데모 실행 (예시)\n",
        "# try:\n",
        "#     wav, sr = load_audio_mono_16k(EXAMPLE_WAV, duration_sec=15)\n",
        "#     # 저장 후 Whisper에 넘기기 (메모리→파일 경로 방식)\n",
        "#     tmp_path = 'tmp_16k.wav'\n",
        "#     torchaudio.save(tmp_path, wav.unsqueeze(0), 16000)\n",
        "#     out = transcribe_with_whisper(tmp_path, model_name='base', language='ko')\n",
        "#     print('전사 결과:', out['text'][:120], '...')\n",
        "# except Exception as e:\n",
        "#     print('데모 실행 중 문제:', e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 정확도 평가(WER) — 참고용\n",
        "- 레퍼런스 텍스트가 있을 때만 사용\n",
        "- 언어/문장부호/대소문자 정규화 후 `jiwer.wer` 계산\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_wer(hypothesis: str, reference: str) -> float:\n",
        "    if not _HAS_JIWER:\n",
        "        raise ImportError('jiwer 패키지가 필요합니다. 설치: pip install jiwer')\n",
        "    # 단순 정규화: 소문자/공백 정리\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.Strip(),\n",
        "    ])\n",
        "    hyp = transformation(hypothesis)\n",
        "    ref = transformation(reference)\n",
        "    return jiwer.wer(ref, hyp)\n",
        "\n",
        "# 예시\n",
        "# hyp = '안녕하세요 만나서 반갑습니다'\n",
        "# ref = '안녕하세요 만나서 반갑습니다'\n",
        "# print('WER:', compute_wer(hyp, ref))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 참고(부록)\n",
        "- 아래에 확장 실습(Transformers/SpeechT5/Gradio)이 있었던 버전은 3강(파이프라인)으로 이동했습니다.\n",
        "- 본 1강에서는 Whisper 전사와 WER까지로 최소 구성만 다룹니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 마무리 및 과제 제안\n",
        "- 모델 크기/언어 옵션을 바꿔 전사 품질 비교\n",
        "- 2~3개 파일 전사 후 간단 리포트(WER, 주관 평가)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2-5. STT/TTS 모델 활용 실습 — 음성-텍스트 pair 데이터\n",
        "\n",
        "- 목표: 음성→텍스트(STT), 텍스트→음성(TTS) 기본 파이프라인을 실습하고 성능지표(WER/CER)로 평가합니다.\n",
        "- 데이터: 공개 샘플(예: LibriSpeech subset) 또는 로컬 폴더(AudioFolder) 사용\n",
        "- 실습 흐름: 환경 설정 → 데이터 준비 → STT 추론 → 지표 평가 → TTS 생성 → 라운드트립 데모 → Gradio 데모 → 확장/과제\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. 환경 설정 및 라이브러리\n",
        "- CPU에서도 실행 가능하도록 기본 모델/배치로 시연합니다.\n",
        "- `transformers`, `datasets[audio]`, `evaluate`, `gradio`, `torchaudio`, `soundfile` 등을 사용합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기본 설치 (인터넷 연결 필요). 이미 설치되어 있으면 건너뜀.\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            __import__(p.split('[')[0].split('==')[0].replace('-', '_'))\n",
        "        except Exception:\n",
        "            print(f'Installing {p} ...')\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', p, '--quiet'])\n",
        "\n",
        "pip_install([\n",
        "    'transformers',\n",
        "    'datasets[audio]',\n",
        "    'evaluate',\n",
        "    'gradio',\n",
        "    'torchaudio',\n",
        "    'soundfile'\n",
        "])\n",
        "\n",
        "import os, random, numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import pipeline\n",
        "import evaluate as hf_evaluate\n",
        "\n",
        "# 디바이스 확인\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)\n",
        "\n",
        "# 시드 고정\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# 맥 한글 폰트 설정(강의 시 그래프 텍스트용, STT/TTS엔 직접 영향 없음)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'AppleGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 데이터 준비\n",
        "- 데모용: Hugging Face `librispeech_asr_dummy` 또는 `audiofolder`(로컬) 선택 가능\n",
        "- 모델 샘플링레이트(보통 16kHz)에 맞춰 리샘플링합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "USE_DUMMY = True   # True: 데모용 소형 샘플, False: 로컬 오디오 폴더 사용\n",
        "LOCAL_AUDIO_DIR = None  # 예: '/path/to/audiofolder' (file_name, text 메타 구성 권장)\n",
        "\n",
        "if USE_DUMMY:\n",
        "    ds = load_dataset('hf-internal-testing/librispeech_asr_dummy', 'clean', split='validation')\n",
        "else:\n",
        "    from datasets import load_dataset\n",
        "    assert LOCAL_AUDIO_DIR is not None and os.path.exists(LOCAL_AUDIO_DIR), '로컬 폴더 경로를 설정하세요.'\n",
        "    ds = load_dataset('audiofolder', data_dir=LOCAL_AUDIO_DIR, split='train')\n",
        "\n",
        "print(ds[0])\n",
        "\n",
        "# 샘플링레이트 맞추기(Whisper는 16kHz)\n",
        "from datasets import Audio\n",
        "TARGET_SR = 16000\n",
        "try:\n",
        "    ds = ds.cast_column('audio', Audio(sampling_rate=TARGET_SR))\n",
        "except Exception:\n",
        "    # 칼럼명이 다르면 유연하게 처리\n",
        "    audio_col = 'audio' if 'audio' in ds.column_names else ds.column_names[0]\n",
        "    ds = ds.cast_column(audio_col, Audio(sampling_rate=TARGET_SR))\n",
        "\n",
        "# 텍스트 정리 유틸\n",
        "import re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"[^a-z0-9' ]+\", ' ', s)  # 데모: 영문 기준 간단 정규화\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    return s\n",
        "\n",
        "texts = []\n",
        "audios = []  # (sr, np.ndarray) 형태로 수집\n",
        "\n",
        "audio_key = 'audio' if 'audio' in ds.column_names else ds.column_names[0]\n",
        "text_key  = 'text' if 'text' in ds.column_names else ('sentence' if 'sentence' in ds.column_names else None)\n",
        "\n",
        "for ex in ds:\n",
        "    aud = ex[audio_key]\n",
        "    arr = aud['array']\n",
        "    sr  = aud['sampling_rate']\n",
        "    audios.append((sr, arr))\n",
        "    if text_key and text_key in ex:\n",
        "        texts.append(normalize_text(str(ex[text_key])))\n",
        "    else:\n",
        "        texts.append('')\n",
        "\n",
        "print('n_samples:', len(audios), 'sr:', audios[0][0], 'shape0:', audios[0][1].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. STT(음성→텍스트) 추론\n",
        "- Whisper 소모형 체크포인트로 CPU에서도 시연 가능 (`openai/whisper-small` 등)\n",
        "- 모델 샘플링레이트에 맞춰 입력을 전달합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "whisper_model = 'openai/whisper-small'  # CPU 데모: small/base 추천\n",
        "\n",
        "asr = pipeline(\n",
        "    task='automatic-speech-recognition',\n",
        "    model=whisper_model,\n",
        "    device_map='auto' if device=='cuda' else None\n",
        ")\n",
        "\n",
        "# 데이터셋 샘플 몇 개만 추론\n",
        "def run_asr_batch(audios: List[Tuple[int, np.ndarray]], max_items: int = 4) -> List[str]:\n",
        "    outs = []\n",
        "    for (sr, arr) in audios[:max_items]:\n",
        "        out = asr({'array': arr, 'sampling_rate': sr})\n",
        "        text = out['text'] if isinstance(out, dict) and 'text' in out else out\n",
        "        outs.append(normalize_text(str(text)))\n",
        "    return outs\n",
        "\n",
        "pred_texts = run_asr_batch(audios, max_items=min(4, len(audios)))\n",
        "print('예측 텍스트:', pred_texts)\n",
        "print('정답 텍스트:', texts[:len(pred_texts)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 평가 지표: WER/CER\n",
        "- `evaluate` 라이브러리의 `wer`, `cer` 또는 `xtreme_s`(WER/CER 동시) 사용\n",
        "- 소문자/구두점 제거 등 정규화 여부에 따라 값이 달라집니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wer_metric = hf_evaluate.load('wer')\n",
        "cer_metric = hf_evaluate.load('cer')\n",
        "\n",
        "# 예측/정답 쌍 정리 (빈 텍스트는 제외)\n",
        "refs = [t for t in texts[:len(pred_texts)] if t]\n",
        "preds = [p for p, t in zip(pred_texts, texts[:len(pred_texts)]) if t]\n",
        "\n",
        "wer = wer_metric.compute(predictions=preds, references=refs) if refs else None\n",
        "cer = cer_metric.compute(predictions=preds, references=refs) if refs else None\n",
        "print({'wer': wer, 'cer': cer})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. TTS(텍스트→음성) 생성\n",
        "- 간편 시연: `microsoft/speecht5_tts` + `microsoft/speecht5_hifigan`\n",
        "- 스피커 임베딩이 필요하므로, 제공된 샘플 임베딩을 사용합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import SpeechT5ForTextToSpeech, SpeechT5Processor, SpeechT5HifiGan\n",
        "import soundfile as sf\n",
        "\n",
        "# 모델/프로세서/보코더 로드\n",
        "processor = SpeechT5Processor.from_pretrained('microsoft/speecht5_tts')\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained('microsoft/speecht5_tts')\n",
        "vocoder = SpeechT5HifiGan.from_pretrained('microsoft/speecht5_hifigan')\n",
        "model = model.to(device)\n",
        "\n",
        "# 샘플 스피커 임베딩 (HF 제공 예시 파일 사용 권장)\n",
        "# 간단화를 위해 랜덤 벡터(512차원) 사용 (음색 고정 X)\n",
        "speaker_embeddings = torch.randn(1, 512, device=model.device)\n",
        "\n",
        "\n",
        "def tts_generate(text: str, sr: int = 16000, out_wav: str | None = None) -> Tuple[int, np.ndarray]:\n",
        "    inputs = processor(text=text, return_tensors='pt')\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # vocoder를 인자로 전달하면 파형으로 변환된 torch.Tensor 반환\n",
        "        audio = model.generate_speech(input_ids, speaker_embeddings, vocoder=vocoder)\n",
        "\n",
        "    audio = audio.cpu().numpy().astype(np.float32)\n",
        "    if out_wav:\n",
        "        sf.write(out_wav, audio, sr)\n",
        "    return sr, audio\n",
        "\n",
        "# 데모 실행\n",
        "sr_out, wav = tts_generate('hello, this is a text to speech demo using SpeechT5.')\n",
        "print('TTS length:', wav.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. 라운드트립 데모\n",
        "- STT→TTS: 음성을 텍스트로 변환 후, 그 텍스트를 다시 음성으로 합성\n",
        "- TTS→STT: 텍스트를 합성한 음성을 다시 텍스트로 변환하여 손실 확인\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STT → TTS\n",
        "sr0, aud0 = audios[0]\n",
        "stt_text = asr({'array': aud0, 'sampling_rate': sr0})\n",
        "stt_text = stt_text['text'] if isinstance(stt_text, dict) else str(stt_text)\n",
        "print('STT 텍스트:', stt_text)\n",
        "\n",
        "sr_syn, wav_syn = tts_generate(stt_text)\n",
        "print('합성 오디오 길이:', wav_syn.shape)\n",
        "\n",
        "# TTS → STT (텍스트에서 합성한 음성을 다시 인식)\n",
        "rt_text = asr({'array': wav_syn, 'sampling_rate': sr_syn})\n",
        "rt_text = rt_text['text'] if isinstance(rt_text, dict) else str(rt_text)\n",
        "print('라운드트립 텍스트:', rt_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Gradio 데모 UI (마이크 STT / 텍스트 TTS)\n",
        "- 마이크 입력으로 STT 수행, 텍스트 입력으로 TTS 생성\n",
        "- 로컬 실행 시 브라우저에서 인터랙티브 체험 가능\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gr_stt(audio):\n",
        "    if audio is None:\n",
        "        return ''\n",
        "    sr, data = audio\n",
        "    out = asr({'array': data, 'sampling_rate': sr})\n",
        "    return out['text'] if isinstance(out, dict) and 'text' in out else str(out)\n",
        "\n",
        "def gr_tts(text):\n",
        "    if text is None or not str(text).strip():\n",
        "        return None\n",
        "    return tts_generate(str(text))\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown('# STT / TTS 데모')\n",
        "    with gr.Tab('STT (마이크 → 텍스트)'):\n",
        "        in_audio = gr.Audio(sources=['microphone'], type='numpy')\n",
        "        out_text = gr.Textbox(label='인식 텍스트')\n",
        "        btn1 = gr.Button('Transcribe')\n",
        "        btn1.click(gr_stt, inputs=in_audio, outputs=out_text)\n",
        "    with gr.Tab('TTS (텍스트 → 오디오)'):\n",
        "        in_text = gr.Textbox(label='합성할 텍스트', value='안녕하세요. STT TTS 데모입니다.')\n",
        "        out_audio = gr.Audio(label='합성 오디오')\n",
        "        btn2 = gr.Button('Synthesize')\n",
        "        btn2.click(gr_tts, inputs=in_text, outputs=out_audio)\n",
        "\n",
        "# 주피터에서 수동 실행 필요: demo.launch()\n",
        "print('Gradio 준비 완료: demo.launch() 를 실행하세요.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. 추가 학습(개념) — LoRA/PEFT로 미세조정\n",
        "- STT(Whisper) 도메인 적응: 특정 발화 스타일/도메인(콜센터/의료)을 위한 LoRA 미세조정\n",
        "- TTS(SpeechT5) 화자 임베딩: 동일 화자 다량 데이터가 없을 경우, speaker embedding 활용 및 multi-speaker 학습 개념\n",
        "- 데이터: 음성-텍스트 pair 정렬, 샘플링레이트/정규화 일치, 텍스트 정규화 규칙 고정\n",
        "- 리소스: GPU 메모리 절약을 위한 8-bit/4-bit 로딩 + LoRA, gradient checkpointing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. 참고 자료\n",
        "- Transformers ASR 파이프라인: [Hugging Face Transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines)\n",
        "- Datasets 오디오 로딩/리샘플링: [Hugging Face Datasets Audio](https://huggingface.co/docs/datasets/audio_load)\n",
        "- 평가 지표 WER/CER: [Hugging Face Evaluate WER](https://huggingface.co/spaces/evaluate-metric/wer), [CER](https://huggingface.co/spaces/evaluate-metric/cer)\n",
        "- Gradio Audio 컴포넌트: [Gradio Audio Docs](https://www.gradio.app/docs/audio)\n",
        "- SpeechT5: [microsoft/speecht5_tts](https://huggingface.co/microsoft/speecht5_tts), [microsoft/speecht5_hifigan](https://huggingface.co/microsoft/speecht5_hifigan)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
