{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae167db",
   "metadata": {},
   "source": [
    "# ğŸ“˜ 1ê°• (PyTorch, CPU): CIFAR-10 ë¶„ë¥˜ â€” **ë°ì´í„° íƒìƒ‰ â†’ MLP vs CNN â†’ ë‚´ë¶€ ì‹œê°í™” â†’ ì˜¤ë¶„ë¥˜ ë¶„ì„ â†’ ê°œì„  â†’ ì±Œë¦°ì§€**\n",
    "\n",
    "> **ì´¬ì˜ìš© ìŠ¤í¬ë¦½íŠ¸ ê°€ì´ë“œ í¬í•¨**: ê° ì„¹ì…˜ ìƒë‹¨ì— ë§í•˜ê¸° í¬ì¸íŠ¸ì™€ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë„£ì—ˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2804145",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09765e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, numpy as np, matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# ì—­ì •ê·œí™” ìœ í‹¸\n",
    "inv_mean = np.array([0.4914,0.4822,0.4465])\n",
    "inv_std  = np.array([0.2470,0.2435,0.2616])\n",
    "def denorm(img_tensor):\n",
    "    img = img_tensor.permute(1,2,0).cpu().numpy()\n",
    "    img = img * inv_std + inv_mean\n",
    "    return np.clip(img, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab86de3",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° íƒìƒ‰ ì‹¬í™” (â‰ˆ6ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- ì‚¬ëŒì€ ì„ /ìƒ‰/ì§ˆê°ìœ¼ë¡œ ì‚¬ë¬¼ì„ êµ¬ë¶„í•©ë‹ˆë‹¤. ëª¨ë¸ë„ ë¹„ìŠ·í•œ ë‹¨ì„œ(íŠ¹ì§•)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "- CIFAR-10ì˜ í¬ê¸°/ì±„ë„/ë²”ìœ„ë¥¼ í™•ì¸í•˜ê³ , ì…ë ¥ ì „ì²˜ë¦¬ì˜ í•„ìš”ì„±ì„ ì§šìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì§ˆë¬¸:** â€œì—¬ëŸ¬ë¶„ì€ ìë™ì°¨ì™€ ê°œë¥¼ ì‚¬ì§„ì—ì„œ ì–´ë–»ê²Œ êµ¬ë¶„í•˜ì‹œë‚˜ìš”?â€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dab88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬: ì •ê·œí™”\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
    "])\n",
    "\n",
    "root='./data'\n",
    "train_full = datasets.CIFAR10(root=root, train=True, download=True, transform=train_tf)\n",
    "test_set   = datasets.CIFAR10(root=root, train=False, download=True, transform=test_tf)\n",
    "class_names = train_full.classes\n",
    "\n",
    "# Split\n",
    "val_ratio=0.2\n",
    "val_len  = int(len(train_full)*val_ratio)\n",
    "train_len= len(train_full)-val_len\n",
    "train_set, val_set = random_split(train_full,[train_len,val_len])\n",
    "\n",
    "train_loader=DataLoader(train_set,batch_size=128,shuffle=True,num_workers=0)\n",
    "val_loader  =DataLoader(val_set,  batch_size=128,shuffle=False,num_workers=0)\n",
    "test_loader =DataLoader(test_set,  batch_size=128,shuffle=False,num_workers=0)\n",
    "\n",
    "print('Train/Val/Test sizes:', len(train_set), len(val_set), len(test_set))\n",
    "print('Sample image shape (C,H,W):', train_set[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82964fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ 5ì¥ì”© ì‹œê°í™”\n",
    "import math\n",
    "fig, axes = plt.subplots(10, 5, figsize=(8,16))\n",
    "counts = {c:0 for c in class_names}\n",
    "for x,y in train_loader:\n",
    "    for img, lab in zip(x, y):\n",
    "        c = class_names[lab.item()]\n",
    "        if counts[c] < 5:\n",
    "            ax = axes[class_names.index(c), counts[c]]\n",
    "            ax.imshow(denorm(img)); ax.set_axis_off()\n",
    "            if counts[c]==0: ax.set_ylabel(c, rotation=0, labelpad=30, va='center')\n",
    "            counts[c]+=1\n",
    "    if all(counts[c]>=5 for c in class_names): break\n",
    "plt.suptitle('ê° í´ë˜ìŠ¤ ìƒ˜í”Œ 5ì¥', fontsize=14); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) ìë™ì°¨ vs ê°œ RGB ì±„ë„ë³„ íˆìŠ¤í† ê·¸ë¨ ë¹„êµ\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "raw_train = CIFAR10(root='./data', train=True, download=False)  # PIL ì´ë¯¸ì§€ ì ‘ê·¼ìš©\n",
    "def collect_pixels(label_name, max_imgs=200):\n",
    "    idx_label = class_names.index(label_name)\n",
    "    vals = []\n",
    "    cnt = 0\n",
    "    for img, lab in raw_train:\n",
    "        if lab == idx_label:\n",
    "            vals.append(np.array(img))  # (H,W,3), 0~255\n",
    "            cnt += 1\n",
    "            if cnt>=max_imgs: break\n",
    "    arr = np.stack(vals, axis=0)  # (N,H,W,3)\n",
    "    return arr\n",
    "\n",
    "cars = collect_pixels('automobile', max_imgs=200)\n",
    "dogs = collect_pixels('dog', max_imgs=200)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "for i, ch in enumerate(['R','G','B']):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.hist(cars[:,:,:,i].ravel(), bins=50, alpha=0.6, label='car')\n",
    "    plt.hist(dogs[:,:,:,i].ravel(), bins=50, alpha=0.6, label='dog')\n",
    "    plt.title(f'Channel {ch} Histogram'); plt.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print('cars array:', cars.shape, 'dogs array:', dogs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce5655",
   "metadata": {},
   "source": [
    "## 2. Baseline ë¹„êµ (â‰ˆ6ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- MLPëŠ” ê³µê°„ ì •ë³´ë¥¼ í™œìš©í•˜ì§€ ëª»í•¨ â†’ íŒŒë¼ë¯¸í„° ìˆ˜ ëŒ€ë¹„ ë¹„íš¨ìœ¨ì .  \n",
    "- CNNì€ ì§€ì—­(ì»¤ë„) ë‹¨ìœ„ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ  â†’ íš¨ìœ¨ì„±â†‘, ì¼ë°˜í™”â†‘.\n",
    "\n",
    "**ì§ˆë¬¸:** â€œê°™ì€ íŒŒë¼ë¯¸í„° ìˆ˜ë¼ë©´ ì–´ë–¤ êµ¬ì¡°ê°€ ë” ìœ ë¦¬í• ê¹Œìš”?â€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c91553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.flatten(x))\n",
    "\n",
    "class CNN_V1(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "mlp = MLP().to(device)\n",
    "cnn = CNN_V1().to(device)\n",
    "print('MLP params:', count_params(mlp))\n",
    "print('CNN_V1 params:', count_params(cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, loss_fn, optim_):\n",
    "    model.train(); tl=0; tc=0; n=0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        optim_.zero_grad()\n",
    "        out = model(x); loss = loss_fn(out,y)\n",
    "        loss.backward(); optim_.step()\n",
    "        tl += loss.item()*x.size(0)\n",
    "        tc += (out.argmax(1)==y).sum().item(); n += x.size(0)\n",
    "    return tl/n, tc/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval(); tl=0; tc=0; n=0\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        out = model(x); loss = loss_fn(out,y)\n",
    "        tl += loss.item()*x.size(0)\n",
    "        tc += (out.argmax(1)==y).sum().item(); n += x.size(0)\n",
    "    return tl/n, tc/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e894462",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def train_model(model, epochs=5, lr=1e-3):\n",
    "    optim_ = optim.Adam(model.parameters(), lr=lr)\n",
    "    hist = {'train_acc':[], 'val_acc':[], 'train_loss':[], 'val_loss':[]}\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_l, tr_a = train_epoch(model, train_loader, criterion, optim_)\n",
    "        va_l, va_a = eval_epoch(model, val_loader, criterion)\n",
    "        hist['train_loss'].append(tr_l); hist['val_loss'].append(va_l)\n",
    "        hist['train_acc'].append(tr_a);  hist['val_acc'].append(va_a)\n",
    "        print(f'[Ep {ep}/{epochs}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')\n",
    "    return hist\n",
    "\n",
    "mlp_hist = train_model(mlp, epochs=5)\n",
    "cnn_hist = train_model(cnn, epochs=10)  # CNNì€ ì¡°ê¸ˆ ë” í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµê³¡ì„  ë‚˜ë€íˆ í”Œë¡¯\n",
    "def plot_hist(h, title):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['train_acc'], label='train_acc')\n",
    "    plt.plot(h['val_acc'], label='val_acc')\n",
    "    plt.title(title+'(Accuracy)'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['train_loss'], label='train_loss')\n",
    "    plt.plot(h['val_loss'], label='val_loss')\n",
    "    plt.title(title+'(Loss)'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_hist(mlp_hist, 'MLP ')\n",
    "plot_hist(cnn_hist, 'CNN_V1 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603661c",
   "metadata": {},
   "source": [
    "## 3. CNN ë‚´ë¶€ ì‹œê°í™” (â‰ˆ5ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- ì´ˆê¸° í•„í„°ëŠ” ì—£ì§€/ìƒ‰ìƒ ê°™ì€ ì €ìˆ˜ì¤€ íŠ¹ì§•ì„ ë½‘ìŠµë‹ˆë‹¤.  \n",
    "- ì´í›„ ë ˆì´ì–´ë¡œ ê°ˆìˆ˜ë¡ íŒ¨í„´/í˜•íƒœ ê°™ì€ ê³ ìˆ˜ì¤€ íŠ¹ì§•ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) ì²« Conv ë ˆì´ì–´ í•„í„° ê°€ì¤‘ì¹˜ ì‹œê°í™”\n",
    "w = cnn.features[0].weight.data.cpu()  # (out, in, k, k)\n",
    "w_min, w_max = w.min().item(), w.max().item()\n",
    "grid = []\n",
    "for i in range(min(32, w.shape[0])):  # 32ê°œê¹Œì§€ë§Œ\n",
    "    filt = w[i]\n",
    "    # ì±„ë„ í‰ê· ìœ¼ë¡œ 2D ë§µ ë§Œë“¤ê¸°\n",
    "    fmap = filt.mean(0)\n",
    "    grid.append((fmap - w_min)/(w_max - w_min + 1e-9))\n",
    "grid = torch.stack(grid,0).numpy()\n",
    "\n",
    "cols = 8; rows = (len(grid)+cols-1)//cols\n",
    "plt.figure(figsize=(10, 2*rows))\n",
    "for i, g in enumerate(grid):\n",
    "    plt.subplot(rows, cols, i+1); plt.imshow(g, cmap='gray'); plt.axis('off')\n",
    "plt.suptitle('ì²« Conv í•„í„°(í‰ê·  ì±„ë„)'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ 1ì¥ì˜ ì²« Conv íŠ¹ì§•ë§µ\n",
    "with torch.no_grad():\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x[0:1].to(device)\n",
    "    features = cnn.features[0](x)  # ì²« Conv ê²°ê³¼ (N=1, C=32, H, W)\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(16):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(features[0,i].cpu(), cmap='gray'); plt.axis('off')\n",
    "plt.suptitle('ì²« Conv Feature Maps'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcdc04",
   "metadata": {},
   "source": [
    "## 4. ì˜¤ë¶„ë¥˜ ë¶„ì„ ì‹¬í™” (â‰ˆ5ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- Confusion Matrixë¡œ **ìì£¼ í—·ê°ˆë¦¬ëŠ” í´ë˜ìŠ¤ ìŒ**ì„ ì°¾ìŠµë‹ˆë‹¤.  \n",
    "- í•´ë‹¹ ìŒì˜ ìƒ˜í”Œì„ ì—¬ëŸ¬ ì¥ ë³´ì—¬ì£¼ë©°, ê³µí†µ íŒ¨í„´ì„ í† ë¡ í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    model.eval(); ys=[]; ps=[]\n",
    "    for x,y in loader:\n",
    "        x = x.to(device); out = model(x)\n",
    "        ps.append(out.argmax(1).cpu().numpy()); ys.append(y.numpy())\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "y_true, y_pred = get_all_preds(cnn, test_loader)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest'); plt.title('Confusion Matrix')\n",
    "plt.colorbar(); plt.xticks(range(10), class_names, rotation=45); plt.yticks(range(10), class_names)\n",
    "plt.tight_layout(); plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()\n",
    "\n",
    "# Top-3 í˜¼ë™ í´ë˜ìŠ¤ ìŒ ì¶”ì¶œ\n",
    "pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i!=j and cm[i,j]>0:\n",
    "            pairs.append(((i,j), cm[i,j]))\n",
    "pairs = sorted(pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "print('Top-3 í˜¼ë™ í´ë˜ìŠ¤ ìŒ:', [(class_names[i], class_names[j], n) for (i,j), n in pairs])\n",
    "\n",
    "# ê° ìŒì—ì„œ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ 6ì¥ í‘œì‹œ\n",
    "raw_test = datasets.CIFAR10(root='./data', train=False, download=False, transform=transforms.ToTensor())\n",
    "idx_map = list(range(len(raw_test)))  # ì›ë³¸ ì¸ë±ìŠ¤ ì ‘ê·¼ìš©\n",
    "# test_loader ìˆœì„œì™€ raw_test ì¸ë±ìŠ¤ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìº”\n",
    "def find_mismatch_indices(true_label, pred_label, max_show=6):\n",
    "    out_idx = []\n",
    "    count = 0\n",
    "    # brute-force: ì „ì²´ ì˜ˆì¸¡ì„ ë‹¤ì‹œ ìˆœì°¨ë¡œ í™•ì¸\n",
    "    ptr = 0\n",
    "    for batch_x, batch_y in DataLoader(test_set, batch_size=128, shuffle=False):\n",
    "        with torch.no_grad():\n",
    "            logits = cnn(batch_x.to(device))\n",
    "            preds = logits.argmax(1).cpu().numpy()\n",
    "        for k in range(len(batch_y)):\n",
    "            if batch_y[k].item()==true_label and preds[k]==pred_label:\n",
    "                out_idx.append(ptr+k)\n",
    "                count += 1\n",
    "                if count>=max_show: return out_idx\n",
    "        ptr += len(batch_y)\n",
    "    return out_idx\n",
    "\n",
    "for (i,j), _ in pairs:\n",
    "    ids = find_mismatch_indices(i,j, max_show=6)\n",
    "    if not ids: continue\n",
    "    plt.figure(figsize=(6,3))\n",
    "    for k, idx in enumerate(ids):\n",
    "        img,_ = raw_test[idx]\n",
    "        plt.subplot(2,3,k+1); plt.imshow(img.permute(1,2,0)); plt.axis('off')\n",
    "        plt.title(f'T:{class_names[i]} â†’ P:{class_names[j]}')\n",
    "    plt.suptitle(f'í˜¼ë™ ì‚¬ë¡€: {class_names[i]} vs {class_names[j]}'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a8b5c",
   "metadata": {},
   "source": [
    "## 5. ì„±ëŠ¥ ê°œì„  ê¸°ë²• (â‰ˆ6ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- ì¦ê°•, ì •ê·œí™”, ë“œë¡­ì•„ì›ƒ/ë°°ì¹˜ì •ê·œí™”, ì˜µí‹°ë§ˆì´ì € ë³€ê²½ ë“±ìœ¼ë¡œ ê³¼ì í•©ì„ ì œì–´í•˜ê³  ì¼ë°˜í™”ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¦ê°• íŒŒì´í”„ë¼ì¸ê³¼ BN/Dropoutì„ í¬í•¨í•œ ê°œì„  ëª¨ë¸\n",
    "aug_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
    "])\n",
    "train_full_aug = datasets.CIFAR10(root='./data', train=True, download=False, transform=aug_tf)\n",
    "train_set2, val_set2 = random_split(train_full_aug, [train_len, val_len])\n",
    "train_loader2=DataLoader(train_set2,batch_size=128,shuffle=True,num_workers=0)\n",
    "val_loader2  =DataLoader(val_set2,  batch_size=128,shuffle=False,num_workers=0)\n",
    "\n",
    "class CNN_V2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "cnn2 = CNN_V2().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer ì‹¤í—˜: SGD vs Adam\n",
    "opt_name = 'SGD'  # 'Adam'ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë¹„êµ ì‹¤í—˜\n",
    "optimizer = optim.SGD(cnn2.parameters(), lr=0.01, momentum=0.9) if opt_name=='SGD' else optim.Adam(cnn2.parameters(), lr=1e-3)\n",
    "\n",
    "hist2={'train_acc':[],'val_acc':[],'train_loss':[],'val_loss':[]}\n",
    "for ep in range(1, 12):  # ì•½ê°„ ë” í•™ìŠµ\n",
    "    tr_l,tr_a = train_epoch(cnn2, train_loader2, criterion, optimizer)\n",
    "    va_l,va_a = eval_epoch(cnn2, val_loader2, criterion)\n",
    "    hist2['train_loss'].append(tr_l); hist2['val_loss'].append(va_l)\n",
    "    hist2['train_acc'].append(tr_a);  hist2['val_acc'].append(va_a)\n",
    "    print(f'[CNN_V2][{opt_name}][Ep {ep}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ\n",
    "def plot_hist(h, title):\n",
    "    plt.figure(figsize=(6,4)); plt.plot(h['train_acc'],label='train_acc'); plt.plot(h['val_acc'],label='val_acc'); plt.title(title+' Acc'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    plt.figure(figsize=(6,4)); plt.plot(h['train_loss'],label='train_loss'); plt.plot(h['val_loss'],label='val_loss'); plt.title(title+' Loss'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_hist(hist2, f'CNN_V2 ({opt_name})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936fe06",
   "metadata": {},
   "source": [
    "## 6. ì‹¤ìŠµ ì±Œë¦°ì§€ (â‰ˆ2ë¶„)\n",
    "\n",
    "- **ë ˆì´ì–´ ìˆ˜ ëŠ˜ë¦¬ê¸°**: Conv ë¸”ë¡ ì¶”ê°€ í›„ ì„±ëŠ¥/ì‹œê°„ ë³€í™” ë¹„êµ  \n",
    "- **ì¦ê°• ê°•ë„ ì¡°ì ˆ**: `ColorJitter`, `RandomErasing` ë“± ì¶”ê°€í•˜ì—¬ ì¼ë°˜í™” í™•ì¸  \n",
    "- **Optimizer êµì²´**: SGD â†” Adam, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(`StepLR`, `CosineAnnealingLR`) ë„ì…\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
