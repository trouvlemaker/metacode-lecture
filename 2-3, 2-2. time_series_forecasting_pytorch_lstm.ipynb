{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2-3, 2-2강 시계열 예측 모델링 — PyTorch LSTM (Bike Sharing)\n",
        "\n",
        "- **목표**: 시계열을 시퀀스 형태로 변환하고 LSTM으로 예측 학습/평가합니다.\n",
        "- **데이터**: Kaggle Bike Sharing Demand (시간 단위, `count` 대상)\n",
        "- **규칙(강의용)**: `matplotlib`만 사용 (seaborn X), 색상 지정 X, 서브플롯 X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. 환경 준비 및 라이브러리 임포트\n",
        "- PyTorch LSTM 구현 (GPU 없어도 CPU에서 동작)\n",
        "- 재현성을 위한 시드 고정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "def _set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 데이터 준비 및 전처리\n",
        "- `bike-sharing-demand/train.csv` 로드, 시간 파생변수 생성\n",
        "- train/val/test 시간 분할\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_hourly_data():\n",
        "    path = 'bike-sharing-demand/train.csv'\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError('train.csv 경로를 찾을 수 없습니다.')\n",
        "    df = pd.read_csv(path)\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "    df = df.sort_values('datetime').reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out['year'] = out['datetime'].dt.year\n",
        "    out['month'] = out['datetime'].dt.month\n",
        "    out['day'] = out['datetime'].dt.day\n",
        "    out['dayofweek'] = out['datetime'].dt.dayofweek\n",
        "    out['hour'] = out['datetime'].dt.hour\n",
        "    return out\n",
        "\n",
        "\n",
        "def split_by_time(df: pd.DataFrame, train_ratio: float = 0.8, val_ratio: float = 0.1):\n",
        "    n = len(df)\n",
        "    n_train = int(n * train_ratio)\n",
        "    n_val = int(n * val_ratio)\n",
        "    train = df.iloc[:n_train]\n",
        "    val = df.iloc[n_train:n_train+n_val]\n",
        "    test = df.iloc[n_train+n_val:]\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def build_feature_matrix(df: pd.DataFrame):\n",
        "    feature_cols = ['temp','atemp','humidity','windspeed','season','holiday','workingday','weather','year','month','dayofweek','hour']\n",
        "    X = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    y = df['count'].astype(np.float32).to_numpy()\n",
        "    return X, y, feature_cols\n",
        "\n",
        "\n",
        "df = load_hourly_data()\n",
        "print('데이터 크기:', df.shape, '기간:', df['datetime'].min(), '→', df['datetime'].max())\n",
        "df = add_time_features(df)\n",
        "train_df, val_df, test_df = split_by_time(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. 시퀀스 데이터셋 생성 및 LSTM 모델 정의\n",
        "- 윈도우 크기 24(하루)로 시퀀스 구성, horizon=1\n",
        "- 단순 LSTM 회귀 헤드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_windows(X: np.ndarray, y: np.ndarray, window: int, horizon: int = 1):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(X) - window - horizon + 1):\n",
        "        xs.append(X[i:i+window])\n",
        "        ys.append(y[i+window:i+window+horizon])\n",
        "    return np.asarray(xs, dtype=np.float32), np.asarray(ys, dtype=np.float32)\n",
        "\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X_seq: np.ndarray, y_seq: np.ndarray):\n",
        "        self.X = X_seq\n",
        "        self.y = y_seq\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, num_features: int, hidden_size: int = 64, num_layers: int = 2, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        last = out[:, -1, :]\n",
        "        y = self.head(last)\n",
        "        return y.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 학습/검증/테스트\n",
        "- 표준화: train 기준 평균/표준편차\n",
        "- 학습: Adam + MSE, 베스트 모델 선택\n",
        "- 예측 vs 실제 시각화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs: int, lr: float, device):\n",
        "    criterion = nn.MSELoss()\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_sum, n = 0.0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            tr_sum += float(loss.item()) * len(xb)\n",
        "            n += len(xb)\n",
        "        tr_loss = tr_sum / max(n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            va_sum, n2 = 0.0, 0\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss = criterion(pred, yb)\n",
        "                va_sum += float(loss.item()) * len(xb)\n",
        "                n2 += len(xb)\n",
        "        va_loss = va_sum / max(n2, 1)\n",
        "        print(f'Epoch {ep:03d} - train MSE: {tr_loss:.4f}, val MSE: {va_loss:.4f}')\n",
        "        if va_loss < best_val:\n",
        "            best_val = va_loss\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "\n",
        "def predict_all(model, loader, device):\n",
        "    model.eval()\n",
        "    outs = []\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            pred = model(xb)\n",
        "            outs.append(pred.cpu().numpy())\n",
        "    return np.concatenate(outs, axis=0)\n",
        "\n",
        "\n",
        "def plot_series(dt_index, y_true, y_pred, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.plot(dt_index, y_true, label='Actual')\n",
        "    plt.plot(dt_index, y_pred, label='Pred')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 데이터 구성 및 학습\n",
        "X_train, y_train, feat_cols = build_feature_matrix(train_df)\n",
        "X_val, y_val, _ = build_feature_matrix(val_df)\n",
        "X_test, y_test, _ = build_feature_matrix(test_df)\n",
        "\n",
        "mean = X_train.mean(axis=0, keepdims=True)\n",
        "std = X_train.std(axis=0, keepdims=True) + 1e-8\n",
        "X_train = (X_train - mean) / std\n",
        "X_val = (X_val - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "window = 24\n",
        "Xtr_seq, ytr_seq = make_windows(X_train, y_train, window)\n",
        "Xva_seq, yva_seq = make_windows(X_val, y_val, window)\n",
        "Xte_seq, yte_seq = make_windows(X_test, y_test, window)\n",
        "\n",
        "train_loader = DataLoader(SequenceDataset(Xtr_seq, ytr_seq.squeeze(-1) if ytr_seq.ndim>1 else ytr_seq), batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(SequenceDataset(Xva_seq, yva_seq.squeeze(-1) if yva_seq.ndim>1 else yva_seq), batch_size=256, shuffle=False)\n",
        "test_loader = DataLoader(SequenceDataset(Xte_seq, yte_seq.squeeze(-1) if yte_seq.ndim>1 else yte_seq), batch_size=256, shuffle=False)\n",
        "\n",
        "_set_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMRegressor(num_features=Xtr_seq.shape[-1], hidden_size=64, num_layers=2, dropout=0.1).to(device)\n",
        "train_model(model, train_loader, val_loader, epochs=20, lr=1e-3, device=device)\n",
        "\n",
        "val_pred = predict_all(model, val_loader, device)\n",
        "test_pred = predict_all(model, test_loader, device)\n",
        "\n",
        "val_idx = val_df['datetime'].iloc[window:].values\n",
        "test_idx = test_df['datetime'].iloc[window:].values\n",
        "plot_series(val_idx, yva_seq.squeeze(-1), val_pred, 'LSTM 검증 예측 vs 실제')\n",
        "plot_series(test_idx, yte_seq.squeeze(-1), test_pred, 'LSTM 테스트 예측 vs 실제')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 성능 평가 및 잔차 분석\n",
        "- MAE/MSE/RMSE/MAPE, 방향정확도(DA)\n",
        "- 예측 vs 실제 플롯, 잔차 시각화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "    mae = float(np.mean(np.abs(y_true - y_pred)))\n",
        "    mse = float(np.mean((y_true - y_pred)**2))\n",
        "    rmse = float(np.sqrt(mse))\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-8, None))) * 100.0)\n",
        "    prev = np.concatenate([[y_true[0]], y_true[:-1]])\n",
        "    da = float(np.mean((np.sign(y_true - prev) == np.sign(y_pred - prev)).astype(float)))\n",
        "    return mae, mse, rmse, mape, da\n",
        "\n",
        "print('LSTM/VAL', compute_metrics(yva_seq.squeeze(-1), val_pred))\n",
        "print('LSTM/TEST', compute_metrics(yte_seq.squeeze(-1), test_pred))\n",
        "\n",
        "# 잔차 분석\n",
        "residual = yte_seq.squeeze(-1) - test_pred\n",
        "plt.figure(figsize=(12,3.5))\n",
        "plt.plot(test_idx, residual)\n",
        "plt.title('잔차 시계열 (LSTM)')\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(residual, bins=30)\n",
        "plt.title('잔차 분포 (LSTM)')\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. 강의 요약 및 다음 단계\n",
        "- 윈도우 기반 시퀀스 모델의 장단점\n",
        "- 베이스라인 대비 개선 포인트 및 추가 실험 아이디어(스택드 LSTM, Dropout, 다중 스텝 예측)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
