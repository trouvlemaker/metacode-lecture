{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43797ad7",
   "metadata": {},
   "source": [
    "# 🚀 2강: Transfer Learning 실습 — MobileNetV2 + Grad-CAM\n",
    "\n",
    "**목표**\n",
    "- 사전학습(Imagenet) 기반 **MobileNetV2**를 활용하여 작은 데이터에도 빠르게 높은 성능을 얻습니다.\n",
    "- **Feature Extraction → Fine-tuning** 두 단계로 학습하고, **Grad-CAM**으로 설명가능성을 더합니다.\n",
    "\n",
    "**소요시간 가이드 (30분)**\n",
    "- 도입/데이터 준비(8분), 모델/학습(15분), 평가/시각화/Grad-CAM(7분)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1769ca",
   "metadata": {},
   "source": [
    "## 0. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54537f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4545c9",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 로드 & 전처리 (CIFAR-10 → 224×224로 리사이즈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_va, y_tr, y_va = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "def resize_images(x):\n",
    "    x = tf.image.resize(x, (IMG_SIZE, IMG_SIZE)).numpy()\n",
    "    return x\n",
    "\n",
    "x_tr = resize_images(x_tr)\n",
    "x_va = resize_images(x_va)\n",
    "x_te = resize_images(x_test)\n",
    "\n",
    "print('Shapes:', x_tr.shape, x_va.shape, x_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🔁 데이터 증강 (실시간)\n",
    "data_aug = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9374b",
   "metadata": {},
   "source": [
    "## 2. 사전학습 모델 — Feature Extraction 단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🧠 MobileNetV2 (ImageNet 사전학습) 불러오기\n",
    "base = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
    "base.trainable = False  # Feature Extraction 단계에서는 Freeze\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_aug(inputs)\n",
    "x = preprocess_input(x)  # MobileNetV2 전처리\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(patience=2, restore_best_weights=True, monitor='val_accuracy')\n",
    "hist_fe = model.fit(x_tr, y_tr, validation_data=(x_va, y_va), epochs=5, batch_size=64, callbacks=[es], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 📈 학습 곡선 함수\n",
    "def plot_history(history, title='Training History'):\n",
    "    h = history.history\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['accuracy'], label='train_acc')\n",
    "    plt.plot(h['val_accuracy'], label='val_acc')\n",
    "    plt.title(title); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['loss'], label='train_loss')\n",
    "    plt.plot(h['val_loss'], label='val_loss')\n",
    "    plt.title(title); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_history(hist_fe, 'Feature Extraction History')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d3d7f",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning 단계 (상위 블록만 재학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🔓 일부 레이어 Unfreeze (상위 50개 레이어 예시)\n",
    "base.trainable = True\n",
    "for layer in base.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist_ft = model.fit(x_tr, y_tr, validation_data=(x_va, y_va), epochs=5, batch_size=64, callbacks=[EarlyStopping(patience=2, restore_best_weights=True, monitor='val_accuracy')], verbose=1)\n",
    "plot_history(hist_ft, 'Fine-tuning History')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e132282",
   "metadata": {},
   "source": [
    "## 4. 테스트 평가 및 혼동행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91978c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_te, y_test, verbose=0)\n",
    "print(f\"[Transfer] Test Acc: {test_acc:.4f} | Loss: {test_loss:.4f}\")\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_te, verbose=0), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred, labels=range(10))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix (Transfer)')\n",
    "plt.colorbar(); plt.xticks(range(10), class_names, rotation=45); plt.yticks(range(10), class_names)\n",
    "plt.tight_layout(); plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()\n",
    "\n",
    "print('\\n[Classification Report]\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397af2f",
   "metadata": {},
   "source": [
    "## 5. 오분류 사례 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d13249",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_idx = np.where(y_pred != y_test)[0]\n",
    "plt.figure(figsize=(8,8))\n",
    "for i, id_ in enumerate(mis_idx[:16], 1):\n",
    "    plt.subplot(4,4,i)\n",
    "    plt.imshow(x_te[id_].astype('uint8'))\n",
    "    plt.title(f\"T:{class_names[y_test[id_]]}\\nP:{class_names[y_pred[id_]]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a08f3",
   "metadata": {},
   "source": [
    "## 6. Grad-CAM으로 모델 시각적 설명 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🔥 Grad-CAM 구현 함수\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        class_idx = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-9)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "## 📌 MobileNetV2의 마지막 Conv 레이어 이름 예: 'Conv_1'\n",
    "last_conv = 'Conv_1'\n",
    "\n",
    "## 샘플 이미지 하나 선택\n",
    "idx = np.random.randint(0, len(x_te))\n",
    "img = x_te[idx].astype('uint8')\n",
    "img_input = np.expand_dims(img, axis=0)\n",
    "img_pre = preprocess_input(img_input.astype('float32'))\n",
    "\n",
    "## Heatmap 생성\n",
    "heatmap = make_gradcam_heatmap(img_pre, model, last_conv)\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "## 원본 위에 오버레이\n",
    "import cv2\n",
    "heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "overlay = cv2.addWeighted(img, 0.6, heatmap_color, 0.4, 0)\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.subplot(1,3,1); plt.imshow(img); plt.title('Original'); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(heatmap, cmap='jet'); plt.title('Grad-CAM Heatmap'); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.imshow(overlay[..., ::-1]); plt.title('Overlay'); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5992945d",
   "metadata": {},
   "source": [
    "## 7. 모델 저장/로딩 및 실전 팁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2503d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('transfer_mobilenetv2_finetuned.keras')\n",
    "print('모델 저장 완료: transfer_mobilenetv2_finetuned.keras')\n",
    "\n",
    "loaded = tf.keras.models.load_model('transfer_mobilenetv2_finetuned.keras')\n",
    "print('로딩 테스트:', loaded.evaluate(x_te, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbde7e6",
   "metadata": {},
   "source": [
    "### ✅ 실전 팁\n",
    "- 작은 데이터셋에서는 **Feature Extraction** 만으로도 충분히 높은 성능을 얻을 수 있음.\n",
    "- Fine-tuning은 **상위 몇 개 블록만** 열고 학습률을 **아주 작게** 설정하세요 (예: 1e-5).\n",
    "- 입력 해상도를 모델 권장값(224×224)로 맞추고, 전처리 함수(`preprocess_input`)를 꼭 사용하세요.\n",
    "- 설명가능성(Grad-CAM)을 통해 **모델이 어디를 보고 판단하는지** 확인하면 품질 관리에 큰 도움이 됩니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
