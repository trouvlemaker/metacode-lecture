{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcb05ee",
   "metadata": {},
   "source": [
    "# ğŸš€ 2ê°• (CPU ì‹¤ìŠµ + GPU ì˜µì…˜): **DeiT-Tiny ì „ì´í•™ìŠµ** â€” ê°œë… â†’ ë°ì´í„° ì¤€ë¹„ â†’ êµ¬ì¡° íƒìƒ‰ â†’ í•™ìŠµ ëª¨ë‹ˆí„°ë§ â†’ Attention â†’ í† ë¡ /ì±Œë¦°ì§€\n",
    "\n",
    "> **ì´¬ì˜ìš© ìŠ¤í¬ë¦½íŠ¸ ê°€ì´ë“œ** í¬í•¨. CPUì—ì„œëŠ” **Feature Extraction**ê¹Œì§€ë§Œ ì‹¤í–‰, **Fine-tuningì€ GPU ì˜µì…˜**ìœ¼ë¡œ ê²°ê³¼ë§Œ ì„¤ëª…í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6f2e0",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5915362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets accelerate evaluate\n",
    "import os, numpy as np, random, matplotlib.pyplot as plt, torch, torch.nn.functional as F\n",
    "from torchvision import datasets as tvdatasets\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db9627",
   "metadata": {},
   "source": [
    "## 1. ì „ì´í•™ìŠµ ê°œë… ë„ì… (â‰ˆ5ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- **ImageNet**: 1ì²œ ê°œ í´ë˜ìŠ¤, ìˆ˜ë°±ë§Œ ì¥ ì´ë¯¸ì§€ â†’ ì¼ë°˜ì  ì‹œê° íŒ¨í„´ì„ ë¯¸ë¦¬ í•™ìŠµ  \n",
    "- **ë¹„ìœ **: ì˜ì–´ë¥¼ ì˜í•˜ë©´ ë…ì¼ì–´ë„ ë¹¨ë¦¬ ë°°ìš´ë‹¤(ë¬¸ì/ë¬¸ë²•/í‘œí˜„ ê³µìœ )  \n",
    "- **ì „ëµ ë¹„êµ**: Feature Extraction(ë°±ë³¸ freeze) vs Fine-tuning(ì¼ë¶€/ì „ì²´ unfreeze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ac6cc",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ì¤€ë¹„ ì‹¬í™” (â‰ˆ5ë¶„)\n",
    "\n",
    "**ì§ˆë¬¸:** â€œì‘ì€ ë°ì´í„°ë¡œë„ í•™ìŠµì´ ê°€ëŠ¥í• ê¹Œìš”?â€  \n",
    "- CIFAR-10 ì´ë¯¸ì§€ë¥¼ 224Ã—224ë¡œ resize (ëª¨ë¸ ì…ë ¥ ê·œê²©)  \n",
    "- ì›ë³¸ vs resize ë¹„êµ ì‹œê°í™”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce839100",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='./data'\n",
    "train_full = tvdatasets.CIFAR10(root=root, train=True, download=True)\n",
    "test_set   = tvdatasets.CIFAR10(root=root, train=False, download=True)\n",
    "class_names = train_full.classes\n",
    "\n",
    "def to_numpy_list(tv_dataset):\n",
    "    imgs, labs = [], []\n",
    "    for img, lab in tv_dataset:\n",
    "        imgs.append(np.array(img)); labs.append(lab)\n",
    "    return imgs, labs\n",
    "\n",
    "images_train, labels_train = to_numpy_list(train_full)\n",
    "images_test,  labels_test  = to_numpy_list(test_set)\n",
    "\n",
    "# Subset for CPU\n",
    "SUBSET=0.2\n",
    "idx = np.random.RandomState(42).permutation(len(images_train))\n",
    "sel = idx[:int(len(images_train)*SUBSET)]\n",
    "images_train = [images_train[i] for i in sel]\n",
    "labels_train = [labels_train[i] for i in sel]\n",
    "\n",
    "tr_imgs, va_imgs, tr_lbls, va_lbls = train_test_split(images_train, labels_train, test_size=0.2, stratify=labels_train, random_state=42)\n",
    "\n",
    "# ì›ë³¸ vs resize ë¯¸ë¦¬ë³´ê¸°\n",
    "from PIL import Image\n",
    "sample = Image.fromarray(tr_imgs[0])\n",
    "resized = sample.resize((224,224))\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1); plt.imshow(sample); plt.title('Original'); plt.axis('off')\n",
    "plt.subplot(1,2,2); plt.imshow(resized); plt.title('Resized 224'); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"image\": tr_imgs, \"label\": tr_lbls}),\n",
    "    \"validation\": Dataset.from_dict({\"image\": va_imgs, \"label\": va_lbls}),\n",
    "    \"test\": Dataset.from_dict({\"image\": images_test, \"label\": labels_test})\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b3334",
   "metadata": {},
   "source": [
    "## 3. ì‚¬ì „í•™ìŠµ ëª¨ë¸ êµ¬ì¡° íƒìƒ‰ (â‰ˆ5ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- `model.config`ë¡œ **hidden_size, num_hidden_layers, num_attention_heads** í™•ì¸  \n",
    "- íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 1ê°• CNNê³¼ ë¹„êµ  \n",
    "- **íŒ¨ì¹˜ ë¶„í• **: 224Ã—224 ì´ë¯¸ì§€ë¥¼ 16Ã—16 íŒ¨ì¹˜ â†’ 14Ã—14=196 í† í° + CLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16071906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/deit-tiny-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=10,\n",
    "    id2label={i:c for i,c in enumerate(class_names)},\n",
    "    label2id={c:i for i,c in enumerate(class_names)},\n",
    ").to(device)\n",
    "\n",
    "print(model.config)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Trainable params (initial):', num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(examples):\n",
    "    inputs = image_processor(images=examples[\"image\"], return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = torch.tensor(examples[\"label\"])\n",
    "    return inputs\n",
    "\n",
    "ds = ds.with_transform(preprocess_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480e4bb",
   "metadata": {},
   "source": [
    "## 4. (CPU) Feature Extraction í•™ìŠµ & ëª¨ë‹ˆí„°ë§ (â‰ˆ5ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- ë°±ë³¸ ë™ê²°ë¡œ **ë¹ ë¥´ê²Œ ìˆ˜ë ´**í•˜ëŠ” ëª¨ìŠµì„ Accuracy ê³¡ì„ ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47686274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone\n",
    "for p in model.base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def collate_fn(batch):\n",
    "    out = {\"pixel_values\": torch.stack([b[\"pixel_values\"] for b in batch])}\n",
    "    out[\"labels\"] = torch.tensor([b[\"labels\"] for b in batch])\n",
    "    return out\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds), \"f1_macro\": f1_score(labels, preds, average=\"macro\")}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./deit_tiny_fe\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "fe_metrics = trainer.evaluate()\n",
    "fe_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed73c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy ê³¡ì„  í”Œë¡¯: Trainer ë¡œê·¸ì—ì„œ ì¶”ì¶œ\n",
    "logs = trainer.state.log_history\n",
    "ep, tr_acc, va_acc = [], [], []\n",
    "for l in logs:\n",
    "    if 'epoch' in l and ('eval_accuracy' in l or 'accuracy' in l):\n",
    "        # eval ë‹¨ê³„\n",
    "        if 'eval_accuracy' in l:\n",
    "            ep.append(l['epoch']); va_acc.append(l['eval_accuracy'])\n",
    "    # (ì£¼ì˜) train_accuracyëŠ” ê¸°ë³¸ ë¡œê·¸ì— ì—†ìŒ â†’ evalë§Œ í”Œë¡¯\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ep, va_acc, marker='o')\n",
    "plt.title('Validation Accuracy (Feature Extraction)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Acc'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2134c19",
   "metadata": {},
   "source": [
    "## 5. (GPU ì˜µì…˜) Fine-tuning â€” ì½”ë“œë§Œ ì†Œê°œ (â‰ˆ5ë¶„)\n",
    "\n",
    "âš ï¸ CPUì—ì„œëŠ” ë§¤ìš° ëŠë¦¼. **GPU(Colab ë“±)** ì—ì„œë§Œ ì‹¤í–‰ ê¶Œì¥.  \n",
    "ì´¬ì˜ ì‹œ **ì½”ë“œ ì„¤ëª… + ë¯¸ë¦¬ í•™ìŠµëœ ê²°ê³¼(ìŠ¤í¬ë¦°ìƒ·)** ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_FINETUNE = False\n",
    "if DO_FINETUNE:\n",
    "    # ìƒìœ„ ë¸”ë¡ ì¼ë¶€ë§Œ unfreeze\n",
    "    for name, p in model.base_model.named_parameters():\n",
    "        if any(k in name for k in [\"layer.10\", \"layer.11\"]):\n",
    "            p.requires_grad = True\n",
    "\n",
    "    args_ft = TrainingArguments(\n",
    "        output_dir=\"./deit_tiny_ft\",\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    trainer_ft = Trainer(\n",
    "        model=model,\n",
    "        args=args_ft,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        tokenizer=image_processor,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer_ft.train()\n",
    "    ft_metrics = trainer_ft.evaluate()\n",
    "    print(ft_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e99ad",
   "metadata": {},
   "source": [
    "## 6. í‰ê°€ + Confusion Matrix + Attention ì‹œê°í™” (â‰ˆ5ë¶„)\n",
    "\n",
    "**ë§í•˜ê¸° í¬ì¸íŠ¸:**  \n",
    "- Feature Extractionë§Œìœ¼ë¡œë„ ê½¤ ë†’ì€ ì •í™•ë„.  \n",
    "- **Attention overlay**ë¡œ ëª¨ë¸ì´ ì–´ë””ì— ì£¼ëª©í–ˆëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª….\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dccb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_metrics = trainer.evaluate(ds[\"test\"])\n",
    "print('[Test] Accuracy:', test_metrics.get('eval_accuracy', None))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_logits(dset):\n",
    "    loader = DataLoader(dset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        preds.append(out.logits.cpu().numpy())\n",
    "        labels.append(batch[\"labels\"].cpu().numpy())\n",
    "    return np.concatenate(preds), np.concatenate(labels)\n",
    "\n",
    "logits, y_true = predict_logits(ds[\"test\"])\n",
    "y_pred = logits.argmax(1)\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred,labels=list(range(10)))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest'); plt.title('Confusion Matrix (DeiT-Tiny)'); plt.colorbar()\n",
    "plt.xticks(range(10), class_names, rotation=45); plt.yticks(range(10), class_names)\n",
    "plt.tight_layout(); plt.xlabel('Pred'); plt.ylabel('True'); plt.show()\n",
    "\n",
    "print('\\n[Classification Report]\\n', classification_report(y_true,y_pred,target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Rollout (ê°„ë‹¨) â†’ ë§ˆì§€ë§‰ ë ˆì´ì–´ í‰ê·  attention ì‚¬ìš©\n",
    "model.config.output_attentions = True\n",
    "_ = model.eval()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ í•˜ë‚˜\n",
    "from PIL import Image\n",
    "raw_test = tvdatasets.CIFAR10(root='./data', train=False, download=False)\n",
    "img_pil, lab = raw_test[0]\n",
    "img_resized = img_pil.resize((224,224))\n",
    "inp = image_processor(images=[img_resized], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model(pixel_values=inp['pixel_values'].to(device))\n",
    "attn = out.attentions[-1][0]  # (heads, tokens, tokens)\n",
    "attn_mean = attn.mean(0)      # (tokens, tokens)\n",
    "cls_to_patches = attn_mean[0, 1:]  # (196,)\n",
    "attn_map = cls_to_patches.reshape(14,14).cpu().numpy()\n",
    "attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min() + 1e-9)\n",
    "\n",
    "# ì—…ìƒ˜í”Œë§ & overlay\n",
    "import cv2\n",
    "attn_up = cv2.resize(attn_map, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "img_np = np.array(img_resized)\n",
    "heat = (attn_up*255).astype(np.uint8)\n",
    "heat_color = cv2.applyColorMap(heat, cv2.COLORMAP_JET)\n",
    "overlay = cv2.addWeighted(img_np, 0.6, heat_color, 0.4, 0)\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.subplot(1,3,1); plt.imshow(img_np); plt.title('Original'); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(heat, cmap='jet'); plt.title('Attention'); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.imshow(overlay[:,:,::-1]); plt.title('Overlay'); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b241aba",
   "metadata": {},
   "source": [
    "## 7. ì‹¤ì „ ì ìš© í† ë¡  (â‰ˆ3ë¶„)\n",
    "\n",
    "- **ì˜ë£Œì˜ìƒ**: ë³‘ë³€ í›„ë³´ì˜ì—­ ë¶„ë¥˜/íƒì§€(ìœ¤ë¦¬Â·ê·œì œ ê³ ë ¤)  \n",
    "- **ë¦¬í…Œì¼**: ì œí’ˆ ì´ë¯¸ì§€ ë¶„ë¥˜/ê²€ìƒ‰, ìƒí’ˆ ì†ì„± íƒœê¹…  \n",
    "- **ì•ˆì „**: CCTV í–‰ë™ ì¸ì‹, í™”ì¬/ì—°ê¸° ê°ì§€(ë°ì´í„° í’ˆì§ˆ/ë¼ë²¨ ì •í™•ë„ ì¤‘ìš”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048135e",
   "metadata": {},
   "source": [
    "## 8. ì‹¤ìŠµ ì±Œë¦°ì§€ (â‰ˆ2ë¶„)\n",
    "\n",
    "- **ë°±ë³¸ êµì²´**: `google/vit-small`, `facebook/deit-base`, ë˜ëŠ” MobileNet/EfficientNet(HF/timm)ë¡œ ë¹„êµ  \n",
    "- **ë°ì´í„° í¬ê¸° ë³€í™”**: `SUBSET=0.5`ë¡œ ëŠ˜ë ¤ ì„±ëŠ¥/ì‹œê°„ íŠ¸ë ˆì´ë“œì˜¤í”„ ê´€ì°°  \n",
    "- **ì •ë°€ ì‹œê°í™”**: Grad-CAM(ConvNet), Attention Rollout(Transformer) ì‹¬í™”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
