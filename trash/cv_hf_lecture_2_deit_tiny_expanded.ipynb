{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcb05ee",
   "metadata": {},
   "source": [
    "# 🚀 2강 (CPU 실습 + GPU 옵션): **DeiT-Tiny 전이학습** — 개념 → 데이터 준비 → 구조 탐색 → 학습 모니터링 → Attention → 토론/챌린지\n",
    "\n",
    "> **촬영용 스크립트 가이드** 포함. CPU에서는 **Feature Extraction**까지만 실행, **Fine-tuning은 GPU 옵션**으로 결과만 설명하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6f2e0",
   "metadata": {},
   "source": [
    "## 0. 환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5915362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets accelerate evaluate\n",
    "import os, numpy as np, random, matplotlib.pyplot as plt, torch, torch.nn.functional as F\n",
    "from torchvision import datasets as tvdatasets\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db9627",
   "metadata": {},
   "source": [
    "## 1. 전이학습 개념 도입 (≈5분)\n",
    "\n",
    "**말하기 포인트:**  \n",
    "- **ImageNet**: 1천 개 클래스, 수백만 장 이미지 → 일반적 시각 패턴을 미리 학습  \n",
    "- **비유**: 영어를 잘하면 독일어도 빨리 배운다(문자/문법/표현 공유)  \n",
    "- **전략 비교**: Feature Extraction(백본 freeze) vs Fine-tuning(일부/전체 unfreeze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ac6cc",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비 심화 (≈5분)\n",
    "\n",
    "**질문:** “작은 데이터로도 학습이 가능할까요?”  \n",
    "- CIFAR-10 이미지를 224×224로 resize (모델 입력 규격)  \n",
    "- 원본 vs resize 비교 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce839100",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='./data'\n",
    "train_full = tvdatasets.CIFAR10(root=root, train=True, download=True)\n",
    "test_set   = tvdatasets.CIFAR10(root=root, train=False, download=True)\n",
    "class_names = train_full.classes\n",
    "\n",
    "def to_numpy_list(tv_dataset):\n",
    "    imgs, labs = [], []\n",
    "    for img, lab in tv_dataset:\n",
    "        imgs.append(np.array(img)); labs.append(lab)\n",
    "    return imgs, labs\n",
    "\n",
    "images_train, labels_train = to_numpy_list(train_full)\n",
    "images_test,  labels_test  = to_numpy_list(test_set)\n",
    "\n",
    "# Subset for CPU\n",
    "SUBSET=0.2\n",
    "idx = np.random.RandomState(42).permutation(len(images_train))\n",
    "sel = idx[:int(len(images_train)*SUBSET)]\n",
    "images_train = [images_train[i] for i in sel]\n",
    "labels_train = [labels_train[i] for i in sel]\n",
    "\n",
    "tr_imgs, va_imgs, tr_lbls, va_lbls = train_test_split(images_train, labels_train, test_size=0.2, stratify=labels_train, random_state=42)\n",
    "\n",
    "# 원본 vs resize 미리보기\n",
    "from PIL import Image\n",
    "sample = Image.fromarray(tr_imgs[0])\n",
    "resized = sample.resize((224,224))\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1); plt.imshow(sample); plt.title('Original'); plt.axis('off')\n",
    "plt.subplot(1,2,2); plt.imshow(resized); plt.title('Resized 224'); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"image\": tr_imgs, \"label\": tr_lbls}),\n",
    "    \"validation\": Dataset.from_dict({\"image\": va_imgs, \"label\": va_lbls}),\n",
    "    \"test\": Dataset.from_dict({\"image\": images_test, \"label\": labels_test})\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b3334",
   "metadata": {},
   "source": [
    "## 3. 사전학습 모델 구조 탐색 (≈5분)\n",
    "\n",
    "**말하기 포인트:**  \n",
    "- `model.config`로 **hidden_size, num_hidden_layers, num_attention_heads** 확인  \n",
    "- 파라미터 수 계산 → 1강 CNN과 비교  \n",
    "- **패치 분할**: 224×224 이미지를 16×16 패치 → 14×14=196 토큰 + CLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16071906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/deit-tiny-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=10,\n",
    "    id2label={i:c for i,c in enumerate(class_names)},\n",
    "    label2id={c:i for i,c in enumerate(class_names)},\n",
    ").to(device)\n",
    "\n",
    "print(model.config)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Trainable params (initial):', num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(examples):\n",
    "    inputs = image_processor(images=examples[\"image\"], return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = torch.tensor(examples[\"label\"])\n",
    "    return inputs\n",
    "\n",
    "ds = ds.with_transform(preprocess_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480e4bb",
   "metadata": {},
   "source": [
    "## 4. (CPU) Feature Extraction 학습 & 모니터링 (≈5분)\n",
    "\n",
    "**말하기 포인트:**  \n",
    "- 백본 동결로 **빠르게 수렴**하는 모습을 Accuracy 곡선으로 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47686274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone\n",
    "for p in model.base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def collate_fn(batch):\n",
    "    out = {\"pixel_values\": torch.stack([b[\"pixel_values\"] for b in batch])}\n",
    "    out[\"labels\"] = torch.tensor([b[\"labels\"] for b in batch])\n",
    "    return out\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds), \"f1_macro\": f1_score(labels, preds, average=\"macro\")}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./deit_tiny_fe\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    "    no_cuda=not torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "fe_metrics = trainer.evaluate()\n",
    "fe_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed73c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy 곡선 플롯: Trainer 로그에서 추출\n",
    "logs = trainer.state.log_history\n",
    "ep, tr_acc, va_acc = [], [], []\n",
    "for l in logs:\n",
    "    if 'epoch' in l and ('eval_accuracy' in l or 'accuracy' in l):\n",
    "        # eval 단계\n",
    "        if 'eval_accuracy' in l:\n",
    "            ep.append(l['epoch']); va_acc.append(l['eval_accuracy'])\n",
    "    # (주의) train_accuracy는 기본 로그에 없음 → eval만 플롯\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ep, va_acc, marker='o')\n",
    "plt.title('Validation Accuracy (Feature Extraction)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Acc'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2134c19",
   "metadata": {},
   "source": [
    "## 5. (GPU 옵션) Fine-tuning — 코드만 소개 (≈5분)\n",
    "\n",
    "⚠️ CPU에서는 매우 느림. **GPU(Colab 등)** 에서만 실행 권장.  \n",
    "촬영 시 **코드 설명 + 미리 학습된 결과(스크린샷)** 를 보여주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_FINETUNE = False\n",
    "if DO_FINETUNE:\n",
    "    # 상위 블록 일부만 unfreeze\n",
    "    for name, p in model.base_model.named_parameters():\n",
    "        if any(k in name for k in [\"layer.10\", \"layer.11\"]):\n",
    "            p.requires_grad = True\n",
    "\n",
    "    args_ft = TrainingArguments(\n",
    "        output_dir=\"./deit_tiny_ft\",\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    trainer_ft = Trainer(\n",
    "        model=model,\n",
    "        args=args_ft,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        tokenizer=image_processor,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer_ft.train()\n",
    "    ft_metrics = trainer_ft.evaluate()\n",
    "    print(ft_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e99ad",
   "metadata": {},
   "source": [
    "## 6. 평가 + Confusion Matrix + Attention 시각화 (≈5분)\n",
    "\n",
    "**말하기 포인트:**  \n",
    "- Feature Extraction만으로도 꽤 높은 정확도.  \n",
    "- **Attention overlay**로 모델이 어디에 주목했는지 직관적으로 설명.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dccb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_metrics = trainer.evaluate(ds[\"test\"])\n",
    "print('[Test] Accuracy:', test_metrics.get('eval_accuracy', None))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_logits(dset):\n",
    "    loader = DataLoader(dset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        preds.append(out.logits.cpu().numpy())\n",
    "        labels.append(batch[\"labels\"].cpu().numpy())\n",
    "    return np.concatenate(preds), np.concatenate(labels)\n",
    "\n",
    "logits, y_true = predict_logits(ds[\"test\"])\n",
    "y_pred = logits.argmax(1)\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred,labels=list(range(10)))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest'); plt.title('Confusion Matrix (DeiT-Tiny)'); plt.colorbar()\n",
    "plt.xticks(range(10), class_names, rotation=45); plt.yticks(range(10), class_names)\n",
    "plt.tight_layout(); plt.xlabel('Pred'); plt.ylabel('True'); plt.show()\n",
    "\n",
    "print('\\n[Classification Report]\\n', classification_report(y_true,y_pred,target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Rollout (간단) → 마지막 레이어 평균 attention 사용\n",
    "model.config.output_attentions = True\n",
    "_ = model.eval()\n",
    "\n",
    "# 테스트 샘플 하나\n",
    "from PIL import Image\n",
    "raw_test = tvdatasets.CIFAR10(root='./data', train=False, download=False)\n",
    "img_pil, lab = raw_test[0]\n",
    "img_resized = img_pil.resize((224,224))\n",
    "inp = image_processor(images=[img_resized], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model(pixel_values=inp['pixel_values'].to(device))\n",
    "attn = out.attentions[-1][0]  # (heads, tokens, tokens)\n",
    "attn_mean = attn.mean(0)      # (tokens, tokens)\n",
    "cls_to_patches = attn_mean[0, 1:]  # (196,)\n",
    "attn_map = cls_to_patches.reshape(14,14).cpu().numpy()\n",
    "attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min() + 1e-9)\n",
    "\n",
    "# 업샘플링 & overlay\n",
    "import cv2\n",
    "attn_up = cv2.resize(attn_map, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "img_np = np.array(img_resized)\n",
    "heat = (attn_up*255).astype(np.uint8)\n",
    "heat_color = cv2.applyColorMap(heat, cv2.COLORMAP_JET)\n",
    "overlay = cv2.addWeighted(img_np, 0.6, heat_color, 0.4, 0)\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.subplot(1,3,1); plt.imshow(img_np); plt.title('Original'); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(heat, cmap='jet'); plt.title('Attention'); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.imshow(overlay[:,:,::-1]); plt.title('Overlay'); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b241aba",
   "metadata": {},
   "source": [
    "## 7. 실전 적용 토론 (≈3분)\n",
    "\n",
    "- **의료영상**: 병변 후보영역 분류/탐지(윤리·규제 고려)  \n",
    "- **리테일**: 제품 이미지 분류/검색, 상품 속성 태깅  \n",
    "- **안전**: CCTV 행동 인식, 화재/연기 감지(데이터 품질/라벨 정확도 중요)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048135e",
   "metadata": {},
   "source": [
    "## 8. 실습 챌린지 (≈2분)\n",
    "\n",
    "- **백본 교체**: `google/vit-small`, `facebook/deit-base`, 또는 MobileNet/EfficientNet(HF/timm)로 비교  \n",
    "- **데이터 크기 변화**: `SUBSET=0.5`로 늘려 성능/시간 트레이드오프 관찰  \n",
    "- **정밀 시각화**: Grad-CAM(ConvNet), Attention Rollout(Transformer) 심화\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
