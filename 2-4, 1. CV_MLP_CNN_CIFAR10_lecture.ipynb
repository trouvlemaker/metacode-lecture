{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae167db",
   "metadata": {},
   "source": [
    "## Chapter 2-4, 1강 Computer Vision MLP & CNN(PyTorch, CPU): CIFAR-10 분류 \n",
    "- 데이터 탐색 → MLP vs CNN → 내부 시각화 → 오분류 분석 → 개선\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2804145",
   "metadata": {},
   "source": [
    "### 0. 환경 설정 및 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os, time, random, numpy as np, matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "\n",
    "# GPU 사용 가능 여부 확인 (CUDA가 있으면 GPU, 없으면 CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "# 시드 고정 함수 (재현성 확보를 위해 랜덤성 통제)\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)                   # 파이썬 기본 random 시드 고정\n",
    "    np.random.seed(seed)                # 넘파이 시드 고정\n",
    "    torch.manual_seed(seed)             # 파이토치 시드 고정 (CPU 연산용)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed) # GPU 연산 시 여러 GPU에 동일 시드 적용\n",
    "set_seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# 한글 글꼴 설정\n",
    "plt.rcParams['font.family'] = 'AppleGothic'  # Mac의 기본 한글 글꼴\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09765e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 역정규화(denormalization) 유틸리티 함수\n",
    "# CIFAR-10 데이터셋의 평균과 표준편차 값을 사용\n",
    "# 학습 시 normalize했던 이미지를 다시 원래 색감으로 되돌리는 용도\n",
    "# -------------------------------\n",
    "inv_mean = np.array([0.4914, 0.4822, 0.4465])  # CIFAR-10 채널별 평균\n",
    "inv_std  = np.array([0.2470, 0.2435, 0.2616])  # CIFAR-10 채널별 표준편차\n",
    "\n",
    "def denorm(img_tensor):\n",
    "    \"\"\"\n",
    "    정규화된 이미지를 다시 원래 픽셀값(0~1) 범위로 변환하는 함수\n",
    "\n",
    "    Args:\n",
    "        img_tensor (torch.Tensor): (C,H,W) 형태의 이미지 텐서\n",
    "    \n",
    "     - C (Channels): 채널 수\n",
    "        . RGB 이미지의 경우: 3 (Red, Green, Blue)\n",
    "        . RGBA 이미지의 경우: 4 (Red, Green, Blue, Alpha)\n",
    "        . H (Height): 이미지의 높이 (세로 픽셀 수)\n",
    "        . W (Width): 이미지의 너비 (가로 픽셀 수)\n",
    "        . 예를 들어:\n",
    "            (3, 224, 224): RGB 이미지, 224x224 픽셀\n",
    "            (1, 28, 28): 그레이스케일 이미지, 28x28 픽셀 (MNIST 같은 경우)\n",
    "            (4, 512, 512): RGBA 이미지, 512x512 픽셀\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: (H,W,C) 형태의 이미지, 픽셀 범위 0~1\n",
    "    \"\"\"\n",
    "    # 텐서를 (H, W, C) 순서로 변환 후 numpy 배열로 변경\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    # 정규화 해제: (이미지 * 표준편차) + 평균\n",
    "    img = img * inv_std + inv_mean\n",
    "    # 0~1 범위로 잘라내기\n",
    "    return np.clip(img, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab86de3",
   "metadata": {},
   "source": [
    "### 1. 데이터 다운로드 및 DataLoader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 전처리: 정규화 (Normalization)\n",
    "# CIFAR-10 데이터셋의 채널별 평균과 표준편차를 사용하여\n",
    "# 입력 이미지를 평균 0, 표준편차 1에 가깝게 맞춰줌\n",
    "# -------------------------------\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 이미지를 Tensor로 변환 (0~255 → 0~1 범위), 차원 순서를 (H, W, C)에서 (C, H, W)로 변경\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),  # 채널별 평균 (R,G,B)\n",
    "        (0.2470, 0.2435, 0.2616)   # 채널별 표준편차 (R,G,B)\n",
    "    )\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e28968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 데이터셋 다운로드 및 로드\n",
    "# root 폴더에 CIFAR-10 데이터셋이 없으면 자동 다운로드\n",
    "# -------------------------------\n",
    "root = './data'\n",
    "train_full = datasets.CIFAR10(\n",
    "    root=root, train=True, download=True, transform=train_tf #정규화만 실행\n",
    ")  # 전체 학습용(train=True) 데이터셋\n",
    "test_set = datasets.CIFAR10(\n",
    "    root=root, train=False, download=True, transform=test_tf #정규화만 실행\n",
    ")  # 테스트용(train=False) 데이터셋\n",
    "\n",
    "# CIFAR-10 클래스 이름 (10개 클래스: airplane, car, bird, cat ...)\n",
    "class_names = train_full.classes\n",
    "\n",
    "# -------------------------------\n",
    "# 학습 데이터 → 학습(train) / 검증(validation) 데이터로 분리\n",
    "# -------------------------------\n",
    "val_ratio = 0.2                           # 전체 학습 데이터 중 20%를 검증용으로 사용\n",
    "val_len   = int(len(train_full) * val_ratio)  # 검증 데이터 개수\n",
    "train_len = len(train_full) - val_len         # 학습 데이터 개수\n",
    "\n",
    "# 랜덤하게 split (train/val 구분)\n",
    "train_set, val_set = random_split(\n",
    "    train_full, [train_len, val_len]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dab88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# DataLoader 생성\n",
    "# 학습 시 배치 단위로 데이터를 불러오고,\n",
    "# shuffle 여부 및 멀티프로세싱(num_workers) 지정 가능\n",
    "# -------------------------------\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=128, shuffle=True, num_workers=0\n",
    ")  # 학습용 (데이터 무작위 섞음)\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=128, shuffle=False, num_workers=0\n",
    ")    # 검증용 (섞지 않음)\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=128, shuffle=False, num_workers=0\n",
    ")   # 테스트용 (섞지 않음)\n",
    "\n",
    "# -------------------------------\n",
    "# 데이터셋 크기 및 샘플 이미지 형태 출력\n",
    "# -------------------------------\n",
    "print('Train/Val/Test sizes:', len(train_set), len(val_set), len(test_set))\n",
    "# 학습용, 검증용 테스트용 데이터셋의 크기를 출력하는 것으로\n",
    "# Train/Val/Test sizes: 40000 10000 10000이라고 출력됨\n",
    "print('Sample image shape (C,H,W):', train_set[0][0].shape)\n",
    "# 샘플 이미지의 텐서 크기를 출력하는 것으로\n",
    "# torch.Size([3, 32, 32])라고 출력되는데 \n",
    "# 채널은 RGB(3갸)이며 이미지 크기는 32픽셀 × 32픽셀이라는 의미\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46dd442",
   "metadata": {},
   "source": [
    "### 2. 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82964fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 클래스별 샘플 5장씩 시각화\n",
    "import math\n",
    "fig, axes = plt.subplots(10, 5, figsize=(8,16))  # 10행(클래스), 5열(샘플) 서브플롯 생성\n",
    "counts = {c:0 for c in class_names}              # 각 클래스별로 몇 장을 그렸는지 카운트\n",
    "\n",
    "# train_loader에서 배치 단위로 이미지와 라벨 가져오기\n",
    "for x, y in train_loader:\n",
    "    # 배치 내 이미지(img)와 라벨(lab) 순회\n",
    "    for img, lab in zip(x, y):\n",
    "        c = class_names[lab.item()]  # 정수 라벨 → 클래스 이름으로 변환\n",
    "        # 해당 클래스에서 아직 5장을 다 못 그렸을 경우에만 처리\n",
    "        if counts[c] < 5:\n",
    "            # 클래스 인덱스(row), 현재까지 그린 장 수(col)로 위치 결정\n",
    "            ax = axes[class_names.index(c), counts[c]]\n",
    "            # 정규화 해제 후 이미지 출력\n",
    "            ax.imshow(denorm(img))\n",
    "            ax.set_axis_off()  # x, y 축 제거\n",
    "            # 각 클래스 첫 번째 열에는 클래스 이름을 y축 레이블로 표시\n",
    "            if counts[c] == 0:\n",
    "                ax.set_ylabel(c, rotation=0, labelpad=30, va='center')\n",
    "            counts[c] += 1  # 카운트 증가\n",
    "    # 모든 클래스에서 5장씩 다 모으면 반복 종료\n",
    "    if all(counts[c] >= 5 for c in class_names):\n",
    "        break\n",
    "\n",
    "# 전체 제목 + 레이아웃 정리 + 시각화\n",
    "plt.suptitle('각 클래스 샘플 5장', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1eca4",
   "metadata": {},
   "source": [
    "#### CIFAR-10의 해상도 특징\n",
    "- 크기: 32 × 32 픽셀\n",
    "- 채널: RGB (3채널)\n",
    "- 데이터 수: 60,000장 (train 50,000 + test 10,000)\n",
    "- 클래스 수: 10개 (비행기, 자동차, 새, 고양이, 사슴, 개, 개구리, 말, 배, 트럭)\n",
    "- 한 이미지가 겨우 32 × 32 픽셀이라 사람 눈에는 뭉개진 저해상도 사진처럼 보이는 게 정상입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동차 vs 개 RGB 채널별 히스토그램 비교\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# -------------------------------\n",
    "# raw_train: 정규화/텐서 변환 전, 원본 PIL 이미지 접근용 CIFAR-10 데이터셋\n",
    "# (ToTensor, Normalize 같은 transform을 적용하지 않고 원본 픽셀값을 보려는 용도)\n",
    "# -------------------------------\n",
    "# CIFAR-10 원본 학습 데이터셋 불러오기 (PIL 이미지 형태)\n",
    "raw_train = CIFAR10(\n",
    "    root='./data',       # 데이터 저장/불러올 경로\n",
    "    train=True,          # 학습용 데이터셋 (True), 테스트셋은 False\n",
    "    download=False       # 데이터 없으면 다운로드 여부 (False면 로컬에 있어야 함)\n",
    ")\n",
    "\n",
    "# 특정 클래스(label_name)의 이미지를 모아서 Numpy 배열로 반환하는 함수\n",
    "def collect_pixels(label_name, max_imgs=200):\n",
    "    idx_label = class_names.index(label_name)  # 클래스 이름 → 정수 라벨 인덱스\n",
    "    vals = []   # 이미지를 담을 리스트\n",
    "    cnt = 0\n",
    "    for img, lab in raw_train:   # 원본 학습 데이터셋 순회\n",
    "        if lab == idx_label:     # 해당 라벨의 이미지일 경우\n",
    "            vals.append(np.array(img))  # PIL.Image → numpy array (H,W,3), 값 범위 0~255\n",
    "            cnt += 1\n",
    "            if cnt >= max_imgs:  # max_imgs 개수만큼 모았으면 종료\n",
    "                break\n",
    "    # (N, H, W, 3) 형태의 배열로 합치기 (N = 이미지 개수)\n",
    "    arr = np.stack(vals, axis=0)\n",
    "    return arr\n",
    "\n",
    "# 자동차(automobile) 이미지 200장 수집\n",
    "cars = collect_pixels('automobile', max_imgs=200)\n",
    "# 개(dog) 이미지 200장 수집\n",
    "dogs = collect_pixels('dog', max_imgs=200)\n",
    "\n",
    "# -------------------------------\n",
    "# RGB 채널별 히스토그램 비교 시각화\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10,3))\n",
    "for i, ch in enumerate(['R','G','B']):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    # 자동차 이미지의 i번째 채널 픽셀 히스토그램\n",
    "    plt.hist(cars[:,:,:,i].ravel(), bins=50, alpha=0.6, label='car')\n",
    "    # 개 이미지의 i번째 채널 픽셀 히스토그램\n",
    "    plt.hist(dogs[:,:,:,i].ravel(), bins=50, alpha=0.6, label='dog')\n",
    "    plt.title(f'Channel {ch} Histogram')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 배열 크기 출력: (N,H,W,C)\n",
    "# 자동차 200장, 개 200장 이미지 수집 결과\n",
    "# -------------------------------\n",
    "print('cars array:', cars.shape, 'dogs array:', dogs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a2b30",
   "metadata": {},
   "source": [
    "##### 자동차(car) vs 개(dog) RGB 채널별 히스토그램 해석\n",
    "- X축: 픽셀 값 (0~255)\n",
    "- Y축: 해당 값의 픽셀 개수 (빈도수)\n",
    "- 색상: 파란색(car), 주황색(dog)\n",
    "\n",
    "##### 1) R 채널 (왼쪽)\n",
    "- 자동차: 낮은 값(0~100) 쪽에 분포 많음 → 차체 색/배경 영향\n",
    "- 개: 중간~높은 값(100~200) 분포 두드러짐 → 갈색/붉은색 털 때문\n",
    "##### 2) G 채널 (가운데)\n",
    "- 자동차: 분포가 넓고 높은 값(200 근처)도 많음 → 하늘/도로 배경 영향\n",
    "- 개: 중간 영역(100~150) 집중 → 털 색깔 영향\n",
    "##### 3) B 채널 (오른쪽)\n",
    "- 자동차: 낮은 값(0~100) + 높은 값(200 이상) 피크 → 배경 하늘 영향\n",
    "- 개: 중간 값에 고르게 분포 → 갈색 털 특성 (파랑 성분 약함)\n",
    "##### 4) 결론:\n",
    "- 자동차는 파랑(B) 성분이 강하고, 개는 R/G 성분이 더 두드러짐\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce5655",
   "metadata": {},
   "source": [
    "### 3. Baseline 비교 (MLP vs CNN)\n",
    "\n",
    "- MLP(Multi-Layer Perceptron)는 공간 정보를 활용하지 못함 → 파라미터 수 대비 비효율적.  \n",
    "- CNN은 지역(커널) 단위로 가중치를 공유 → 효율성↑, 일반화↑.\n",
    "\n",
    "**질문:** “같은 파라미터 수라면 어떤 구조가 더 유리할까요?”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 다층 퍼셉트론 (MLP:Multi-Layer Perceptron) 모델 정의\n",
    "# CIFAR-10 입력(32x32x3)을 일렬로 펼쳐서 전결합층으로 분류\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # (C,H,W) → (C*H*W)로 평탄화\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 256),  # 입력: 3072 → 은닉층 256\n",
    "            nn.ReLU(),                # 활성화 함수\n",
    "            nn.Dropout(0.3),          # 과적합 방지 dropout\n",
    "            nn.Linear(256, num_classes)  # 출력: 클래스 수(10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # (N,3,32,32) → flatten → (N,3072) → fc → (N,10)\n",
    "        return self.fc(self.flatten(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fbe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 간단한 CNN (Convolutional Neural Network) 모델 정의\n",
    "# 이미지의 공간적 특성을 고려하여 합성곱 + 풀링을 적용\n",
    "# -------------------------------\n",
    "class CNN_V1(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        # 특징 추출부 (convolution + pooling)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),   # 입력: RGB(3) → 32채널\n",
    "            nn.MaxPool2d(2),                            # 크기 절반 (32→16)\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), # 32채널 → 64채널\n",
    "            nn.MaxPool2d(2),                            # 크기 절반 (16→8)\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU() # 64채널 → 128채널\n",
    "        )\n",
    "        # 분류기 (classifier)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),   # 출력 크기를 (1,1)로 고정 → GAP(Global Avg Pooling) 효과, 공간 정보를 제거하고 채널별 특징만 남겨서 최종 분류에 사용\n",
    "            nn.Flatten(),                  # (N,128,1,1) → (N,128)\n",
    "            nn.Dropout(0.3),               # dropout 적용\n",
    "            nn.Linear(128, num_classes)    # 128 → 클래스 수(10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # 입력 이미지 → 특징 추출(features) → 분류(classifier)\n",
    "        return self.classifier(self.features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c91553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 파라미터 수 계산 함수\n",
    "# -------------------------------\n",
    "def count_params(model):\n",
    "    # requires_grad=True인 학습 대상 파라미터들의 총 개수를 합산\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 모델 인스턴스 생성 및 디바이스 할당 (CPU/GPU)\n",
    "mlp = MLP().to(device)\n",
    "cnn = CNN_V1().to(device)\n",
    "\n",
    "# 각 모델의 학습 가능한 파라미터 수 출력\n",
    "print('MLP params:', count_params(mlp))\n",
    "print('CNN_V1 params:', count_params(cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c85b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# CNN_V1의 각 레이어별 출력 텐서 크기 출력\n",
    "# -------------------------------\n",
    "shapes = []  # 각 레이어의 출력 텐서 크기를 순서대로 기록할 리스트\n",
    "\n",
    "model = CNN_V1()  # 모델 인스턴스 생성\n",
    "\n",
    "def make_hook(name):\n",
    "    # forward_hook 팩토리: 레이어 이름과 출력 텐서의 shape를 기록하는 훅을 생성\n",
    "    def _hook(module, inp, out):\n",
    "        shapes.append((name, out.shape))  # (레이어이름, 출력크기) 저장\n",
    "    return _hook\n",
    "\n",
    "hooks = []  # 등록된 hook 핸들을 보관(나중에 해제용)\n",
    "for i, layer in enumerate(list(model.features) + list(model.classifier)):\n",
    "    name = f\"{layer.__class__.__name__}_{i}\"  # 같은 클래스명이라도 인덱스로 고유하게\n",
    "    hooks.append(layer.register_forward_hook(make_hook(name)))  # 훅 등록\n",
    "\n",
    "out = model(x)  # 순전파 실행 → 각 레이어에서 훅이 호출되어 shapes가 채워짐\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()  # 사용이 끝난 훅 해제(메모리/중복 호출 방지)\n",
    "\n",
    "for name, shape in shapes:\n",
    "    print(f\"{name:20s} -> {shape}\")  # 레이어 이름과 출력 텐서 크기 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# CNN_V1의 Conv 레이어별\n",
    "# 1) 필터(커널 가중치)\n",
    "# 2) 해당 필터로 나온 feature map\n",
    "# 을 나란히 시각화\n",
    "# =========================================================\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# activations 저장용 딕셔너리\n",
    "activations = {}\n",
    "\n",
    "def save_activation(name):\n",
    "    def hook(m, i, o):\n",
    "        activations[name] = o.detach().cpu()[0]  # (C,H,W) → 배치 첫 이미지\n",
    "    return hook\n",
    "\n",
    "# CNN_V1 모델 새로 정의/로드\n",
    "cnn = CNN_V1().to(device)\n",
    "\n",
    "# Conv 레이어마다 hook 달고 이름 붙이기\n",
    "conv_layers = []\n",
    "conv_counter = 1\n",
    "for idx, layer in enumerate(cnn.features):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        name = f\"Conv{conv_counter}\"\n",
    "        conv_layers.append((name, layer))\n",
    "        layer.register_forward_hook(save_activation(name))\n",
    "        conv_counter += 1\n",
    "\n",
    "# 샘플 이미지 1장 통과시켜 feature map 저장\n",
    "sample_x, _ = next(iter(train_loader))\n",
    "sample_x = sample_x[:1].to(device)\n",
    "_ = cnn(sample_x)\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸: 필터(가중치) 시각화\n",
    "# -------------------------------\n",
    "def plot_filters(weights, title, max_channels=32, n_cols=8, cmap='gray'):\n",
    "    \"\"\"\n",
    "    weights: (out_channels, in_channels, k, k)\n",
    "    \"\"\"\n",
    "    out_ch, in_ch, k, _ = weights.shape\n",
    "    out_ch = min(out_ch, max_channels)\n",
    "    n_rows = math.ceil(out_ch / n_cols)\n",
    "\n",
    "    plt.figure(figsize=(n_cols*1.5, n_rows*1.5))\n",
    "    for c in range(out_ch):\n",
    "        w = weights[c]\n",
    "        # 여러 입력 채널(RGB 등)을 평균내어 2D로\n",
    "        w2d = w.mean(0).cpu().numpy()\n",
    "        # 0~1 정규화\n",
    "        w_min, w_max = w2d.min(), w2d.max()\n",
    "        norm = (w2d - w_min) / (w_max - w_min + 1e-9)\n",
    "\n",
    "        ax = plt.subplot(n_rows, n_cols, c+1)\n",
    "        ax.imshow(norm, cmap=cmap)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "    plt.suptitle(title + \" (Filters)\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 유틸: feature map 시각화\n",
    "# -------------------------------\n",
    "def plot_feature_map_grid(tensor_CHW, title, max_channels=32, n_cols=8, cmap='gray'):\n",
    "    C, H, W = tensor_CHW.shape\n",
    "    C = min(C, max_channels)\n",
    "    n_rows = math.ceil(C / n_cols)\n",
    "\n",
    "    plt.figure(figsize=(n_cols*1.5, n_rows*1.5))\n",
    "    for c in range(C):\n",
    "        fm = tensor_CHW[c].numpy()\n",
    "        fm_min, fm_max = fm.min(), fm.max()\n",
    "        if fm_max - fm_min < 1e-8:\n",
    "            norm = np.zeros_like(fm)\n",
    "        else:\n",
    "            norm = (fm - fm_min) / (fm_max - fm_min)\n",
    "\n",
    "        ax = plt.subplot(n_rows, n_cols, c+1)\n",
    "        ax.imshow(norm, cmap=cmap)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "    plt.suptitle(title + \" (Feature Maps)\", fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 각 Conv 레이어별로 필터 + feature map 함께 시각화\n",
    "# -------------------------------\n",
    "for name, layer in conv_layers:\n",
    "    # 1) 필터(커널 가중치)\n",
    "    plot_filters(layer.weight.data, title=name, max_channels=32, n_cols=8, cmap='gray')\n",
    "    # 2) feature map\n",
    "    fmap = activations[name]\n",
    "    C, H, W = fmap.shape\n",
    "    plot_feature_map_grid(fmap, title=f\"{name}: shape=({C},{H},{W})\", max_channels=32, n_cols=8, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a5d20",
   "metadata": {},
   "source": [
    "#### 필터를 씌울 수록 그림이 투박해지네필터를 씌울 수록 그림이 투박해지는 이유.\n",
    "#### 1) 해상도 감소 (Pooling 효과)\n",
    "- Conv 레이어 뒤에는 보통 MaxPool2d 같은 풀링이 있어서,\n",
    "- feature map 크기 (H×W)가 절반으로 줄어듭니다.\n",
    "- 예: 32×32 → 16×16 → 8×8\n",
    "- 공간 해상도가 줄어들면서 세부 디테일은 사라지고, 더 “투박한” 형태만 남게 됩니다.\n",
    "\n",
    "#### 2) 추상화 (Feature Abstraction)\n",
    "- 초기 레이어 (Conv1): 색상 대비, 에지(edge), 점 같은 저수준 특징을 잡음.\n",
    "- 중간 레이어 (Conv2): 눈, 귀, 바퀴 같은 부분적 패턴을 인식.\n",
    "- 깊은 레이어 (Conv3+): “개 vs 고양이”처럼 전체적인 구조/개념을 표현.\n",
    "- 이 과정에서 원본 이미지처럼 보이는 게 아니라, 모호하고 투박한 패턴만 남습니다.\n",
    "\n",
    "#### 3) 채널 수 증가\n",
    "- 레이어가 깊어질수록 채널 수(필터 수)는 늘어납니다.\n",
    "\n",
    "#### 즉, 각 채널은 특정 패턴(예: 수평 줄무늬, 곡선, 특정 질감)에만 반응하므로,전체 이미지를 그대로 재현하지 않고 부분적 신호만 강조합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 한 epoch(훈련 데이터 전체 1회) 동안 학습을 수행하는 함수\n",
    "# -------------------------------\n",
    "def train_epoch(model, loader, loss_fn, optim_):\n",
    "    model.train()                # 모델을 학습 모드로 전환 (Dropout, BN 동작 다르게 함)\n",
    "    tl = 0; tc = 0; n = 0        # tl: 총 loss, tc: 맞힌 개수, n: 샘플 수\n",
    "    for x, y in loader:          # DataLoader에서 배치 단위로 데이터 가져오기\n",
    "        x, y = x.to(device), y.to(device)   # GPU/CPU로 데이터 이동\n",
    "        optim_.zero_grad()                  # 이전 step의 gradient 초기화\n",
    "        out = model(x)                      # 모델 forward pass → 예측값\n",
    "        loss = loss_fn(out, y)              # 손실(loss) 계산\n",
    "        loss.backward()                     # backward pass (gradient 계산)\n",
    "        optim_.step()                       # optimizer로 파라미터 업데이트\n",
    "        # 배치 단위 통계 축적\n",
    "        tl += loss.item() * x.size(0)       # loss 총합 (배치 평균 × 배치 크기)\n",
    "        tc += (out.argmax(1) == y).sum().item()  # 맞춘 개수(정확도 계산용)\n",
    "        n  += x.size(0)                     # 전체 샘플 수 카운트\n",
    "    # 평균 loss와 정확도 반환\n",
    "    return tl/n, tc/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 한 epoch 동안 검증/평가를 수행하는 함수\n",
    "# -------------------------------\n",
    "@torch.no_grad()   # 평가 시 gradient 계산 끄기 (메모리/속도 최적화)\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval()                # 모델을 평가 모드로 전환 (Dropout, BN 고정)\n",
    "    tl = 0; tc = 0; n = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)                      # forward pass\n",
    "        loss = loss_fn(out, y)              # 손실 계산\n",
    "        # 배치 단위 통계 축적\n",
    "        tl += loss.item() * x.size(0)       \n",
    "        tc += (out.argmax(1) == y).sum().item()\n",
    "        n  += x.size(0)\n",
    "    # 평균 loss와 정확도 반환\n",
    "    return tl/n, tc/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e894462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 손실 함수 (다중 클래스 분류용 CrossEntropyLoss)\n",
    "# -------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -------------------------------\n",
    "# 모델 학습 함수\n",
    "# -------------------------------\n",
    "def train_model(model, epochs=5, lr=1e-3):\n",
    "    # Adam 옵티마이저 생성\n",
    "    optim_ = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 학습 과정 기록용 딕셔너리 (정확도/손실)\n",
    "    hist = {'train_acc':[], 'val_acc':[], 'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "    # epoch 반복\n",
    "    for ep in range(1, epochs+1):\n",
    "        # 1) 학습 데이터셋으로 1 epoch 학습\n",
    "        tr_l, tr_a = train_epoch(model, train_loader, criterion, optim_)\n",
    "        # 2) 검증 데이터셋으로 평가\n",
    "        va_l, va_a = eval_epoch(model, val_loader, criterion)\n",
    "\n",
    "        # 3) 로그 저장\n",
    "        hist['train_loss'].append(tr_l); hist['val_loss'].append(va_l)\n",
    "        hist['train_acc'].append(tr_a);  hist['val_acc'].append(va_a)\n",
    "\n",
    "        # 4) 학습/검증 결과 출력\n",
    "        #    형식: [Ep 현재/전체] train=정확도/손실  val=정확도/손실\n",
    "        print(f'[Ep {ep}/{epochs}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')\n",
    "\n",
    "    # 5) 학습 로그 반환 (그래프 그리기용)\n",
    "    return hist\n",
    "\n",
    "# -------------------------------\n",
    "# MLP 학습 (에폭 5회)\n",
    "# -------------------------------\n",
    "print('MLP_train_start')\n",
    "mlp_hist = train_model(mlp, epochs=10)\n",
    "print('MLP_train_end')\n",
    "print('--------------------------------')\n",
    "# -------------------------------\n",
    "# CNN 학습 (에폭 10회, MLP보다 더 오래 학습)\n",
    "# -------------------------------\n",
    "print('CNN_train_start')\n",
    "cnn_hist = train_model(cnn, epochs=10)\n",
    "print('CNN_train_end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 학습곡선(Accuracy, Loss)을 나란히 플롯하는 함수\n",
    "# -------------------------------\n",
    "def plot_hist(h, title):\n",
    "    # 정확도 곡선\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['train_acc'], label='train_acc')  # 학습 정확도\n",
    "    plt.plot(h['val_acc'], label='val_acc')      # 검증 정확도\n",
    "    plt.title(title+'(Accuracy)')                # 제목 (모델 이름 + Accuracy)\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # 손실 곡선\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['train_loss'], label='train_loss')  # 학습 손실\n",
    "    plt.plot(h['val_loss'], label='val_loss')      # 검증 손실\n",
    "    plt.title(title+'(Loss)')                      # 제목 (모델 이름 + Loss)\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# MLP 학습곡선 출력\n",
    "# -------------------------------\n",
    "plot_hist(mlp_hist, 'MLP ')\n",
    "\n",
    "# -------------------------------\n",
    "# CNN_V1 학습곡선 출력\n",
    "# -------------------------------\n",
    "plot_hist(cnn_hist, 'CNN_V1 ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcdc04",
   "metadata": {},
   "source": [
    "### 4. 오분류 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b61993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 모델 예측값 전체 얻기 (테스트셋 등에서)\n",
    "# -------------------------------\n",
    "@torch.no_grad()   # 평가 시에는 gradient 계산 끔 → 메모리/속도 최적화\n",
    "def get_all_preds(model, loader):\n",
    "    model.eval()        # 평가 모드 (Dropout, BN 고정)\n",
    "    ys=[]; ps=[]        # 정답 레이블(ys), 예측 레이블(ps) 저장용\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        out = model(x)  # forward → logits\n",
    "        ps.append(out.argmax(1).cpu().numpy())  # 예측 레이블\n",
    "        ys.append(y.numpy())                    # 정답 레이블\n",
    "    # 리스트들을 하나의 큰 배열로 합치기\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "# 전체 테스트셋 예측값 얻기\n",
    "y_true, y_pred = get_all_preds(cnn, test_loader)\n",
    "\n",
    "# -------------------------------\n",
    "# 혼동 행렬(confusion matrix) 계산 및 시각화\n",
    "# -------------------------------\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest')  # 혼동행렬 heatmap\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(10), class_names, rotation=45)  # X축 = 예측 클래스\n",
    "plt.yticks(range(10), class_names)               # Y축 = 실제 클래스\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 가장 많이 혼동된 클래스 쌍 Top-3 추출\n",
    "# -------------------------------\n",
    "pairs = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i!=j and cm[i,j] > 0:       # i≠j: 정답과 예측이 다른 경우\n",
    "            pairs.append(((i,j), cm[i,j]))   # ((실제, 예측), 횟수)\n",
    "\n",
    "# 혼동 빈도 기준으로 내림차순 정렬 → 상위 3개\n",
    "pairs = sorted(pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "print('Top-3 혼동 클래스 쌍:', [(class_names[i], class_names[j], n) for (i,j), n in pairs])\n",
    "\n",
    "# -------------------------------\n",
    "# 혼동 사례 이미지 표시\n",
    "# -------------------------------\n",
    "# 원본 CIFAR-10 테스트셋 (정규화 안 된 형태로)\n",
    "raw_test = datasets.CIFAR10(root='./data', train=False, download=False,\n",
    "                            transform=transforms.ToTensor())\n",
    "\n",
    "# test_loader와 raw_test 인덱스는 다를 수 있으므로, 직접 다시 예측해서 위치 확인\n",
    "def find_mismatch_indices(true_label, pred_label, max_show=6):\n",
    "    \"\"\"\n",
    "    true_label: 실제 라벨\n",
    "    pred_label: 잘못 예측된 라벨\n",
    "    max_show  : 최대 몇 장 보여줄지\n",
    "    \"\"\"\n",
    "    out_idx = []\n",
    "    count = 0\n",
    "    ptr = 0  # 전체 데이터셋 인덱스 추적\n",
    "    for batch_x, batch_y in DataLoader(test_set, batch_size=128, shuffle=False):\n",
    "        with torch.no_grad():\n",
    "            logits = cnn(batch_x.to(device))\n",
    "            preds = logits.argmax(1).cpu().numpy()\n",
    "        # 배치 내 샘플 확인\n",
    "        for k in range(len(batch_y)):\n",
    "            if batch_y[k].item()==true_label and preds[k]==pred_label:\n",
    "                out_idx.append(ptr+k)   # 원본 데이터셋에서의 인덱스\n",
    "                count += 1\n",
    "                if count >= max_show:\n",
    "                    return out_idx\n",
    "        ptr += len(batch_y)\n",
    "    return out_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Top-3 혼동 클래스 쌍별로 샘플 이미지 6장씩 시각화\n",
    "# -------------------------------\n",
    "for (i,j), _ in pairs:\n",
    "    ids = find_mismatch_indices(i,j, max_show=6)\n",
    "    if not ids: continue\n",
    "    plt.figure(figsize=(6,3))\n",
    "    for k, idx in enumerate(ids):\n",
    "        img, _ = raw_test[idx]   # (C,H,W)\n",
    "        plt.subplot(2,3,k+1)\n",
    "        plt.imshow(img.permute(1,2,0))  # (H,W,C)로 변환 후 출력\n",
    "        plt.axis('off')\n",
    "        plt.title(f'T:{class_names[i]} → P:{class_names[j]}')  # 실제 vs 예측\n",
    "    plt.suptitle(f'혼동 사례: {class_names[i]} vs {class_names[j]}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d34c4",
   "metadata": {},
   "source": [
    "#### 모델 개선 방안\n",
    "- Data Augmentation(회전, 뒤집기 등): 일반화 성능 개선.\n",
    "- Conv 블록마다 BatchNorm(학습 안정화/수렴 가속) & Dropout (과적합 방지).\n",
    "- 학습(epoch) 횟수를 늘려보기.\n",
    "- CNN_V1 대신 MobileNet, EfficientNet 같은 검증된 아키텍처 활용.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a8b5c",
   "metadata": {},
   "source": [
    "### 5. 성능 개선 기법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95800276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 데이터 증강(Augmentation) 파이프라인 + 정규화\n",
    "#  - 좌우반전, 랜덤 크롭, 색상 변형으로 데이터 다양성 ↑ → 과적합 완화\n",
    "#  - 마지막에 CIFAR-10 통계로 Normalize\n",
    "# ------------------------------------------------------------\n",
    "aug_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),                 # 50% 확률로 좌우 반전\n",
    "    transforms.RandomCrop(32, padding=4),              # 가장자리에 4픽셀 패딩 후 32x32 무작위 크롭\n",
    "    transforms.ColorJitter(brightness=0.2,             # 밝기/대비/채도 약간 변형\n",
    "                           contrast=0.2,\n",
    "                           saturation=0.2),\n",
    "    transforms.ToTensor(),                             # [0,255] → [0,1] Tensor\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),       # 채널별 평균/표준편차 (CIFAR-10)\n",
    "                         (0.2470,0.2435,0.2616))\n",
    "])\n",
    "\n",
    "# 증강 적용한 학습 전체셋을 동일한 비율로 train/val 분할\n",
    "train_full_aug = datasets.CIFAR10(root='./data', train=True, download=False, transform=aug_tf)\n",
    "train_set2, val_set2 = random_split(train_full_aug, [train_len, val_len])\n",
    "\n",
    "# DataLoader: 학습은 섞고(shuffle=True), 검증은 고정(shuffle=False)\n",
    "train_loader2 = DataLoader(train_set2, batch_size=128, shuffle=True,  num_workers=0)\n",
    "val_loader2   = DataLoader(val_set2,   batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28611cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 개선 모델: CNN_V2\n",
    "#  - Conv 블록마다 BatchNorm → 학습 안정화/수렴 가속\n",
    "#  - Dropout → 과적합 방지\n",
    "#  - MaxPool로 해상도 축소, 마지막은 GAP(AdaptiveAvgPool2d(1,1))\n",
    "# ------------------------------------------------------------\n",
    "class CNN_V2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Block1: 32채널 두 번의 3x3 Conv + BN + ReLU\n",
    "            nn.Conv2d(3, 32, 3, padding=1),  nn.BatchNorm2d(32),  nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32),  nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                  # 32→16로 다운샘플\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block2: 64채널 두 번의 3x3 Conv + BN + ReLU\n",
    "            nn.Conv2d(32, 64, 3, padding=1),  nn.BatchNorm2d(64),  nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),  nn.BatchNorm2d(64),  nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                  # 16→8로 다운샘플\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block3: 128채널 Conv + BN + ReLU\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "\n",
    "            # 분류 헤드: GAP → Flatten → Dropout → Linear\n",
    "            nn.AdaptiveAvgPool2d((1,1)),      # (C, H, W) → (C, 1, 1) : Global Avg Pooling\n",
    "            nn.Flatten(),                      # (N, 128, 1, 1) → (N, 128)\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)        # 최종 로지트\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "cnn2 = CNN_V2().to(device)\n",
    "\n",
    "# 다중분류 손실\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Optimizer 실험: SGD(momentum) vs Adam\n",
    "#  - opt_name만 바꿔 두 방식 비교 가능\n",
    "# ------------------------------------------------------------\n",
    "opt_name  = 'SGD'  # 'Adam'으로 바꾸면 Adam으로 실험\n",
    "optimizer = (optim.SGD(cnn2.parameters(), lr=0.01, momentum=0.9)\n",
    "             if opt_name == 'SGD'\n",
    "             else optim.Adam(cnn2.parameters(), lr=1e-3))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 학습 루프: train/val 손실·정확도 기록 및 출력\n",
    "# ------------------------------------------------------------\n",
    "hist2 = {'train_acc': [], 'val_acc': [], 'train_loss': [], 'val_loss': []}\n",
    "for ep in range(1, 16):  # 기본 11epoch; 필요시 더 늘려도 좋음(예: 30~50)\n",
    "    tr_l, tr_a = train_epoch(cnn2, train_loader2, criterion, optimizer)  # 1 epoch 학습\n",
    "    va_l, va_a = eval_epoch(cnn2,  val_loader2,  criterion)              # 1 epoch 검증\n",
    "\n",
    "    hist2['train_loss'].append(tr_l); hist2['val_loss'].append(va_l)\n",
    "    hist2['train_acc'].append(tr_a);  hist2['val_acc'].append(va_a)\n",
    "\n",
    "    print(f'[CNN_V2][{opt_name}][Ep {ep}] train={tr_a:.3f}/{tr_l:.3f}  val={va_a:.3f}/{va_l:.3f}')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 학습 곡선(정확도/손실) 시각화\n",
    "# ------------------------------------------------------------\n",
    "def plot_hist(h, title):\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['train_acc'], label='train_acc')\n",
    "    plt.plot(h['val_acc'],   label='val_acc')\n",
    "    plt.title(title+' Acc'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(h['train_loss'], label='train_loss')\n",
    "    plt.plot(h['val_loss'],   label='val_loss')\n",
    "    plt.title(title+' Loss'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_hist(hist2, f'CNN_V2 ({opt_name})')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Metacode Lecture (Python 3.13)",
   "language": "python",
   "name": "metacode-lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
